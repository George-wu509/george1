
|                                                                                                                                                                  |                                                                                                                                                                                                                                                                               |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Classification                                                                                                                                                   | 1. [==K-nearest neighbors(KNN)==](https://medium.com/programming-with-data/k-%E8%BF%91%E9%84%B0%E6%BC%94%E7%AE%97%E6%B3%95-k-nearest-neighbors-72b6cec68367), 2. [==SVM==](https://zhuanlan.zhihu.com/p/696741013), 3. Decision Tree, 4. Naive Bayes, 5. Logistic Regression, |
| Cluster                                                                                                                                                          | 1. [==K-means==](https://chih-sheng-huang821.medium.com/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E9%9B%86%E7%BE%A4%E5%88%86%E6%9E%90-k-means-clustering-e608a7fe1b43), 2. Hierarchical Clustering                                                                                |
| Regression                                                                                                                                                       | 1. Linear Regression, 2. XGBoost, 3. MLP                                                                                                                                                                                                                                      |
| Dimension                                                                                                                                                        | 1. PCA                                                                                                                                                                                                                                                                        |
| Optimization                                                                                                                                                     | 1. SGD                                                                                                                                                                                                                                                                        |
| [Ensemble](https://chih-sheng-huang821.medium.com/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-ensemble-learning%E4%B9%8Bbagging-boosting%E5%92%8Cadaboost-af031229ebc3) | 1. Bagging、2. Random Forest, 3. Boosting, 4. AdaBoost                                                                                                                                                                                                                         |
|                                                                                                                                                                  |                                                                                                                                                                                                                                                                               |
| others                                                                                                                                                           | [[### Bias-Variance Tradeoff]]                                                                                                                                                                                                                                                |
|                                                                                                                                                                  |                                                                                                                                                                                                                                                                               |

|                   |     |
| ----------------- | --- |
| [[##### QA-list]] |     |


Ref: [機器學習: Ensemble learning之Bagging、Boosting和AdaBoost](https://chih-sheng-huang821.medium.com/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-ensemble-learning%E4%B9%8Bbagging-boosting%E5%92%8Cadaboost-af031229ebc3)


以下是機器學習中最常用或在面試中經常出現的 10 種算法的詳細解釋，並提供了對應的 MATLAB 代碼示例。這些算法涵蓋監督學習、無監督學習和增強學習。

### 1. 線性回歸（Linear Regression）

**應用**：線性回歸用於建模目標變量和一個或多個輸入特徵之間的線性關係。這是最簡單的回歸算法之一。

**原理**：假設目標變量 yyy 和輸入特徵 XXX 之間呈線性關係，即 y=wX+by = wX + by=wX+b，其中 www 是權重，bbb 是偏置項。

**MATLAB 範例代碼**：

matlab

複製程式碼

`% 線性回歸模型 x = (1:10)'; y = [1; 2; 3; 4; 5; 6; 7; 8; 9; 10];  % 構建線性回歸模型 mdl = fitlm(x, y);  % 預測 y_pred = predict(mdl, x); plot(x, y, 'o', x, y_pred, '-'); title('線性回歸擬合');`

### 2. 邏輯回歸（Logistic Regression）

**應用**：邏輯回歸主要用於二分類問題，例如癌症檢測（陽性/陰性）。

**原理**：邏輯回歸是一種分類算法，它預測目標變量的概率，通常用 sigmoid 函數將線性回歸轉換為二分類模型。

**MATLAB 範例代碼**：

matlab

複製程式碼

`% 邏輯回歸模型 X = [1, 2; 2, 3; 3, 4; 4, 5; 5, 6]; % 特徵矩陣 Y = [0; 0; 1; 1; 1]; % 標籤  % 構建邏輯回歸模型 mdl = fitglm(X, Y, 'Distribution', 'binomial');  % 預測 Y_pred = predict(mdl, X) > 0.5;  % 分類結果`

### 3. K-近鄰算法（K-Nearest Neighbors, KNN）

**應用**：KNN 用於分類和回歸，特別適合模式識別和異常檢測。

**原理**：KNN 是一種非參數算法，它基於距離度量，通過查找最近的 K 個鄰居來進行分類或回歸。

**MATLAB 範例代碼**：

matlab

複製程式碼

`% KNN 分類器 X = [1,2; 2,3; 3,3; 4,5; 5,6];  % 特徵 Y = [0; 0; 1; 1; 1];             % 標籤 mdl = fitcknn(X, Y, 'NumNeighbors', 3);  % 預測 X_new = [3, 4]; label = predict(mdl, X_new);`

### 4. 支持向量機（Support Vector Machine, SVM）

**應用**：SVM 常用於圖像分類、文本分類等二分類問題，並且也可擴展到多分類。

**原理**：SVM 尋找一個最大化類別間距的超平面來區分不同的類別，並且可以通過核函數應對非線性分類。

**MATLAB 範例代碼**：

matlab

複製程式碼

`% SVM 分類器 X = [1,2; 2,3; 3,3; 4,5; 5,6];  % 特徵 Y = [0; 0; 1; 1; 1];             % 標籤 mdl = fitcsvm(X, Y);  % 預測 X_new = [3, 4]; label = predict(mdl, X_new);`

### 5. 決策樹（Decision Tree）

**應用**：決策樹用於分類和回歸，常見於金融風險評估、醫學診斷等領域。

**原理**：決策樹是一種基於特徵的樹狀結構算法，通過選擇信息增益（或基尼不純度）最大的特徵進行劃分。

**MATLAB 範例代碼**：

matlab

複製程式碼

`% 決策樹分類器 X = [1,2; 2,3; 3,3; 4,5; 5,6];  % 特徵 Y = [0; 0; 1; 1; 1];             % 標籤 mdl = fitctree(X, Y);  % 預測 X_new = [3, 4]; label = predict(mdl, X_new); view(mdl, 'Mode', 'graph');  % 顯示決策樹`

### 6. 隨機森林（Random Forest）

**應用**：隨機森林用於分類和回歸，適用於各種領域，並且在處理缺失數據和非線性問題時表現良好。

**原理**：隨機森林是一種集成算法，它通過構建多棵決策樹來提高分類或回歸的準確性，最終的預測結果是所有決策樹結果的平均值或眾數。

**MATLAB 範例代碼**：

matlab

複製程式碼

`% 隨機森林分類器 X = [1,2; 2,3; 3,3; 4,5; 5,6];  % 特徵 Y = [0; 0; 1; 1; 1];             % 標籤 mdl = TreeBagger(50, X, Y, 'Method', 'classification');  % 預測 X_new = [3, 4]; [label, scores] = predict(mdl, X_new);`

### 7. 朴素貝葉斯（Naive Bayes）

**應用**：朴素貝葉斯經常用於文本分類，如垃圾郵件檢測，並且能夠處理高維數據。

**原理**：朴素貝葉斯基於貝葉斯定理，並假設特徵之間是條件獨立的。這使得它計算效率高且容易實現。

**MATLAB 範例代碼**：

matlab

複製程式碼

`% 朴素貝葉斯分類器 X = [1,2; 2,3; 3,3; 4,5; 5,6];  % 特徵 Y = [0; 0; 1; 1; 1];             % 標籤 mdl = fitcnb(X, Y);  % 預測 X_new = [3, 4]; label = predict(mdl, X_new);`

### 8. K-Means 聚類（K-Means Clustering）

**應用**：K-Means 用於無監督學習中的聚類問題，例如市場分割和圖像壓縮。

**原理**：K-Means 通過最小化每個數據點與最近的簇中心之間的距離來劃分數據。

**MATLAB 範例代碼**：

matlab

複製程式碼

`% K-Means 聚類 X = [1,2; 2,3; 3,3; 4,5; 5,6];  % 特徵 k = 2;  % 聚類數 [idx, C] = kmeans(X, k);  % 顯示結果 scatter(X(:,1), X(:,2), 50, idx, 'filled'); hold on; plot(C(:,1), C(:,2), 'kx', 'MarkerSize', 15, 'LineWidth', 3);`

### 9. 主成分分析（Principal Component Analysis, PCA）

**應用**：PCA 用於數據降維，特別是在高維數據集中，通過投影到低維空間來保留盡可能多的數據變異性。

**原理**：PCA 通過計算數據的主成分（特徵向量）來最大化變異性，並丟棄低變異性的成分。

**MATLAB 範例代碼**：

matlab

複製程式碼

`% 主成分分析 (PCA) X = [1,2; 2,3; 3,3; 4,5; 5,6];  % 特徵矩陣 [coeff, score, latent] = pca(X);  %`

除了之前介紹的常見機器學習算法，以下是一些面試中經常出現的聚類（Clustering）、預測（Prediction）和其他重要的機器學習算法。這些算法不僅常用，還涉及到很多實際應用。每個算法的介紹都附有 MATLAB 代碼範例，以便更好地理解這些技術。

### 1. **層次聚類（Hierarchical Clustering）**

**應用**：層次聚類用於無監督學習的聚類任務，特別適合基因表達分析、客戶群體細分等場景。它不需要事先指定簇數，可以構建層次結構（樹狀圖）。

**原理**：層次聚類從每個數據點作為一個簇開始，然後不斷將最相似的簇合併，直到所有數據點都合併到一個簇中。

**MATLAB 範例代碼**：

matlab

複製程式碼

`% 示例數據 X = rand(10, 2);  % 計算距離矩陣 distanceMatrix = pdist(X);  % 進行層次聚類 tree = linkage(distanceMatrix, 'average');  % 畫出樹狀圖 dendrogram(tree);`

### 2. **DBSCAN（Density-Based Spatial Clustering of Applications with Noise）**

**應用**：DBSCAN 用於發現數據集中任意形狀的簇，特別適合處理具有異常點（噪聲）的數據。它在聚類數未知時也能很好地工作。

**原理**：DBSCAN 基於密度進行聚類，對每個數據點進行密度檢查，將密度相鄰的點分配到相同簇，異常點會被標記為噪聲。

**MATLAB 範例代碼**：

matlab

複製程式碼

`% 示例數據 X = [randn(100, 2) + 1; randn(100, 2) - 1];  % 使用 DBSCAN 聚類 epsilon = 0.5; % 鄰域半徑 minpts = 5;    % 最少鄰居數 labels = dbscan(X, epsilon, minpts);  % 可視化結果 gscatter(X(:,1), X(:,2), labels);`

### 3. **拉索回歸（Lasso Regression）**

**應用**：拉索回歸適用於特徵選擇和避免過擬合。它通過在損失函數中加入 L1 正則化項來約束模型的複雜度。

**原理**：Lasso 回歸的損失函數中加入了一個正則化項，該項懲罰大係數，從而迫使某些特徵的權重變為零，實現特徵選擇。

**MATLAB 範例代碼**：

matlab

複製程式碼

`% 示例數據 X = rand(100, 5); y = rand(100, 1);  % Lasso 回歸 [B, FitInfo] = lasso(X, y);  % 顯示回歸系數 lassoPlot(B, FitInfo, 'PlotType', 'Lambda', 'XScale', 'log');`

### 4. **梯度提升機（Gradient Boosting Machine, GBM）**

**應用**：梯度提升機常用於回歸和分類問題，尤其在比賽中（如 Kaggle 比賽）表現良好。它用於建立一個強學習器，通過迭代地將弱學習器（如決策樹）進行加權組合。

**原理**：GBM 通過一系列決策樹逐步修正前一棵樹的錯誤預測。每次迭代中，新樹的權重是基於當前模型的殘差計算的。

**MATLAB 範例代碼**：

matlab

複製程式碼

`% 示例數據 X = rand(100, 5); y = rand(100, 1);  % 梯度提升回歸模型 Mdl = fitrensemble(X, y, 'Method', 'LSBoost', 'NumLearningCycles', 100);  % 預測 y_pred = predict(Mdl, X);`

### 5. **XGBoost（Extreme Gradient Boosting）**

**應用**：XGBoost 是一種增強型的梯度提升算法，在各種回歸和分類問題中表現優秀。它是基於梯度提升樹的強化版本，能高效處理缺失數據和非線性問題。

**原理**：XGBoost 提供了正則化梯度提升模型，通過加入 L1 和 L2 正則化來防止過擬合，並且使用了樹結構提升演算法。

MATLAB 本身不直接支持 XGBoost，但你可以使用 XGBoost 的 MEX 接口或 Python 版本。

### 6. **貝葉斯優化（Bayesian Optimization）**

**應用**：貝葉斯優化常用於參數調優，特別是當算法參數空間複雜且昂貴時。它能夠高效地搜索最優參數組合。

**原理**：貝葉斯優化基於概率模型（如高斯過程）來預測最有希望的參數，並逐步更新模型以接近全局最優解。

**MATLAB 範例代碼**：

matlab

複製程式碼

`% 設定目標函數 objFcn = @(x) (x.^2 + 3*x + 5);  % 使用貝葉斯優化找到最優解 results = bayesopt(objFcn, [optimizableVariable('x', [-10, 10])]);  % 顯示最優解 bestX = results.XAtMinObjective;`

### 7. **AdaBoost（Adaptive Boosting）**

**應用**：AdaBoost 是一種經典的增強算法，經常用於處理分類問題，如人臉識別和文本分類。它通過組合弱學習器來構建一個強學習器。

**原理**：AdaBoost 通過對錯誤分類的數據點加大權重，使得後續的弱學習器更關注這些錯誤分類點，最終達到提高分類性能的效果。

**MATLAB 範例代碼**：

matlab

複製程式碼

`% 示例數據 X = rand(100, 5); Y = randi([0,1], 100, 1);  % 二分類標籤  % 使用 AdaBoost 進行分類 Mdl = fitcensemble(X, Y, 'Method', 'AdaBoostM1');  % 預測 y_pred = predict(Mdl, X);`

### 8. **多層感知器（Multilayer Perceptron, MLP）**

**應用**：MLP 是一種經典的前向神經網絡，常用於處理分類、回歸以及圖像識別等任務。

**原理**：MLP 包含輸入層、隱藏層和輸出層，通過反向傳播算法來調整權重以最小化誤差。每個神經元使用激活函數（如 ReLU 或 Sigmoid）來實現非線性映射。

**MATLAB 範例代碼**：

matlab

複製程式碼

`% 創建一個 MLP 網絡 net = feedforwardnet(10);  % 隱藏層有 10 個神經元  % 訓練數據 X = rand(100, 5);  % 輸入數據 Y = randi([0,1], 100, 1);  % 標籤  % 訓練神經網絡 net = train(net, X', Y');  % 預測 y_pred = net(X');`

### 9. **隨機梯度下降（Stochastic Gradient Descent, SGD）**

**應用**：SGD 是一種優化算法，廣泛用於深度學習中進行參數更新。特別適合大數據集上的優化問題。

**原理**：SGD 在每次迭代中隨機選取一個或小批量樣本來更新模型參數，從而降低計算成本。它是一種快速且內存效率高的優化算法。

**MATLAB 範例代碼**： MATLAB 本身不直接提供 SGD，但可以手動實現。

matlab

複製程式碼

`% 假設目標函數 loss = (theta - x)^2 theta = 0;  % 初始化參數 learning_rate = 0.01;  % 使用 SGD 更新 theta for i`



### Bias-Variance Tradeoff

詳細解釋一下什麼是偏差-方差權衡 (Bias-Variance Tradeoff)，以及它與模型複雜度和過擬合 (Overfitting) 的關係。

## 偏差-方差權衡 (Bias-Variance Tradeoff)

偏差-方差權衡是機器學習中一個核心概念，它描述了在訓練一個預測模型時，由模型的**偏差 (Bias)** 和**方差 (Variance)** 引起的泛化誤差之間的權衡關係。泛化誤差是指模型在未見過的新數據上的預測誤差。

![[Pasted image 20250501160939.png]]

**1. 偏差 (Bias):**

- **定義:** 偏差是指模型預測值與真實值之間的**平均差異**。高偏差的模型通常會做出簡化的假設，導致模型**欠擬合 (Underfitting)** 訓練數據，並且在訓練集和測試集上的性能都較差。
- **理解:** 偏差可以看作是模型對真實數據關係的錯誤假設程度。一個高偏差的模型可能假設數據是線性的，但真實關係可能是非線性的，因此它無法很好地捕捉到數據的真實模式。
- **例子:** 一個線性回歸模型試圖擬合一個高度非線性的數據集，就會產生高偏差。模型的能力不足以捕捉數據的複雜性。

**2. 方差 (Variance):**

- **定義:** 方差是指模型在**不同訓練數據集上的預測值的變化程度**。高方差的模型對訓練數據中的隨機噪聲非常敏感，導致模型**過擬合 (Overfitting)** 訓練數據，在訓練集上表現很好，但在未見過的測試集上表現很差。
- **理解:** 方差反映了模型對訓練數據微小波動的敏感性。一個高方差的模型會學習到訓練數據中的特定細節和噪聲，這些細節和噪聲可能並不能推廣到新的數據上。
- **例子:** 一個非常高階的多項式回歸模型擬合一個只有少量數據點的數據集，可能會產生高方差。模型學習到了每個訓練數據點，包括其中的噪聲，導致它在新的數據點上的預測很不穩定。

**3. 泛化誤差的分解:**

在迴歸問題中，泛化誤差 (通常用均方誤差 MSE 來衡量) 可以分解為三個部分：

E[(y−f^​(x))2]=Bias(f^​(x))2+Variance(f^​(x))+σϵ2​

其中：

- E[(y−f^​(x))2] 是預測值 f^​(x) 與真實值 y 的期望平方誤差 (泛化誤差)。
- Bias(f^​(x))2 是偏差的平方，衡量了模型預測值的平均值與真實值之間的差異。
- Variance(f^​(x)) 是方差，衡量了模型在不同訓練集上的預測值的變化程度。
- σϵ2​ 是不可約誤差 (Irreducible Error)，這是由數據本身的噪聲引起的，無法通過模型改進來消除。

**偏差-方差權衡的本質:**

偏差和方差之間通常存在一種權衡關係。

- **簡單的模型 (低複雜度):** 往往具有**高偏差**和**低方差**。它們對訓練數據的假設較強，不容易受到訓練數據中噪聲的影響，但在擬合複雜的真實關係時能力不足。
- **複雜的模型 (高複雜度):** 往往具有**低偏差**和**高方差**。它們對訓練數據的假設較弱，能夠捕捉到訓練數據中的細節，但也更容易受到訓練數據中噪聲的影響，導致在新的數據上表現不佳。

**我們的目標是找到一個模型複雜度適中的平衡點，使得模型的偏差和方差都相對較低，從而獲得較好的泛化能力。**

## 偏差-方差權衡與模型複雜度的關係

**模型複雜度**是指模型能夠擬合複雜數據關係的能力。模型的複雜度通常由以下因素決定：

- **模型類型:** 例如，線性模型比非線性模型 (如高階多項式、決策樹、神經網路) 更簡單。
- **模型參數數量:** 參數越多的模型通常越複雜，能夠學習更複雜的函數。
- **模型靈活性:** 模型越靈活，能夠擬合各種形狀的數據。

**關係:**

- **低模型複雜度:**
    - 模型對數據的假設很強 (高偏差)。
    - 模型不容易受到訓練數據變化的影響 (低方差)。
    - 模型傾向於欠擬合。
- **高模型複雜度:**
    - 模型對數據的假設很弱 (低偏差)。
    - 模型容易受到訓練數據變化的影響 (高方差)。
    - 模型傾向於過擬合。

## 偏差-方差權衡與過擬合的關係

**過擬合 (Overfitting)** 是指模型在訓練數據上表現非常好，但在未見過的測試數據上表現很差的現象。過擬合通常是由於模型學習了訓練數據中的噪聲和特定細節，而這些噪聲和細節並不能推廣到新的數據上。

**偏差-方差權衡解釋過擬合:**

- **高方差是導致過擬合的主要原因之一。** 當模型具有高方差時，它會對訓練數據中的微小波動非常敏感，導致它學習到訓練數據中的噪聲。這種模型在新的、略有不同的數據上表現會很差，因為它學習到的噪聲與新數據無關。
- **低偏差的模型更容易出現過擬合。** 為了在訓練數據上獲得非常好的性能，模型通常需要具有足夠的複雜度來捕捉數據的真實模式 (低偏差)。然而，如果模型過於複雜，它就可能開始學習訓練數據中的噪聲，導致高方差和過擬合。

**欠擬合 (Underfitting):**

與過擬合相反，欠擬合是指模型在訓練數據和測試數據上的性能都很差。這通常是由於模型過於簡單 (高偏差)，無法捕捉到數據的真實模式。

**總結:**

偏差-方差權衡告訴我們，在選擇模型時，需要考慮模型的複雜度，以在偏差和方差之間找到一個平衡點。過於簡單的模型 (高偏差、低方差) 會欠擬合，而過於複雜的模型 (低偏差、高方差) 則容易過擬合。我們的目標是選擇一個複雜度適中的模型，使其既能捕捉到數據的真實模式 (低偏差)，又不會過度學習訓練數據中的噪聲 (低方差)，從而獲得良好的泛化能力。

在實踐中，我們通常會使用交叉驗證等技術來評估不同模型複雜度的性能，並選擇在驗證集上表現最好的模型，以期在偏差和方差之間取得一個好的平衡。


##### QA-list

| Q                          | Ans            |
| -------------------------- | -------------- |
| 實現k-means                  |                |
| Bagging和Boosting的区别？       | [[##### Ans2]] |
| LR(逻辑回归)和SVM的联系和区别？        | [[##### Ans3]] |
| 逻辑回归和线性回归的联系和区别            |                |
| LR的损失函数                    |                |
| K-means聚类的原理以及过程？怎么衡量相似度的？ |                |
| XGBoost的正则项是什么？            |                |
| K-means和EM算法的关系            |                |
| 长尾问题怎么处理？                  | [[##### Ans9]] |
| SVM的损失函数是什么？具体形式？          |                |
| SVM的核函数了解哪些？为什么要用核函数？      |                |
| SVM如何解决线性不可分问题？            |                |
| LR和SVM介绍+区别，什么场景用SVM比较好    |                |
|                            |                |


##### Ans2

_**【3】Bagging和Boosting的区别？**_
### [随机森林](https://zhida.zhihu.com/search?content_id=167439930&content_type=Article&match_order=1&q=%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97&zhida_source=entity)属于集成学习(ensemble learning)中的bagging算法，在集成算法中主要分为bagging算法与boosting算法，

### Bagging算法(套袋法)

- bagging的算法过程如下：

1. 从原始样本集中使用Bootstraping 方法随机抽取n个训练样本，共进行k轮抽取，得到k个训练集（k个训练集之间相互独立，元素可以有重复）。
2. 对于n个训练集，我们训练k个模型，（这个模型可根据具体的情况而定，可以是[决策树](https://zhida.zhihu.com/search?content_id=167439930&content_type=Article&match_order=1&q=%E5%86%B3%E7%AD%96%E6%A0%91&zhida_source=entity)，KNN等）
3. 对于分类问题：由投票表决产生的分类结果；对于回归问题，由k个模型预测结果的均值作为最后预测的结果（所有模型的重要性相同）。

### Boosting（提升法）

- boosting的算法过程如下：

1. 对于训练集中的每个样本建立权值wi，表示对每个样本的权重， 其关键在于对于被错误分类的样本权重会在下一轮的分类中获得更大的权重（错误分类的样本的权重增加）。
2. 同时加大分类 误差概率小的弱分类器的权值，使其在表决中起到更大的作用，减小分类误差率较大弱分类器的权值，使其在表决中起到较小的作用。每一次迭代都得到一个弱分类器，需要使用某种策略将其组合，最为最终模型，(adaboost给每个迭代之后的弱分类器一个权值，将其线性组合作为最终的分类器,误差小的分类器权值越大。)

  

### Bagging和Boosting 的主要区别

- **样本选择上:** Bagging采取Bootstraping的是随机有放回的取样，Boosting的每一轮训练的样本是固定的，改变的是每个样本的权重。
- **样本权重上：**Bagging采取的是均匀取样，且每个样本的权重相同，Boosting根据错误率调整样本权重，错误率越大的样本权重会变大
- **预测函数上：**Bagging所有的预测函数权值相同，Boosting中误差越小的预测函数其权值越大。
- **[并行计算](https://zhida.zhihu.com/search?content_id=167439930&content_type=Article&match_order=1&q=%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97&zhida_source=entity):** Bagging 的各个预测函数可以并行生成;Boosting的各个预测函数必须按照顺序迭代生成.

### 将决策树与以上框架组合成新的算法

- Bagging + 决策树 = 随机森林
- AdaBoost + 决策树 = 提升树
- gradient + 决策树 = GDBT


##### Ans3
_**【5】LR(逻辑回归)和SVM的联系和区别？**_

- **联系：**

- LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题
- 两个方法都可以增加不同的正则化项，如L1、L2[正则化项](https://zhida.zhihu.com/search?content_id=167439930&content_type=Article&match_order=2&q=%E6%AD%A3%E5%88%99%E5%8C%96%E9%A1%B9&zhida_source=entity)

- **区别：**

- LR是参数模型，SVM是非参数模型
- 从损失函数来看，LR使用的是**[交叉熵](https://zhida.zhihu.com/search?content_id=167439930&content_type=Article&match_order=1&q=%E4%BA%A4%E5%8F%89%E7%86%B5&zhida_source=entity)损失函数**，SVM使用的**hinge损失函数**，这两个损失函数的目的都是**增加对分类影响较大的样本点的权重，减小与分类关系比较小的数据点的权重。**
- SVM的处理方法只考虑**支持向量**，也就是只考虑和分类最相关的少数样本点来学习分类器。而逻辑回归通过**非线性映射**，大大减小了离分离超平面远的样本点权重，相对提升了与分类最相关的样本点的权重。
- LR模型相对来说简单好理解，一般用于大规模的线性分类。SVM的理解和优化比较复杂，在处理复制非线性分类时，使用核技巧来计算优势明显。
- LR能做的SVM也能做，但可能准确率是上有问题，但SVM能做的LR做不了。


##### Ans9

**什么是长尾问题？**

[长尾效应](https://zhida.zhihu.com/search?content_id=167439930&content_type=Article&match_order=1&q=%E9%95%BF%E5%B0%BE%E6%95%88%E5%BA%94&zhida_source=entity)本质上就是**数据类别不均衡**导致**少部分类别占大多数样本**，而**大多数类别只有小部分样本**，在数量分布图上呈现出长长的尾巴的现象。一般的解决办法是人工平衡类别。标签集中，部分标签与很多文本样本相关联，但是还有一些标签与之相关联性非常少，甚至说没有和文本样本相关联，可以理解为长尾问题。

**如何处理？**

其实本质上是信息不足，那么可以从补充信息的角度去思考，比如对于[文本分类](https://zhida.zhihu.com/search?content_id=167439930&content_type=Article&match_order=1&q=%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB&zhida_source=entity)问题，可以尝试着将标签集的更多信息，比如层级信息等等，放入模型中，让模型有更多的信息可以去学习，从而弥补信息不足的问题。那么与这种思考方式类似的，还有过采样，欠采样的方式来平衡数据集分布。

如果从集成学习的角度来说，比如boosting中的投票里的各个基分类器的权值来考虑，我们也可以给这些标签设置不一样的权值；并且集成学习本身也可以在一定程度上解决长尾问题。

还有很多处理思路，我觉得有效的其实就是，更偏向于学术界的创新思路了，比如重构文本与标签的对应关系，比如原来是文本预测其多个标签，其实可以重构为文本与多个标签的匹配问题，有不少论文也是这样处理的，同时也会解决全连接层性价比低的问题，这些方法是真正有效能够从根本上解决的，之前所说的过采样、[欠采样](https://zhida.zhihu.com/search?content_id=167439930&content_type=Article&match_order=2&q=%E6%AC%A0%E9%87%87%E6%A0%B7&zhida_source=entity)等等方法都是缓兵之计，从根本上解决问题，才是解决问题。

可以总的分为以下几种方法：

- **Re-sampling：**主要是**在训练集上实现样本平衡**，如对tail中的类别样本进行过采样，或者对head类别样本进行欠采样。基于重采样的解决方案适用于检测框架，但可能会导致训练时间增加以及对tail类别的过度拟合风险。
- **Re-weighting：**主要在训练loss中，给不同的类别的loss**设置不同的权重**，对tail类别loss设置更大的权重。但是这种方法对超参数选择非常敏感，并且由于难以处理特殊背景类（非常多的类别）而不适用于检测框架。
- **Learning strategy：**有专门为解决少样本问题涉及的学习方法可以借鉴，如：meta-learning、[metric learning](https://zhida.zhihu.com/search?content_id=167439930&content_type=Article&match_order=1&q=metric+learning&zhida_source=entity)、transfer learing。另外，还可以**调整训练策略**，将训练过程分为两步：第一步不区分head样本和tail样本，对模型正常训练；第二步，设置小的学习率，对第一步的模型使用各种样本平衡的策略进行finetune。
- 综合使用以上策略