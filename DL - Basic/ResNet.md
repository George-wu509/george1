
## ResNet-50 架構概述

ResNet-50 是一個50層的深度殘差網絡,主要由Bottleneck blocks組成。它的整體架構如下:

1. 一個7x7的卷積層
2. 一個最大池化層
3. 4組殘差層(每組包含多個Bottleneck blocks)
4. 一個全局平均池化層
5. 一個全連接層

## Residual Block 解釋

殘差塊(Residual Block)是ResNet的核心組件。它的主要思想是:

1. 讓網絡學習殘差函數F(x) = H(x) - x,而不是直接學習H(x)。
2. 通過skip connection(跳躍連接)將輸入直接加到輸出上。

這樣做的好處是:

1. 緩解了深度網絡的梯度消失問題。
2. 使得網絡更容易學習恆等映射,有助於網絡的訓練。

## Skip Connections 解釋

Skip connections(跳躍連接)是將輸入直接加到某一層的輸出上的連接。在ResNet中,它將輸入x直接加到F(x)上,形成H(x) = F(x) + x。Skip connections的優點:

1. 允許梯度直接流回早期層,緩解梯度消失問題。
2. 使網絡能夠輕鬆學習恆等函數,有助於訓練非常深的網絡。

## Bottleneck Block 解釋

Bottleneck block是ResNet-50及更深層次ResNet使用的一種特殊的殘差塊。它的結構是:

1. 1x1卷積降維
2. 3x3卷積
3. 1x1卷積升維

使用Bottleneck block的原因:

1. 減少參數量和計算量,使得訓練更深的網絡成為可能。
2. 1x1卷積可以有效地改變通道數,實現降維和升維。

這個實現包含了以下幾個關鍵點:

1. `Bottleneck` 類實現了Bottleneck block,包含三個卷積層和skip connection。
2. `ResNet` 類實現了整個ResNet架構,包括初始卷積層、四個殘差層組、全局平均池化和全連接層。
3. `_make_layer` 方法用於創建包含多個Bottleneck blocks的層。
4. `forward` 方法定義了數據在網絡中的前向傳播路徑。
5. `resnet50()` 函數返回一個配置為50層的ResNet模型。

通過這種方式,我們實現了一個完整的ResNet-50模型,其中包含了Residual Block、Skip Connections和Bottleneck結構。這種設計使得ResNet-50能夠在保持較低計算複雜度的同時,實現深層網絡的有效訓練和優秀性能


### Q: resnet的網絡學習殘差函數是在resnet training跟inference時都會作用到嗎?請舉例residual layer在training跟inference時的每個輸入輸出跟gradient運算並中文詳細解釋

是的,ResNet的殘差學習機制在訓練(training)和推理(inference)階段都會發揮作用。讓我們通過一個具體的例子來詳細解釋殘差層在這兩個階段的運作過程:假設我們有一個簡單的殘差層,其中包含兩個卷積層和一個跳躍連接(skip connection)。訓練階段:

1. 輸入: 假設輸入張量 x 的形狀為 [64, 256, 56, 56] (批次大小, 通道數, 高度, 寬度)
2. 前向傳播:
    
    - 主路徑: x 經過兩個卷積層,得到 F(x)
    - 跳躍連接: x 直接傳遞
    - 輸出: y = F(x) + x
    
3. 反向傳播:
    
    - 計算損失函數對輸出 y 的梯度: ∂L/∂y
    - 梯度分流:
        
        - 主路徑梯度: ∂L/∂F(x) = ∂L/∂y
        - 跳躍連接梯度: ∂L/∂x = ∂L/∂y + ∂L/∂F(x) * ∂F(x)/∂x
        
    
    這裡的關鍵是,跳躍連接允許梯度直接流回早期層,緩解了梯度消失問題。

推理階段:

1. 輸入: 同訓練階段
2. 前向傳播:
    
    - 主路徑: x 經過兩個卷積層,得到 F(x)
    - 跳躍連接: x 直接傳遞
    - 輸出: y = F(x) + x
    
3. 沒有反向傳播

在兩個階段中,殘差學習的核心思想都得到了體現:

1. 網絡學習殘差函數 F(x),而不是直接學習目標映射 H(x)。
2. 跳躍連接使得網絡能夠輕鬆學習恆等映射(當 F(x) ≈ 0 時)。
3. 在訓練階段,殘差結構有助於梯度的流動,使深層網絡更容易訓練。
4. 在推理階段,殘差結構允許信息更直接地從淺層傳遞到深層,保留了更多的原始特徵。

這種設計使得 ResNet 能夠訓練非常深的網絡,同時在推理時保持高效的特徵提取和傳遞。殘差學習不僅幫助網絡更容易學習複雜的函數,還在整個網絡中保持了強大的特徵表示能力。

### 1. 殘差塊的基本結構

假設一個簡單的**殘差塊（Residual Block）**結構如下：

y=F(x)+xy = F(x) + xy=F(x)+x

其中：

- xxx 是殘差塊的輸入。
- F(x)F(x)F(x) 是卷積層和非線性激活（ReLU）層的組合輸出，表示網絡學習的殘差函數。
- yyy 是殘差塊的最終輸出。

在這個結構中，F(x)F(x)F(x) 是通過兩層卷積操作和ReLU激活得到的：

1. 第一層：卷積 + 批量歸一化（Batch Normalization, BN） + ReLU激活
2. 第二層：卷積 + 批量歸一化（無ReLU）

### 2. 訓練（Training）階段的運算步驟

在訓練過程中，殘差塊會進行前向傳播和反向傳播，以學習權重的梯度。以下是訓練時的詳細運算步驟：

#### 前向傳播（Forward Pass）

假設：

- 輸入 x=x0x = x_0x=x0​
- 卷積核的權重為 W1W_1W1​ 和 W2W_2W2​
- BN層的縮放係數為 γ1,γ2\gamma_1, \gamma_2γ1​,γ2​ 和偏移量 β1,β2\beta_1, \beta_2β1​,β2​

前向傳播過程如下：

1. **第一層卷積**：x1=W1∗x0+b1x_1 = W_1 * x_0 + b_1x1​=W1​∗x0​+b1​
2. **第一層BN和ReLU**：x2=ReLU(γ1⋅BN(x1)+β1)x_2 = \text{ReLU}(\gamma_1 \cdot \text{BN}(x_1) + \beta_1)x2​=ReLU(γ1​⋅BN(x1​)+β1​)
3. **第二層卷積**：x3=W2∗x2+b2x_3 = W_2 * x_2 + b_2x3​=W2​∗x2​+b2​
4. **第二層BN**：F(x)=γ2⋅BN(x3)+β2F(x) = \gamma_2 \cdot \text{BN}(x_3) + \beta_2F(x)=γ2​⋅BN(x3​)+β2​
5. **殘差塊輸出**：y=F(x)+x0y = F(x) + x_0y=F(x)+x0​

#### 反向傳播（Backward Pass）

在反向傳播中，通過計算損失對輸出的偏導數，將梯度逐層傳遞回去，以更新權重。

假設損失函數為 LLL，那麼：

1. **殘差加法的梯度計算**：
    
    - 對於輸出 y=F(x)+x0y = F(x) + x_0y=F(x)+x0​，其梯度為： ∂L∂y=∂L∂F(x)+∂L∂x0\frac{\partial L}{\partial y} = \frac{\partial L}{\partial F(x)} + \frac{\partial L}{\partial x_0}∂y∂L​=∂F(x)∂L​+∂x0​∂L​
    - 這裡梯度直接分別傳遞到 F(x)F(x)F(x) 和 x0x_0x0​ 的分支，這樣即使在很深的網絡中，梯度仍然能夠直接傳到輸入 x0x_0x0​。
2. **反向傳播到第二層卷積**：
    
    - 對於 F(x)F(x)F(x) 的梯度 ∂L∂F(x)\frac{\partial L}{\partial F(x)}∂F(x)∂L​，將反向傳播到 x3x_3x3​： ∂L∂x3=∂L∂F(x)⋅γ2\frac{\partial L}{\partial x_3} = \frac{\partial L}{\partial F(x)} \cdot \gamma_2∂x3​∂L​=∂F(x)∂L​⋅γ2​
    - 然後繼續向前傳播以更新 W2W_2W2​ 的權重。
3. **反向傳播到第一層卷積**：
    
    - 在此過程中，對於每層權重 W1W_1W1​ 和 W2W_2W2​ 進行權重更新，最終完成殘差函數的學習。

---

### 3. 推理（Inference）階段的運算步驟

在推理階段，由於不再進行反向傳播，只需進行前向傳播即可。在這一過程中，BN層的均值和方差會使用訓練中得到的移動平均值，不再依賴於當前批次的數據。因此，輸入輸出步驟與訓練過程的前向傳播部分一致：

1. **第一層卷積**：x1=W1∗x0+b1x_1 = W_1 * x_0 + b_1x1​=W1​∗x0​+b1​
2. **第一層BN和ReLU**：x2=ReLU(γ1⋅BN(x1)+β1)x_2 = \text{ReLU}(\gamma_1 \cdot \text{BN}(x_1) + \beta_1)x2​=ReLU(γ1​⋅BN(x1​)+β1​)
3. **第二層卷積**：x3=W2∗x2+b2x_3 = W_2 * x_2 + b_2x3​=W2​∗x2​+b2​
4. **第二層BN**：F(x)=γ2⋅BN(x3)+β2F(x) = \gamma_2 \cdot \text{BN}(x_3) + \beta_2F(x)=γ2​⋅BN(x3​)+β2​
5. **殘差塊輸出**：y=F(x)+x0y = F(x) + x_0y=F(x)+x0​

在這裡，BN層不再計算每批數據的均值和方差，而是使用訓練階段的移動平均值，從而在推理過程中提供穩定的輸出。

---

### 4. 舉例說明輸入、輸出和梯度運算

假設：

- 初始輸入 x0=1.0x_0 = 1.0x0​=1.0
- 卷積核 W1,W2W_1, W_2W1​,W2​ 初始化為隨機值
- 損失函數 LLL 對於輸出 yyy 的梯度為1（假設）

在訓練階段的反向傳播中：

1. **殘差梯度傳遞**：
    
    - ∂L∂y=1\frac{\partial L}{\partial y} = 1∂y∂L​=1
    - 對應於 F(x)F(x)F(x) 和 x0x_0x0​ 的梯度分別為 ∂L∂F(x)=1\frac{\partial L}{\partial F(x)} = 1∂F(x)∂L​=1 和 ∂L∂x0=1\frac{\partial L}{\partial x_0} = 1∂x0​∂L​=1。
2. **反向傳播至第二層卷積**：
    
    - 計算 ∂L∂x3\frac{\partial L}{\partial x_3}∂x3​∂L​ 並更新 W2W_2W2​。
3. **反向傳播至第一層卷積**：
    
    - 計算 ∂L∂x1\frac{\partial L}{\partial x_1}∂x1​∂L​ 並更新 W1W_1W1​。

在推理階段，由於不涉及梯度計算，直接依賴已學習的 W1W_1W1​、W2W_2W2​、γ\gammaγ、β\betaβ 參數進行前向傳播。

---

總結來說，**ResNet在訓練和推理階段都會使用殘差結構**。在訓練階段，殘差函數的學習涉及梯度計算和反向傳播；在推理階段，則使用已學習的參數進行推理，使得殘差結構能夠更高效地提取特徵，減少梯度消失的問題，特別適合訓練深層網絡。