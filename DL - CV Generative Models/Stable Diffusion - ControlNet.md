[Controlnet.ipynb](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/controlnet.ipynb)

è‡ªå¾Stable Diffusionå¸­æ²å…¨çƒä»¥ä¾†ï¼Œäººå€‘ä¸€ç›´åœ¨å°‹æ‰¾å°ç”Ÿæˆéç¨‹çµæœæœ‰æ›´å¤šæ§åˆ¶çš„æ–¹æ³•ã€‚ ControlNet æä¾›äº†ä¸€å€‹æœ€å°çš„ä»‹é¢ï¼Œå…è¨±ä½¿ç”¨è€…åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šè‡ªè¨‚ç”Ÿæˆéç¨‹ã€‚é€é ControlNetï¼Œä½¿ç”¨è€…å¯ä»¥è¼•é¬†åœ°ä½¿ç”¨ä¸åŒçš„ç©ºé–“æƒ…å¢ƒï¼ˆä¾‹å¦‚æ·±åº¦åœ–depth mapã€åˆ†â€‹â€‹å‰²åœ–segmentation mapã€å¡—é´‰scribbleã€é—œéµé»keypointsç­‰ï¼‰ä¾†èª¿ç¯€ç”Ÿæˆï¼

å¼•å…¥äº†ä¸€å€‹æ¡†æ¶frameworkï¼Œå…è¨±æ”¯æ´å„ç¨®ç©ºé–“ä¸Šä¸‹æ–‡spatial contextsï¼Œé€™äº›ç©ºé–“ä¸Šä¸‹æ–‡å¯ä»¥ä½œç‚ºæ“´æ•£æ¨¡å‹ï¼ˆä¾‹å¦‚ç©©å®šæ“´æ•£ï¼‰çš„é™„åŠ æ¢ä»¶additional conditioningsã€‚


|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **[CASE 1] Basic - Controlnet + SD**<br><br>1. Import<br>from diffusers import StableDiffusionControlNetPipeline, ControlNetModel<br><br>2. Input Image<br>canny_image = cv2.Canny(image, low_th, high_th)<br><br>3. Load ControlNet<br>controlnet = ControlNetModel.from_pretrained("==sd-controlnet-canny==")<br><br>4. Crate pipeline<br>pipe = StableDiffusionControlNetPipeline.from_pretrained(<br>    "stable-diffusion-v1-5", controlnet)<br><br>5. Run the pipeline<br>output = pipe(prompt, canny_image, negative_prompt, generator, steps)<br><br>                                                                                                            |
| **[CASE 2] Finetune - ç”¨finetuneçš„diffusion model** <br><br>1. Import<br>from diffusers import StableDiffusionControlNetPipeline, ControlNetModel<br><br>2. Input Image<br>canny_image = cv2.Canny(image, low_th, high_th)<br><br>3. Load ControlNet<br>controlnet = ControlNetModel.from_pretrained("==sd-controlnet-canny==")<br><br>4. Crate pipeline<br>pipe = StableDiffusionControlNetPipeline.from_pretrained(<br>    =="mr-potato-head"==, controlnet)<br><br>5. Run the pipeline<br>output = pipe(prompt, canny_image, negative_prompt, generator, steps)<br>                                                                                                     |
| **[CASE 3] Open pose controlnet** <br><br>1. Import<br>from diffusers import StableDiffusionControlNetPipeline, ControlNetModel<br>==from controlnet_aux import OpenposeDetector==<br><br>2. Input Image<br>==model = OpenposeDetector.from_pretrained("lllyasviel/ControlNet")==<br>==poses = model(image)==<br><br>3. Load ControlNet<br>controlnet = ControlNetModel.from_pretrained("==stable-diffusion-v1-5-controlnet-openpose==")<br><br>4. Crate pipeline<br>pipe = StableDiffusionControlNetPipeline.from_pretrained(<br>    "stable-diffusion-v1-5", controlnet)<br><br>5. Run the pipeline<br>output = pipe(prompt, poses, negative_prompt, generator, steps) |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |



è¨“ç·´ ControlNet åŒ…å«ä»¥ä¸‹æ­¥é©Ÿï¼š

1. Cloning Diffusionæ¨¡å‹çš„é è¨“ç·´åƒæ•¸(pre-trained parameters)ï¼Œä¾‹å¦‚ Stable Diffusion çš„latent UNetï¼ˆç¨±ç‚ºã€Œå¯è¨“ç·´å‰¯æœ¬ã€(trainable copy)ï¼‰ï¼ŒåŒæ™‚ä¹Ÿå–®ç¨ç¶­è­·é è¨“ç·´åƒæ•¸ï¼ˆã€Œé–å®šå‰¯æœ¬ã€(locked copy)ï¼‰ã€‚é€™æ¨£åšæ˜¯ç‚ºäº†ä½¿é–å®šçš„åƒæ•¸å‰¯æœ¬(locked parameter copy)å¯ä»¥ä¿ç•™å¾å¤§å‹è³‡æ–™é›†ä¸­å­¸åˆ°çš„å¤§é‡çŸ¥è­˜ï¼Œè€Œå¯è¨“ç·´çš„å‰¯æœ¬(trainable copy)å‰‡ç”¨æ–¼å­¸ç¿’ç‰¹å®šæ–¼ä»»å‹™çš„æ–¹é¢ã€‚

2. åƒæ•¸çš„å¯è¨“ç·´(trainable copy)å’Œé–å®šå‰¯æœ¬(locked parameter copy)é€éã€Œé›¶å·ç©ã€å±¤(zero convolution layers)ï¼Œé€™äº›å±¤ä½œç‚º ControlNet æ¡†æ¶çš„ä¸€éƒ¨åˆ†é€²è¡Œäº†æœ€ä½³åŒ–ã€‚é€™æ˜¯ä¸€ç¨®è¨“ç·´æŠ€å·§(training trick)ï¼Œç”¨æ–¼åœ¨è¨“ç·´æ–°æ¢ä»¶æ™‚ä¿ç•™å‡çµæ¨¡å‹(frozen model)å·²ç¶“å­¸åˆ°çš„èªç¾©(semantics)ã€‚

æ¯ä¸€ç¨®æ–°çš„èª¿ç¯€é¡å‹(conditioning)éƒ½éœ€è¦è¨“ç·´ä¸€å€‹æ–°çš„ControlNet weightsã€‚è«–æ–‡æå‡ºäº† 8 ç¨®ä¸åŒçš„èª¿ç¯€æ¨¡å‹(conditioning models)ï¼ŒDiffusers éƒ½æ”¯æŒé€™äº›æ¨¡å‹ï¼ç‚ºäº†é€²è¡Œæ¨ç†ï¼Œéœ€è¦é å…ˆè¨“ç·´çš„æ“´æ•£æ¨¡å‹æ¬Šé‡(pre-trained diffusion models weights)ä»¥åŠç¶“éè¨“ç·´çš„ ControlNet æ¬Šé‡(trained ControlNet weights)ã€‚ä¾‹å¦‚ï¼Œèˆ‡åƒ…ä½¿ç”¨åŸå§‹ç©©å®šæ“´æ•£æ¨¡å‹ç›¸æ¯”ï¼Œå°‡Stable Diffusion v1-5 èˆ‡ ControlNet æª¢æŸ¥é»çµåˆä½¿ç”¨å¤§ç´„éœ€è¦ 7 å„„å€‹åƒæ•¸ï¼Œé€™ä½¿å¾— ControlNet çš„æ¨ç†è¨˜æ†¶é«”æ¶ˆè€—æ›´å¤§ã€‚ç”±æ–¼åœ¨è¨“ç·´éç¨‹ä¸­æœƒæŸ¥çœ‹é å…ˆè¨“ç·´çš„æ“´æ•£æ¨¡å‹(pre-trained diffusion models)ï¼Œå› æ­¤åœ¨ä½¿ç”¨ä¸åŒçš„æ¢ä»¶æ™‚åªéœ€åˆ‡æ› ControlNet åƒæ•¸ã€‚é€™ä½¿å¾—åœ¨ä¸€å€‹æ‡‰ç”¨ç¨‹å¼ä¸­éƒ¨ç½²å¤šå€‹ ControlNet æ¬Šé‡è®Šå¾—ç›¸ç•¶ç°¡å–®ï¼Œ

### The StableDiffusionControlNetPipeline

#### 1. Import 
```python hlredt:ControlNetModel hlwhitet:pipe hlbluet:prompt
pip install -q diffusers==0.14.0 transformers xformers git+https://github.com/huggingface/accelerate.git

pip install -q opencv-contrib-python
pip install -q controlnet_aux

# Import
from diffusers import StableDiffusionControlNetPipeline
from diffusers.utils import load_image
```
To process different conditionings depending on the chosen ControlNet, we also need to install some additional dependencies:
- [OpenCV](https://www.google.com/url?q=https%3A%2F%2Fopencv.org%2F)
- [controlnet-aux](https://github.com/patrickvonplaten/controlnet_aux#controlnet-auxiliary-models)Â - a simple collection of pre-processing models for ControlNet

#### 2. Load controlnet and stable diffusion pipeline
```python hlredt:ControlNetModel hlwhitet:pipe hlbluet:prompt
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel
import torch

# controlnet
controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny", torch_dtype=torch.float16)

# stable diffusion pipeline
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16)
)
```


#### 3. The scheduler and optimize
```python hlredt:hlredt:ControlNetModel hlwhitet:pipe hlbluet:prompt
from diffusers import UniPCMultistepScheduler

pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)

# smart CPU offloading
pipe.enable_model_cpu_offload()

# attention layer acceleration
pipe.enable_xformers_memory_efficient_attention()
```
æˆ‘å€‘ä¸ä½¿ç”¨ Stable Diffusion çš„é è¨­ PNDMSchedulerï¼Œè€Œæ˜¯ä½¿ç”¨ç›®å‰æœ€å¿«çš„æ“´æ•£æ¨¡å‹èª¿åº¦ç¨‹åºä¹‹ä¸€ï¼Œç¨±ç‚º UniPCMultistepSchedulerã€‚é¸æ“‡æ”¹é€²çš„èª¿åº¦ç¨‹åºå¯ä»¥å¤§å¤§æ¸›å°‘æ¨ç†æ™‚é–“ - åœ¨æˆ‘å€‘çš„ä¾‹å­ä¸­ï¼Œæˆ‘å€‘èƒ½å¤ å°‡æ¨ç†æ­¥é©Ÿæ•¸å¾ 50 å€‹æ¸›å°‘åˆ° 20 å€‹ï¼ŒåŒæ™‚æˆ–å¤šæˆ–å°‘ä¿æŒç›¸åŒçš„åœ–åƒç”Ÿæˆå“è³ªã€‚

æˆ‘å€‘ä¸æ˜¯å°‡ç®¡é“ç›´æ¥è¼‰å…¥åˆ° GPUè€Œæ˜¯å•Ÿç”¨æ™ºæ…§ CPU å¸è¼‰, é€™å¯ä»¥é€é enable_model_cpu_offload å‡½æ•¸ä¾†å¯¦ç¾ã€‚åœ¨ä½¿ç”¨ ControlNet é€²è¡Œç©©å®šæ“´æ•£çš„æƒ…æ³ä¸‹ï¼Œæˆ‘å€‘é¦–å…ˆä½¿ç”¨ CLIP æ–‡å­—ç·¨ç¢¼å™¨ï¼Œç„¶å¾Œä½¿ç”¨æ“´æ•£æ¨¡å‹unet å’Œæ§è£½ç¶²çµ¡ï¼Œç„¶å¾Œä½¿ç”¨ VAE è§£ç¢¼å™¨ï¼Œæœ€å¾Œé‹è¡Œå®‰å…¨æª¢æŸ¥å™¨

å¤§å¤šæ•¸çµ„ä»¶åœ¨æ“´æ•£éç¨‹ä¸­åªé‹è¡Œä¸€æ¬¡ï¼Œå› æ­¤ä¸éœ€è¦ä¸€ç›´ä½”ç”¨ GPU è¨˜æ†¶é«”ã€‚é€éå•Ÿç”¨æ™ºæ…§æ¨¡å‹å¸è¼‰ï¼Œæˆ‘å€‘ç¢ºä¿æ¯å€‹å…ƒä»¶åƒ…åœ¨éœ€è¦æ™‚æ‰è¼‰å…¥åˆ° GPU ä¸­ï¼Œé€™æ¨£æˆ‘å€‘å°±å¯ä»¥é¡¯è‘—ç¯€çœè¨˜æ†¶é«”æ¶ˆè€—ï¼Œè€Œä¸æœƒé¡¯è‘—æ¸›æ…¢æ”»æ“Šé€Ÿåº¦

#### 4. run the ControlNet pipeline
```python hlredt:ControlNetModel hlwhitet:pipe hlbluet:prompt

prompt = ", best quality, extremely detailed"
prompt = [t + prompt for t in ["Sandra Oh", "Kim Kardashian", "rihanna", "taylor swift"]]
generator = [torch.Generator(device="cpu").manual_seed(2) for i in range(len(prompt))]

output = pipe(prompt,canny_image,
    negative_prompt=["monochrome, lowres, bad anatomy, worst quality, low quality"] * len(prompt),
    generator=generator,
    num_inference_steps=20,)

```
prompt = [
'Sandra Oh, best quality, extremely detailed',
'Kim Kardashian, best quality, extremely detailed',
'rihanna, best quality, extremely detailed', 
'taylor swift, best quality, extremely detailed']

æˆ‘å€‘ä»ç„¶æä¾›æç¤º(prompt)ä¾†æŒ‡å°å½±åƒç”Ÿæˆéç¨‹(image generation process)ï¼Œå°±åƒæˆ‘å€‘é€šå¸¸ä½¿ç”¨ç©©å®šæ“´æ•£å½±åƒåˆ°å½±åƒç®¡é“(Stable Diffusion image-to-image pipeline)æ‰€åšçš„é‚£æ¨£ã€‚ç„¶è€Œï¼ŒControlNet å°‡å…è¨±å°ç”Ÿæˆçš„åœ–åƒé€²è¡Œæ›´å¤šçš„æ§åˆ¶ï¼Œå› ç‚ºæˆ‘å€‘å°‡èƒ½å¤ ä½¿ç”¨æˆ‘å€‘å‰›å‰›å‰µå»ºçš„ç²¾æ˜é‚Šç·£åœ–åƒ(canny edge image)ä¾†æ§åˆ¶ç”Ÿæˆåœ–åƒçš„ç²¾ç¢ºçµ„æˆã€‚

çœ‹åˆ°ä¸€äº›ç•¶ä»£åäººç‚ºé€™å¹… 17 ä¸–ç´€çš„ç•«ä½œæ“ºå‡ºå§¿å‹¢çš„ç…§ç‰‡å°‡æœƒå¾ˆæœ‰è¶£ã€‚ä½¿ç”¨ ControlNet åšåˆ°é€™ä¸€é»éå¸¸å®¹æ˜“ï¼Œæˆ‘å€‘æ‰€è¦åšçš„å°±æ˜¯åœ¨æç¤ºä¸­åŒ…å«é€™äº›åäººçš„åå­—ï¼

#### 5. ControlNet with finetune
```python hlredt:ControlNetModel hlwhitet:pipe hlbluet:prompt
model_id = "sd-dreambooth-library/mr-potato-head"
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    model_id,
    controlnet=controlnet,
    torch_dtype=torch.float16,
)
pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()
pipe.enable_xformers_memory_efficient_attention()

generator = torch.manual_seed(2)
prompt = "a photo of sks mr potato head, best quality, extremely detailed"
output = pipe(
    prompt,
    canny_image,
    negative_prompt="monochrome, lowres, bad anatomy, worst quality, low quality",
    generator=generator,
    num_inference_steps=20,
)
```
æˆ‘å€‘ä¹Ÿå¯ä»¥æ¯«ä¸è²»åŠ›åœ°å°‡ ControlNet èˆ‡å¾®èª¿(fine-tuning)çµåˆï¼ä¾‹å¦‚ï¼Œæˆ‘å€‘å¯ä»¥ä½¿ç”¨fine-tune a model with DreamBoothï¼Œä¸¦ä½¿ç”¨å®ƒå°‡è‡ªå·±æ¸²æŸ“(render)åˆ°ä¸åŒçš„å ´æ™¯ä¸­ã€‚æˆ‘å€‘å¯ä»¥ä½¿ç”¨ç›¸åŒçš„ ContrlNetï¼Œä½†æˆ‘å€‘ä¸ä½¿ç”¨ Stable Diffusion 1.5ï¼Œè€Œæ˜¯å°‡ Mr Potato Head æ¨¡å‹åŠ è¼‰åˆ°æˆ‘å€‘çš„ç®¡é“ä¸­ - Mr Potato Head æ˜¯ä¸€å€‹ä½¿ç”¨ Dreambooth èˆ‡ Mr Potato Head æ¦‚å¿µé€²è¡Œå¾®èª¿çš„ç©©å®šæ“´æ•£æ¨¡å‹ ğŸ¥”


ControlNet çš„å¦ä¸€å€‹ç¨ç‰¹æ‡‰ç”¨æ˜¯ï¼Œæˆ‘å€‘å¯ä»¥å¾ä¸€å¼µåœ–åƒä¸­ç²å–ä¸€å€‹å§¿å‹¢(pose)ï¼Œç„¶å¾Œé‡æ–°ä½¿ç”¨å®ƒä¾†ç”¢ç”Ÿå…·æœ‰å®Œå…¨ç›¸åŒå§¿å‹¢çš„ä¸åŒåœ–åƒã€‚å› æ­¤ï¼Œåœ¨ä¸‹ä¸€å€‹ç¯„ä¾‹ä¸­ï¼Œæˆ‘å€‘å°‡æ•™å°è¶…ç´šè‹±é›„å¦‚ä½•ä½¿ç”¨ Open Pose ControlNet é€²è¡Œç‘œçˆï¼
#### 6. Open Pose ControlNet
```python hlredt:ControlNetModel hlwhitet:pipe hlbluet:prompt
# Get yoga images for pose
urls = "yoga1.jpeg", "yoga2.jpeg", "yoga3.jpeg", "yoga4.jpeg"
imgs = [
Â  Â  load_image("https://hf.co/datasets/YiYiXu/controlnet-testing/resolve/main/" + url)
Â  Â  for url in urls]

# extract yoga poses using the OpenPose pre-processors
from controlnet_aux import OpenposeDetector

model = OpenposeDetector.from_pretrained("lllyasviel/ControlNet")
poses = [model(img) for img in imgs]

# Load controlnet
controlnet = ControlNetModel.from_pretrained(
    "fusing/stable-diffusion-v1-5-controlnet-openpose", torch_dtype=torch.float16
)
# Create StableDiffusionControlNetPipeline
model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    model_id,
    controlnet=controlnet,
    torch_dtype=torch.float16,
)
pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()
pipe.enable_xformers_memory_efficient_attention()

# Run the pipeline
generator = [torch.Generator(device="cpu").manual_seed(2) for i in range(4)]
prompt = "super-hero character, best quality, extremely detailed"
output = pipe(
    [prompt] * 4,
    poses,
    negative_prompt=["monochrome, lowres, bad anatomy, worst quality, low quality"] * 4,
    generator=generator,
    num_inference_steps=20,
)
```