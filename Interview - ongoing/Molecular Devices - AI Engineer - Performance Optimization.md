
AI Engineer - Performance Optimization  
  
Location:  Bellevue, WA  
Salary: $130k - 150k  
  
2025.04.07 [HR 30min] 11:30am Keith Crawford  
2025.04.17 [Tech 60min] 4:00pm Sucharitha Aekula, Michael Grinberg

|                                      |     |
| ------------------------------------ | --- |
| [[###職位描述 Performance Optimization]] |     |
| [[###分散式訓練 (Distributed Training)]]  |     |
|                                      |     |

https://www.linkedin.com/jobs/view/4164541790/?refId=eb35c9ae-8a43-4636-8ddf-e5c9fb976ef1&trackingId=SPDrFIBNTEect9qCGgAk1g%3D%3D&trk=flagship3_job_home_savedjobs

We are seeking an experienced **AI Engineer - Performance Optimization** specializing in <mark style="background: #BBFABBA6;">GPU programming</mark> to optimize AI applications for <mark style="background: #BBFABBA6;">high-performance computing</mark> in the life sciences. The ideal candidate will drive the development of industrial-grade applications by leveraging advanced machine learning and computer vision techniques to analyze complex biological images. You will play a key role in <mark style="background: #ADCCFFA6;">architecting, developing, and optimizing software solutions</mark> that meet the demanding performance requirements of the life science industry.  我們正在尋找一位經驗豐富的 AI 工程師 - 性能優化，專門從事 GPU 編程以優化生命科學領域高效能運算的 AI 應用程式。理想的候選人將利用先進的機器學習和電腦視覺技術來分析複雜的生物影像，從而推動工業級應用的發展。您將在架構、開發和優化滿足生命科學產業嚴格效能要求的軟體解決方案方面發揮關鍵作用。 
  
This position reports to the Manager, Software Engineering and is part of the R&D-Team located in Bellevue, Seattle and will be an on-site role.  該職位向軟體工程經理匯報，是位於西雅圖貝爾維尤的研發團隊的一部分，並將擔任現場職務。
  
**In this role, you will have the opportunity to:  

- Software Development & Architecture: <mark style="background: #FFF3A3A6;">Design, develop, and maintain high-performance machine learning and computer vision</mark> applications tailored for life science imaging challenges. Architect robust, scalable, high-performance, and maintainable software systems that integrate seamlessly with existing industrial workflows. 軟體開發和架構：設計、開發和維護針對生命科學成像挑戰的高效能機器學習和電腦視覺應用程式。建構強大、可擴展、高效且可維護的軟體系統，與現有工業工作流程無縫整合。
- Machine Learning & Computer Vision: Develop and implement state-of-the-art algorithms (e.g., convolutional neural networks, object detection, segmentation) to process and analyze biological image data. Optimize existing machine learning models and computer vision pipelines for performance, accuracy, and scalability. 機器學習與電腦視覺：開發和實施最先進的演算法（例如捲積神經網路、物件偵測、分割）來處理和分析生物影像資料。優化現有的機器學習模型和電腦視覺管道的性能、準確性和可擴展性。
- Performance Optimization: Identify <mark style="background: #BBFABBA6;">performance bottlenecks</mark> in software and implement optimization strategies to enhance processing speed and efficiency. <mark style="background: #FF5582A6;">Custom kernel development by write and optimize custom CUDA kernels for specialized AI and deep learning workloads</mark>. <mark style="background: #ADCCFFA6;">Develop algorithms for large-scale data processing, model predection and model</mark> training. 效能優化：識別軟體中的效能瓶頸並實施最佳化策略，以提高處理速度和效率。透過編寫和優化自訂 CUDA 內核來開發客製化內核，以滿足專門的 AI 和深度學習工作負載的需求。開發大規模資料處理、模型預測和模型訓練的演算法。
- Industrial Application Development: Ensure the software meets industrial standards for reliability, security, and compliance with regulatory requirements specific to the life science domain. Develop comprehensive documentation, including technical specifications, user guides, and maintenance procedures. 工業應用開發：確保軟體符合可靠性、安全性的工業標準，並遵守生命科學領域特定的監管要求。制定全面的文檔，包括技術規格、使用者指南和維護程序。
- Collaboration: Work closely with AI research teams and other stakeholders to understand user requirements and translate them into technical solutions. Contribute to a culture of continuous improvement by sharing best practices, tools, and techniques across the team.  合作：與人工智慧研究團隊和其他利害關係人密切合作，了解用戶需求並將其轉化為技術解決方案。透過在團隊中分享最佳實踐、工具和技術來促進持續改進的文化。 


**The essential requirements of the job include:  

- Education: Bachelor’s degree in Computer Science, Engineering, or a related field; Master’s or Ph.D. preferred. 教育背景：計算機科學、工程或相關領域的學士學位；碩士或博士學位首選。
- Experience: 7+ years of professional software development experience, with a focus on industrial application development and <mark style="background: #FFF3A3A6;">machine learning/computer vision solution</mark>s. 經驗：7年以上專業軟體開發經驗，專注於產業應用開發和機器學習/電腦視覺解決方案。
- Technical Skills: Proficiency in Python and C/C++, experience with machine learning frameworks (e.g., TensorFlow, PyTorch), and strong understanding of computer vision algorithms. 技術技能：精通 Python 和 C/C++，具有機器學習框架（例如 TensorFlow、PyTorch）經驗，並對電腦視覺演算法有深入的理解。
- Domain Knowledge: Understanding of life science imaging techniques and knowledge of regulatory standards and data privacy requirements in the life sciences. 領域知識：了解生命科學成像技術並了解生命科學中的監管標準和資料隱私要求。
- Soft Skills: Excellent problem-solving skills, strong communication and collaboration abilities, and the ability to manage multiple projects concurrently.  軟技能：出色的解決問題能力、強大的溝通和協作能力以及同時管理多個專案的能力。

**It would be a plus if you also possess previous experience in:  

- Experience in deploying machine learning models in production environments. 在生產環境中部署機器學習模型的經驗。
- Publications or contributions to open-source projects in computer vision or machine learning. 電腦視覺或機器學習領域的開源專案的出版或貢獻。
- Previous experience in a regulated industry such as healthcare or pharmaceuticals. 具有醫療保健或製藥等受監管行業的經驗。



## 🧬 一、Molecular Devices 公司介紹

**Molecular Devices** 是一家成立於 1983 年的 **美國生物科技公司**，隸屬於全球知名的 Danaher 集團。該公司專門提供 **高通量成像（High-throughput imaging）與細胞分析設備**，支援藥物開發、生物製劑、癌症研究與再生醫學等領域。

### 公司主要產品與技術領域：

1. **細胞影像系統（Cell Imaging Systems）**
    
    - 例如 ImageXpress 系列，提供自動顯微鏡、高解析圖像分析。
        
2. **多功能微孔板閱讀器（Microplate Readers）**
    
    - 應用於光學、螢光、螢光共振能量轉移等實驗。
        
3. **自動化軟體**
    
    - 包含 AI 加速的圖像分析、數據視覺化、機器學習建模。
        

### 客戶對象：

- 生技製藥公司
    
- 生物醫學實驗室
    
- 醫學研究機構
    
- 高等學府實驗單位
    

---

## 👨‍💻 二、AI Engineer - Performance Optimization 職缺內容

這個職缺的工作地點是位於 **華盛頓州 Bellevue 的 R&D 團隊**，屬於 **軟體研發部門的核心職位**。你將會專注在以下幾個核心任務：

### 📌 核心職責：

1. **使用 GPU 加速來最佳化 AI 模型的推理與訓練效能**
    
    - 包括 TensorRT、CUDA、ONNX Runtime 等技術。
        
2. **提升影像分析與機器學習演算法的運行效率**
    
    - 特別針對生物醫學影像（如細胞影像、組織切片、螢光影像）進行加速與優化。
        
3. **與硬體工程師和應用科學家合作**
    
    - 整合影像擷取設備與 AI 分析演算法，提升整體生產流程與準確性。
        
4. **開發可部署於產品端的工業級 AI 解決方案**
    
    - 包括部署到儀器上的嵌入式 GPU 或雲端平台。
        

---

## 🧠 三、應該具備的技能與經驗

|技術類別|詳細技能與工具|
|---|---|
|**GPU 加速與優化**|CUDA、cuDNN、TensorRT、ONNX Runtime、NVIDIA Nsight|
|**深度學習框架**|PyTorch、TensorFlow、Keras|
|**影像處理經驗**|OpenCV、scikit-image，熟悉細胞/組織影像分析最佳|
|**C++/Python 程式能力**|實作可部署程式碼，跨平台與跨模組協作能力|
|**系統效能剖析工具**|NVIDIA Nsight、VTune、perf 等工具經驗佳|
|**部署與編譯優化**|熟悉模型壓縮、量化（quantization）、融合（fusion）等技術|

### 加分條件：

- 有 **醫療影像分析** 經驗或了解生醫領域資料特性（如細胞分割、螢光影像分類）
    
- 熟悉工業級產品開發流程（包含測試、版本控制、部署）
    
- 有使用過 **ImageXpress** 或其他高通量影像設備經驗尤佳
    

---

## 🎯 四、面試流程（可能會經歷以下幾個階段）

### 1. HR 初步電話訪談（30 分鐘）

- 自我介紹、了解你是否熟悉生醫領域與 GPU 優化。
    
- 問你為什麼對這個職位感興趣。
    
- 評估是否具備合法工作身份。
    

### 2. 技術電話面試（1 小時）

- 與資深工程師進行一對一討論，主題可能包含：
    
    - 如何用 TensorRT 優化 PyTorch 模型？
        
    - 如何使用 CUDA 開發高效能影像處理 pipeline？
        
    - 實際經驗：講述你處理過最具挑戰性的效能問題。
        

### 3. Take-home assignment（視情況而定）

- 要你優化一段深度學習模型的推論時間，可能提供細胞影像或合成資料進行測試。
    

### 4. Onsite 面試（或 virtual onsite，4 小時左右）

- 技術簡報：你要簡報一個過去的專案，並強調如何優化效能。
    
- Coding interview：白板或 live coding，測試演算法、程式邏輯。
    
- 團隊面談：與產品經理、科學家、QA 團隊交流，看你是否能跨部門溝通。
    

---

## 🔚 小結

| 項目   | 說明                                              |
| ---- | ----------------------------------------------- |
| 公司   | Molecular Devices（隸屬 Danaher）                   |
| 職位   | AI Engineer - Performance Optimization          |
| 工作內容 | GPU 加速、影像分析 AI 模型優化、部署工業級 AI                    |
| 產品   | ImageXpress 細胞影像系統、螢光顯微鏡分析平台                    |
| 技能   | CUDA、TensorRT、PyTorch、影像處理、C++/Python           |
| 面試流程 | HR → 技術電話 → take-home → Onsite 或 virtual onsite |






### 職位描述 Performance Optimization

根據您提供的職位描述，針對 Performance Optimization 部分，以下將詳細介紹所需的技術和技能：

**Performance Optimization (效能優化)**

- Performance Optimization: 
- Identify <mark style="background: #BBFABBA6;">performance bottlenecks</mark> in software and implement optimization strategies to enhance processing speed and efficiency. 
- Custom kernel development by write and optimize custom <mark style="background: #BBFABBA6;">CUDA kernels</mark> for specialized AI and deep learning workloads. 
- Develop algorithms for<mark style="background: #BBFABBA6;"> large-scale data processing</mark>, model prediction and model training.

這個部分是這個 AI 工程師職位的核心職責之一，目標是提升 AI 應用程式在高效能運算環境下的執行效率。具體來說，需要掌握以下技術和技能：

|                                                  |     |
| ------------------------------------------------ | --- |
| [[###NVIDIA Nsight Systems 和 Nsight Compute 簡介]] |     |
| 分散式訓練 (Distributed Training)                     |     |
|                                                  |     |
|                                                  |     |



**1. 效能瓶頸識別與分析 (Performance Bottleneck Identification and Analysis):**

- **系統層級監控與分析工具 (System-level Monitoring and Analysis Tools):**
    
    - **CPU 和記憶體分析 (CPU and Memory Profiling):** 熟悉使用如 `top`、`htop`、`vmstat`、`sar` 等 Linux 系統工具，以及 Windows 上的效能監視器，能夠監控 CPU 使用率、記憶體使用情況、磁碟 I/O、網路 I/O 等關鍵指標，判斷效能瓶頸是否出現在硬體資源上。
    - **GPU 監控工具 (GPU Monitoring Tools):** 熟悉使用如 `nvidia-smi`、`nvtop` 等工具，監控 GPU 的利用率、記憶體使用情況、溫度等，了解 GPU 是否成為效能瓶頸。
    - **應用程式效能分析工具 (Application Performance Analysis Tools):** 熟悉使用如 Intel VTune Profiler、AMD μProf、NVIDIA Nsight Systems/Compute 等專業效能分析工具，能夠深入分析應用程式的執行時間、函數調用堆疊、熱點函數等，精確定位效能瓶頸的程式碼位置。
    - **Python Profiling 工具 (Python Profiling Tools):** 熟悉使用如 `cProfile`、`line_profiler`、`memory_profiler` 等 Python 效能分析工具，針對 Python 程式碼進行細粒度的效能分析。
- **效能分析方法論 (Performance Analysis Methodologies):**
    
    - **根本原因分析 (Root Cause Analysis):** 能夠系統性地分析效能問題，找出導致瓶頸的根本原因，而不僅僅是表面現象。
    - **瓶頸類型識別 (Bottleneck Type Identification):** 能夠識別不同類型的效能瓶頸，例如 CPU bound、GPU bound、Memory bound、I/O bound 等，並針對不同類型採取不同的優化策略。

**2. 優化策略實施 (Implementation of Optimization Strategies):**

- **演算法優化 (Algorithm Optimization):**
    
    - **時間複雜度和空間複雜度分析 (Time and Space Complexity Analysis):** 能夠分析演算法的時間複雜度和空間複雜度，選擇更有效率的演算法來解決問題。
    - **演算法重構 (Algorithm Refactoring):** 能夠根據效能分析結果，對現有演算法進行重構，以減少計算量或記憶體使用。
    - **並行化和並發 (Parallelization and Concurrency):** 熟悉多執行緒 (multi-threading)、多進程 (multi-processing) 等並行化技術，以及非同步 (asynchronous) 編程模型，能夠利用多核心 CPU 或多個 GPU 來加速計算。
- **程式碼優化 (Code Optimization):**
    
    - **語言特性優化 (Language Feature Optimization):** 熟悉所使用程式語言 (例如 Python, C++) 的高效編程技巧，避免常見的效能陷阱。
    - **資料結構優化 (Data Structure Optimization):** 根據應用場景選擇最適合的資料結構，以提高資料存取和操作的效率。
    - **記憶體管理優化 (Memory Management Optimization):** 了解記憶體配置和釋放的機制，避免記憶體洩漏和不必要的記憶體複製。
    - **編譯器優化 (Compiler Optimization):** 了解編譯器的優化選項，並能根據需要進行調整，以生成更高效的機器碼 (尤其是在使用 C++ 等編譯型語言時)。

**3. 客製化核心開發 (Custom Kernel Development):**

- **CUDA 程式設計 (CUDA Programming):**
    
    - **CUDA 架構理解 (Understanding of CUDA Architecture):** 深入了解 NVIDIA GPU 的硬體架構，包括 Streaming Multiprocessors (SMs)、Warp、Thread Block、Shared Memory、Global Memory 等。
    - **CUDA C/C++ 程式設計 (CUDA C/C++ Programming):** 熟練使用 CUDA C/C++ 語言編寫並行執行的 GPU 核心 (kernels)。
    - **CUDA 記憶體模型 (CUDA Memory Model):** 深入理解 GPU 的不同層級記憶體 (Global, Shared, Local, Register) 的特性和使用方式，能夠有效地管理和利用 GPU 記憶體以提高效能。
    - **CUDA 同步機制 (CUDA Synchronization Mechanisms):** 熟悉使用 `__syncthreads()` 等同步原語，確保 GPU 執行緒之間的正確協作。
    - **CUDA 分析與除錯工具 (CUDA Profiling and Debugging Tools):** 熟悉使用 NVIDIA Nsight Compute 等工具分析 CUDA 核心的效能瓶頸，並進行除錯。
- **GPU 最佳化技巧 (GPU Optimization Techniques):**
    
    - **Thread Block 和 Grid 設定 (Thread Block and Grid Configuration):** 能夠根據問題的特性，合理地設定 Thread Block 的大小和 Grid 的維度，以最大化 GPU 的利用率。
    - **Shared Memory 的有效利用 (Effective Use of Shared Memory):** 了解 Shared Memory 的高速存取特性，並能利用它來減少對 Global Memory 的存取，提高資料局部性。
    - **Warp Divergence 的最小化 (Minimization of Warp Divergence):** 了解 Warp 的執行模型，並編寫程式碼以減少 Warp 內執行緒的路徑分歧，提高執行效率。
    - **Memory Coalescing (記憶體合併存取):** 了解 GPU 如何存取 Global Memory，並編寫程式碼以實現合併存取，提高記憶體頻寬利用率。
    - **Instruction-Level Parallelism (指令級並行):** 了解 GPU 的指令執行特性，並編寫程式碼以充分利用指令級並行。

**4. 大規模資料處理、模型預測和模型訓練演算法開發 (Develop Algorithms for Large-Scale Data Processing, Model Prediction, and Model Training):**

- **分散式計算框架 (Distributed Computing Frameworks):**
    
    - 熟悉至少一種分散式計算框架，例如 Apache Spark、Dask 等，能夠開發和優化大規模資料處理的演算法。
    - 了解分散式訓練 (Distributed Training) 的概念和技術，例如資料並行 (Data Parallelism)、模型並行 (Model Parallelism) 等。
- **模型預測優化 (Model Prediction Optimization):**
    
    - **模型壓縮 (Model Compression):** 了解模型剪枝 (Pruning)、量化 (Quantization)、知識蒸餾 (Knowledge Distillation) 等模型壓縮技術，以減小模型大小、降低計算複雜度並提高預測速度。
    - **模型推論引擎 (Model Inference Engines):** 熟悉使用如 NVIDIA TensorRT、OpenVINO、ONNX Runtime 等模型推論引擎，能夠將訓練好的模型部署到高效能硬體上並進行優化。
- **模型訓練優化 (Model Training Optimization):**
    
    - **資料載入與預處理優化 (Data Loading and Preprocessing Optimization):** 了解如何高效地載入和預處理大規模生物影像資料，避免成為訓練瓶頸。
    - **梯度累積 (Gradient Accumulation):** 了解在硬體資源有限的情況下，如何使用梯度累積來模擬更大的 Batch Size。
    - **混合精度訓練 (Mixed-Precision Training):** 熟悉使用半精度浮點數 (FP16) 等混合精度訓練技術，以加速訓練過程並減少記憶體使用。
    - **優化器的選擇與調整 (Optimizer Selection and Tuning):** 了解不同優化器的特性，並能根據問題選擇和調整合適的優化器。

**總結來說，作為一個專精於 GPU 編程以優化生命科學領域 AI 應用效能的 AI 工程師，您需要在效能分析、程式碼和演算法優化、客製化 CUDA 核心開發以及大規模資料處理和模型訓練/預測等方面都具備深入的理解和實戰經驗。您需要能夠識別效能瓶頸，設計並實施有效的優化策略，並充分利用 GPU 的並行計算能力來提升 AI 應用在高效能運算環境下的效率。**



### NVIDIA Nsight Systems 和 Nsight Compute 簡介

詳細解釋如何使用 NVIDIA Nsight Systems 和 Nsight Compute 進行效能瓶頸識別與分析，特別是在 AI 模型推論時的瓶頸分析，並提供具體範例和額外分析的可能性。

**NVIDIA Nsight Systems 和 Nsight Compute 簡介**

- **NVIDIA Nsight Systems:** 是一個系統級的效能分析工具，它可以收集 CPU、GPU、記憶體、網路等系統資源的使用情況和事件追蹤。它主要用於了解應用程式的整體行為，識別跨不同硬體組件的瓶頸，例如 CPU 和 GPU 之間的資料傳輸瓶頸。
    
- **NVIDIA Nsight Compute:** 是一個針對 CUDA 應用程式的低階效能分析工具。它可以詳細分析 GPU 核心 (kernels) 的執行情況，包括指令級的效能指標、記憶體存取模式、並行性等。它主要用於深入了解 GPU 內部發生的瓶頸。
    

在 AI 模型推論的場景下，我們通常會先使用 Nsight Systems 觀察整體的系統行為，然後針對 GPU 密集型的部分使用 Nsight Compute 進行更深入的分析。

**使用 Nsight Systems 進行效能瓶頸識別與分析 (AI 模型推論)**

1. **收集效能資料 (Profiling):**
    
    - 啟動 Nsight Systems 應用程式。
    - 配置 Profile 設定，選擇需要追蹤的事件和指標，例如 CUDA API 調用、GPU 活動、CPU 使用率、記憶體傳輸等。
    - 啟動您的 AI 模型推論應用程式。
    - 在 Nsight Systems 中開始錄製 (Start Capture)。
    - 執行您的模型推論流程，涵蓋資料載入、模型前處理、模型推論、後處理等步驟。
    - 停止錄製 (Stop Capture)。
2. **分析效能報告 (Analysis):**
    
    - Nsight Systems 會生成一個包含時間線的報告，顯示不同硬體組件和軟體執行緒的活動。
    - **CPU 和 GPU 活動 (CPU and GPU Activity):** 觀察 CPU 和 GPU 的利用率。如果 GPU 利用率不高，可能表示瓶頸在於 CPU 端的資料準備、模型前處理或後處理。如果 CPU 利用率很高，可能表示 CPU 計算成為瓶頸，或者 CPU 沒有有效地將工作提交給 GPU。
    - **CUDA API 調用 (CUDA API Calls):** 查看 CUDA API 的調用順序和耗時。長時間的同步操作 (例如 `cudaDeviceSynchronize()`) 可能表示 GPU 計算之間存在依賴關係，或者 CPU 需要等待 GPU 完成某些操作。頻繁的小型記憶體傳輸 (例如 `cudaMemcpyAsync()`) 也可能導致效能下降。
    - **記憶體傳輸 (Memory Transfers):** 觀察 CPU 和 GPU 之間資料傳輸的頻率和大小。大量的資料傳輸可能成為瓶頸，特別是從主機記憶體到 GPU 記憶體的傳輸。
    - **Kernel 執行 (Kernel Execution):** 查看 GPU kernel 的執行時間和頻率。長時間執行的 kernel 是 GPU 計算的主要部分，需要使用 Nsight Compute 進行更深入的分析。

**具體範例說明 (AI 模型推論瓶頸)**

假設您正在使用一個基於 PyTorch 的圖像分類模型進行推論。您使用 Nsight Systems 進行分析後發現：

- **CPU 利用率長時間維持在較高水平 (例如 80%)，而 GPU 利用率波動較大，平均不高 (例如 40%)。** 這可能表示 CPU 在資料載入和預處理部分成為了瓶頸。例如，CPU 需要花費大量時間從磁碟讀取圖像、解碼、調整大小和進行歸一化等操作，導致 GPU 沒有足夠的資料進行計算而處於閒置狀態。
    
- **在時間線上觀察到頻繁且耗時的 `cudaMemcpyAsync()` 調用，將小批量的資料從 CPU 傳輸到 GPU。** 這表示資料傳輸的效率不高，可能是因為傳輸的批次太小，導致啟動傳輸的開銷佔比過高。
    
- **GPU kernel 的執行時間相對較短，但 kernel 之間的間隔較長，且伴隨著 CPU 的一些等待時間。** 這可能表示 CPU 在提交 kernel 給 GPU 之間存在延遲，或者 CPU 需要等待 GPU 完成某些操作才能繼續下一步。
    

**額外分析 (Nsight Systems)**

- **多進程/多執行緒分析 (Multi-process/Multi-thread Analysis):** 如果您的推論應用程式使用多個進程或執行緒，Nsight Systems 可以幫助您了解它們之間的交互和並行性，識別由於鎖競爭或同步問題導致的瓶頸。
- **API 調用堆疊 (API Call Stacks):** 您可以查看 API 調用的堆疊信息，了解效能瓶頸是由哪個函數或程式碼路徑引起的。
- **自定義事件追蹤 (Custom Event Tracing):** 您可以在您的應用程式中插入自定義的事件標記，以便在 Nsight Systems 的時間線上進行追蹤和分析，例如模型中不同層的執行時間。

**使用 Nsight Compute 進行效能瓶頸識別與分析 (針對 GPU Kernel)**

當 Nsight Systems 指出 GPU kernel 的執行可能是瓶頸時，或者您想深入了解 GPU 內部效能時，可以使用 Nsight Compute。

1. **收集 Kernel 效能資料 (Profiling):**
    
    - 啟動 Nsight Compute 應用程式。
    - 配置 Profile 設定，選擇需要分析的指標。Nsight Compute 提供了大量的硬體指標，例如指令吞吐量、記憶體頻寬、快取命中率、warp 發散等。您可以根據您的分析目標選擇相關的指標。
    - 指定要分析的目標應用程式和 kernel。您可以選擇分析所有 kernel 或特定的 kernel。
    - 啟動分析 (Start)。
    - 執行您的 AI 模型推論應用程式，觸發您想要分析的 kernel。
    - Nsight Compute 會收集 GPU kernel 的執行數據。
2. **分析效能報告 (Analysis):**
    
    - Nsight Compute 會生成詳細的報告，包含 kernel 的配置信息 (例如 grid 和 block 的大小)、指令級的統計數據、記憶體存取模式分析等。
    - **指令吞吐量 (Instruction Throughput):** 觀察 kernel 的指令發射速率和執行效率。低吞吐量可能表示存在指令依賴、控制流發散或資源瓶頸。
    - **記憶體存取分析 (Memory Access Analysis):** 分析 kernel 的記憶體存取模式，例如 Global Memory 的存取是否合併 (coalesced)，Shared Memory 的使用是否有效，L1/L2 快取的命中率等。低效的記憶體存取是 GPU 效能的常見瓶頸。
    - **Warp 發散 (Warp Divergence):** 觀察 warp 內不同執行緒的控制流是否一致。過多的 warp 發散會導致 warp 中的執行緒需要串行執行不同的指令，降低並行效率。
    - **算術運算單元利用率 (Arithmetic Throughput):** 觀察 GPU 的浮點和整數運算單元的利用率。如果利用率不高，可能表示計算強度不足或者存在其他瓶頸限制了計算單元的發揮。
    - **Occupancy (佔用率):** 觀察 GPU 上 active warps 的數量與硬體限制的比率。低佔用率可能導致 GPU 的計算資源沒有被充分利用。

**具體範例說明 (AI 模型推論 Kernel 瓶頸)**

假設您使用 Nsight Compute 分析了模型中的一個卷積層 (Convolutional Layer) 的 CUDA kernel，發現：

- **Global Memory 的存取效率很低，出現大量的非合併存取 (non-coalesced access)。** 這表示 kernel 中相鄰的執行緒存取了不連續的記憶體地址，導致 GPU 需要進行多次低效的記憶體讀取操作。優化策略可能是調整資料排布或 kernel 的存取模式，以實現合併存取。
    
- **Shared Memory 的使用效率不高，load 和 store 操作的延遲較高。** 這可能表示 Shared Memory 的 bank conflict 較為嚴重，導致並行存取的執行緒需要串行訪問不同的 memory bank。優化策略可能是調整 Shared Memory 的使用方式或資料的組織方式，以減少 bank conflict。
    
- **Warp 發散率很高。** 這表示在同一個 warp 內，不同的執行緒由於條件判斷等原因執行了不同的程式碼路徑，導致 warp 的並行性降低。優化策略可能是重新設計 kernel 的邏輯，減少控制流的分歧。
    

**額外分析 (Nsight Compute)**

- **原始碼關聯 (Source Code Correlation):** Nsight Compute 可以將效能指標與您的 CUDA 原始碼關聯起來，幫助您快速定位效能瓶頸的程式碼行。
- **建議 (Recommendations):** Nsight Compute 會根據分析結果提供一些優化建議，例如如何改進記憶體存取、減少 warp 發散等。
- **實驗和比較 (Experimentation and Comparison):** 您可以修改您的 CUDA 程式碼，然後再次使用 Nsight Compute 進行分析，比較不同優化策略的效果。

**總結**

使用 NVIDIA Nsight Systems 和 Nsight Compute 是識別和分析 AI 模型推論效能瓶頸的強大工具。Nsight Systems 提供系統級的視角，幫助您了解 CPU、GPU 和記憶體之間的交互，找出宏觀的瓶頸。Nsight Compute 則深入分析 GPU kernel 的執行細節，幫助您理解微觀的效能問題，並指導您進行更精細的優化。通過結合使用這兩個工具，您可以全面地了解您的 AI 模型推論應用程式的效能瓶頸，並採取有針對性的優化措施來提高其執行效率。


### 分散式訓練 (Distributed Training)

詳細解釋分散式訓練的概念和技術，並提供使用 Azure Machine Learning (AzureML) 一步步實作訓練一個 AI 分割模型的大致流程。

**分散式訓練 (Distributed Training) 的概念和技術**

隨著 AI 模型變得越來越大、資料集越來越龐大，單個 GPU 或機器往往無法在合理的時間內完成模型的訓練。分散式訓練利用多個計算節點（通常是多個 GPU 或多台機器）協同工作，共同完成模型的訓練過程，從而顯著縮短訓練時間。

分散式訓練的核心思想是將訓練任務分解並分配到多個計算單元上，然後將各個單元的計算結果進行整合，以更新模型的參數。常見的分散式訓練技術可以分為以下幾種：

**1. 資料並行 (Data Parallelism)**

- **概念：** 資料並行是最常見的分散式訓練方法之一。它將整個訓練資料集劃分成多個不重疊的子集 (mini-batches)，每個計算節點都擁有模型的完整副本，並使用分配給它的資料子集獨立地計算梯度。然後，所有計算節點會將它們計算出的梯度進行聚合（例如，取平均），並用聚合後的梯度來更新模型的參數。由於每個節點都處理不同的資料，因此可以有效地利用多個計算資源來加速訓練。
    
- **運作流程：**
    
    1. 將訓練資料集劃分為 N 個子集，其中 N 是計算節點的數量。
    2. 將模型的完整副本複製到每個計算節點上。
    3. 每個計算節點使用分配給它的資料子集計算梯度。
    4. 使用某種同步或非同步的機制聚合所有節點計算出的梯度。常見的聚合方法包括：
        - **同步 SGD (Synchronous SGD):** 所有節點完成梯度計算後，統一進行梯度聚合和模型更新。這種方法確保了訓練過程的穩定性，但速度可能受到最慢節點的限制。
        - **非同步 SGD (Asynchronous SGD):** 每個節點獨立地計算梯度並更新本地模型副本，其他節點可以隨時從參數伺服器獲取最新的模型參數。這種方法可以提高訓練速度，但可能引入模型更新的不一致性。
    5. 使用聚合後的梯度更新每個節點上的模型副本。
    6. 重複步驟 3-5 直到模型收斂。
- **優點：**
    
    - 實現相對簡單。
    - 可以有效地利用大量的計算資源。
    - 每個節點都處理不同的資料，有助於提高模型的泛化能力。
- **缺點：**
    
    - 需要將模型的完整副本儲存在每個節點的記憶體中，對於非常大的模型可能會遇到記憶體限制。
    - 同步 SGD 可能會受到通訊瓶頸的影響，特別是在節點數量較多或網路速度較慢的情況下。

**2. 模型並行 (Model Parallelism)**

- **概念：** 模型並行適用於模型非常龐大，以至於單個 GPU 或機器的記憶體無法容納整個模型的情況。它將模型的不同部分（例如，不同的層或子網路）分配到不同的計算節點上。每個節點只負責計算模型的一部分的前向和反向傳播。節點之間需要協同工作，將中間結果傳遞給下一個負責計算的節點。
    
- **運作流程：**
    
    1. 將模型劃分為多個部分。
    2. 將模型的每個部分分配到不同的計算節點上。
    3. 在前向傳播過程中，資料依序通過分配在不同節點上的模型部分，中間結果需要在節點之間傳遞。
    4. 在反向傳播過程中，梯度以相反的方向傳播，不同節點計算其負責的模型部分的梯度，並將梯度傳遞給前一個節點。
    5. 每個節點根據接收到的梯度更新其負責的模型部分的參數。
- **優點：**
    
    - 可以訓練非常大的模型，克服單個設備的記憶體限制。
- **缺點：**
    
    - 實現複雜度高，需要仔細設計模型的劃分和節點之間的通信。
    - 節點之間的通信量可能很大，容易成為效能瓶頸。
    - 不同節點的計算負載可能不均衡。

**3. 資料並行與模型並行的混合並行 (Hybrid Parallelism)**

- **概念：** 混合並行結合了資料並行和模型並行的優點。例如，可以在每個機器內部使用模型並行來處理大型模型的部分，然後在多個機器之間使用資料並行來加速訓練。
    
- **應用場景：** 適用於非常龐大的模型和非常大的資料集。
    

**使用 Azure Machine Learning (AzureML) 一步步實作訓練一個 AI 分割模型 (概念性流程)**

以下是在 AzureML 中實作分散式訓練一個 AI 分割模型的概念性步驟。具體的程式碼和配置會根據您使用的深度學習框架（例如 PyTorch、TensorFlow）、模型架構和資料集而有所不同。

**步驟 1：準備 AzureML 環境**

1. **建立 Azure Machine Learning 工作區 (Workspace):** 如果您還沒有 AzureML 工作區，需要在 Azure Portal 中建立一個。
2. **設定計算資源 (Compute Targets):**
    - **建立 GPU 叢集 (GPU Cluster):** 在 AzureML 工作區中建立一個或多個具有多個 GPU 節點的計算叢集。這是進行分散式訓練的關鍵資源。您可以根據您的需求選擇不同的 VM 大小和 GPU 數量。
    - **（可選）建立 CPU 計算執行個體 (Compute Instance):** 可以使用 CPU 計算執行個體來進行程式碼開發、資料探索和實驗管理。
3. **註冊資料集 (Register Dataset):** 將您的分割訓練資料集上傳到 Azure Blob Storage 或其他 Azure 資料儲存服務，並在 AzureML 工作區中將其註冊為資料集。這使得您的資料可以被訓練腳本輕鬆訪問。
4. **建立環境 (Environment):** 建立一個 AzureML 環境，其中包含訓練腳本所需的所有 Python 套件、深度學習框架（例如 `torch`、`tensorflow`）、CUDA 和 cuDNN 庫等。您可以從 AzureML 的策劃環境開始，然後根據需要進行自訂。

**步驟 2：準備訓練腳本**

1. **撰寫訓練腳本 (Training Script):** 編寫您的 Python 訓練腳本，該腳本應包含以下部分：
    - **資料載入和預處理:** 使用 AzureML 註冊的資料集載入和預處理您的分割資料。
    - **模型定義:** 定義您的分割模型架構（例如，U-Net、Mask R-CNN 等）。
    - **損失函數和優化器:** 定義用於訓練的損失函數和優化器。
    - **訓練迴圈:** 實現模型的訓練迴圈，包括前向傳播、計算損失、反向傳播和參數更新。
    - **分散式訓練的整合:** 根據您選擇的分散式訓練策略（資料並行或模型並行），在訓練腳本中加入相應的程式碼。
        - **資料並行 (PyTorch 範例，使用 `torch.distributed`):**

```
        def main():
            dist.init_process_group(backend='nccl', init_method='env://')
            rank = dist.get_rank()
            world_size = dist.get_world_size()
            local_rank = int(os.environ['LOCAL_RANK'])
            torch.cuda.set_device(local_rank)
            device = torch.device(f"cuda:{local_rank}")

            model = YourSegmentationModel().to(device)
            model = DDP(model, device_ids=[local_rank])

            # 載入資料集 (使用 DataLoader 並使用 DistributedSampler)
            train_dataset = YourSegmentationDataset(...)
            train_sampler = torch.utils.data.distributed.DistributedSampler(
                train_dataset, num_replicas=world_size, rank=rank, shuffle=True
            )
            train_loader = torch.utils.data.DataLoader(
                train_dataset, batch_size=your_batch_size // world_size,
                shuffle=False, num_workers=your_num_workers, sampler=train_sampler
            )

            optimizer = torch.optim.Adam(model.parameters(), lr=your_learning_rate)
            criterion = YourSegmentationLoss()

            for epoch in range(your_num_epochs):
                for i, (images, masks) in enumerate(train_loader):
                    images = images.to(device)
                    masks = masks.to(device)

                    optimizer.zero_grad()
                    outputs = model(images)
                    loss = criterion(outputs, masks)
                    loss.backward()
                    optimizer.step()

                    if i % 10 == 0 and rank == 0:
                        print(f"Epoch [{epoch}/{your_num_epochs}], Step [{i}/{len(train_loader)}], Loss: {loss.item():.4f}")

                if rank == 0:
                    # 在主要進程上儲存模型
                    torch.save(model.module.state_dict(), f"segmentation_model_epoch_{epoch}.pth")

        if __name__ == "__main__":
            main()
        ```
    * **模型並行 (TensorFlow 範例，使用 `tf.distribute.experimental.partitioners`):** 模型並行的實作在 TensorFlow 中更為複雜，通常需要仔細設計模型的劃分和使用特定的策略。

* **模型儲存:** 在訓練結束或每個 epoch 結束時儲存訓練好的模型。
* **參數解析:** 使用 `argparse` 或類似的庫來接收訓練所需的超參數（例如學習率、批次大小、epoch 數量等）。
```

**步驟 3：配置和提交 AzureML 實驗**

1. **建立訓練腳本的目錄:** 將您的訓練腳本和任何相關的輔助檔案放在一個目錄中。
    
2. **建立 AzureML 訓練作業的設定 (Job Configuration):** 使用 AzureML SDK 或 CLI 來定義您的訓練作業。這包括：
    
    - **計算目標 (Compute Target):** 指定您在步驟 1 中建立的 GPU 叢集。
    - **環境 (Environment):** 指定您在步驟 1 中建立的包含所需依賴項的環境。
    - **命令 (Command):** 定義在計算目標上執行的命令，通常是執行您的訓練腳本，並傳遞所需的超參數。
    - **分散式訓練配置 (Distributed Training Configuration):** 根據您選擇的並行策略配置分散式訓練。
        - **資料並行 (PyTorch):** 使用 `PyTorch` 或 `Mpi` 分散式組態，指定節點數量和每個節點的進程數 (通常等於 GPU 數量)。
        - **資料並行 (TensorFlow):** 使用 `TensorFlow` 或 `Mpi` 分散式組態。
    - **輸入資料 (Input Data):** 指定您在步驟 1 中註冊的資料集，以便訓練腳本可以訪問。
    - **輸出 (Outputs):** 定義儲存訓練好的模型和其他輸出的位置。
3. **提交實驗 (Submit Experiment):** 使用 AzureML SDK 或 CLI 將您的訓練作業提交到 AzureML 工作區。AzureML 會將您的腳本和環境部署到指定的計算目標上，並開始執行訓練。
    
4. **監控實驗 (Monitor Experiment):** 您可以在 AzureML 工作室或使用 SDK/CLI 監控實驗的進度、日誌和指標。
    

**步驟 4：評估和部署模型**

1. **註冊訓練好的模型 (Register Trained Model):** 一旦訓練完成，將訓練好的模型註冊到 AzureML 工作區中。
2. **評估模型 (Evaluate Model):** 使用註冊的模型在驗證集或測試集上進行評估，以衡量模型的效能。您可以在 AzureML 中建立一個評估作業來完成此步驟。
3. **部署模型 (Deploy Model):** 將註冊的模型部署為 Azure Container Instances (ACI)、Azure Kubernetes Service (AKS) 或其他 Azure 部署目標，以便用於實際的推論任務。

**總結**

在 AzureML 中實作分散式訓練需要仔細的環境設定、訓練腳本的編寫（包含分散式訓練的邏輯）以及正確的作業配置。AzureML 提供了豐富的工具和 SDK 來簡化這個過程，讓您可以專注於模型開發和優化，而無需過多關注底層的基礎設施管理。選擇哪種並行策略取決於您的模型大小、資料集大小和可用的計算資源。對於大多數 AI 分割任務，資料並行通常是一個較好的起點。