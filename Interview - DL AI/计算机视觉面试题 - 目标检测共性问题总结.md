https://zhuanlan.zhihu.com/p/555123262

1. 目标检测两阶段和一阶段的核心区别
2. 目标检测两阶段比一阶段的算法精度高的原因
3. 如何解决目标检测中密集遮挡问题
4. “狭长形状”目标检测有什么合适方法
5. 如何解决动态目标检测FPN的作用
6. 为什么FPN采用融合以后效果要比使用pyramidal feature hierarchy这种方式要好？
7. FPN在RPN中的应用如何解决小目标识别问题
8. 介绍目标检测RCNN系列和Yolo系列的区别
9. YOLO和SSD区别
10. 前景背景样本不均衡解决方案：Focal Loss,GHM与PISA
11. 如何解决训练数据样本过少的问题
12. 如何解决类别不平衡的问题
13. 手撕代码IOU
14. 手撕代码NMS
15. NMS的改进思路
16. IOU相关优化(giou,diou,ciou)
17. 
### 1. 目標檢測兩階段（Two-stage）和一階段（One-stage）的核心區別

**目標檢測**（Object Detection）的模型主要分為兩種類型：兩階段（如 R-CNN 系列）和一階段（如 YOLO 和 SSD 系列）。兩者的核心區別在於流程設計和對檢測精度與速度的平衡。

#### 兩階段模型

- 在兩階段模型中，第一階段生成候選區域（Region Proposal），第二階段在候選區域上進行精確的分類與邊界框回歸。
- 第一階段通常使用 RPN（Region Proposal Network）生成可能包含物體的候選區域，這是基於特徵圖上滑窗（sliding window）方法或特徵範圍計算來生成。
- 第二階段則是對這些候選區域做進一步的分類（判斷是何種物體）和邊界框回歸（Boundary Box Regression），以精確定位物體。

**例子**：Faster R-CNN 就是典型的兩階段模型。

#### 一階段模型

- 一階段模型直接在整個圖片上生成預測（包括物體分類和邊界框回歸），不經過明確的候選區域生成階段。
- 這樣做的好處是大大簡化了流程，從而提高了檢測速度。
- 一階段模型通常使用密集的網格（grid）來進行滑窗，直接在每個網格中計算物體存在的概率與邊界框回歸。

**例子**：YOLO（You Only Look Once）和 SSD（Single Shot MultiBox Detector）屬於典型的一階段模型。

**總結**：兩階段模型的**主要特點**是先生成候選區域，然後再細化結果，適合需要更高精度的應用；一階段模型則直接預測，速度較快但精度略低。

### 2. 目標檢測中兩階段算法比一階段算法精度高的原因

**原因一：候選區域精確性**

- 在兩階段方法中，第一階段生成的候選區域更精確地覆蓋了可能存在的物體，這使得後續的分類和邊界框回歸有了更準確的基礎。
- RPN 通常會生成大量的候選框，這樣可以有效地提高檢測的召回率（Recall），減少漏檢（Missing Detection）情況。

**原因二：專注的細化階段**

- 兩階段模型的第二階段專注於對候選區域進行詳細的分類和精確定位。
- 與一階段模型相比，這個細化過程可以帶來更高的分類精度和定位精度，尤其是對小物體和密集物體的檢測。

**原因三：降低背景干擾**

- 一階段模型在整個圖像上進行預測，易受背景的干擾，而兩階段模型因為專注於候選區域，能夠有效避免背景噪聲對檢測結果的影響。

### 3. 如何解決目標檢測中密集遮擋問題

密集遮擋（Occlusion）問題通常是由於多個物體彼此遮擋，導致部分物體的特徵被遮擋，難以被模型正確識別。解決密集遮擋問題的方法有以下幾種：

#### 方法一：使用多尺度特徵（Multi-scale Features）

- 在多層特徵圖中提取物體特徵，尤其是低層特徵圖保留了更多細節，適合小物體和被遮擋物體的檢測。
- FPN（Feature Pyramid Network）是一個典型的多尺度特徵融合方法，它將不同層次的特徵圖進行融合，從而提高了被遮擋物體的檢測效果。
```
# FPN 示例代碼 (PyTorch)
import torch
import torch.nn as nn

class FPN(nn.Module):
    def __init__(self, backbone):
        super(FPN, self).__init__()
        # 假設 backbone 已有 4 層特徵圖
        self.backbone = backbone
        self.lat_layers = nn.ModuleList([nn.Conv2d(in_channels, 256, 1) for in_channels in [256, 512, 1024, 2048]])
        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')
    
    def forward(self, x):
        features = self.backbone(x)
        # 自頂向下傳播
        for i in range(len(features) - 1, 0, -1):
            features[i - 1] += self.upsample(features[i])
        return features

```

#### 方法二：應用上下文信息（Contextual Information）

- 使用上下文信息來推斷遮擋物體的類別。
- 如 Relation Network 可以學習物體之間的關係和佈局，根據上下文信息來補充遮擋部分的信息。

#### 方法三：增加空間注意力機制（Spatial Attention Mechanism）

- 空間注意力機制可以幫助模型專注於重要的區域，忽略背景和被遮擋區域，從而提高檢測精度。
- SA（Spatial Attention）和 CA（Channel Attention）是典型的注意力機制，可用於提高遮擋物體的檢測效果。
```
# 空間注意力示例代碼
class SpatialAttention(nn.Module):
    def __init__(self):
        super(SpatialAttention, self).__init__()
        self.conv1 = nn.Conv2d(2, 1, kernel_size=7, padding=3)
    
    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x = torch.cat([avg_out, max_out], dim=1)
        x = self.conv1(x)
        return torch.sigmoid(x)

```

#### 方法四：生成對抗網絡（Generative Adversarial Networks，GAN）或類似方法進行遮擋修復

- 使用生成對抗網絡來填補或修復遮擋部分，從而為檢測模型提供更多信息。
- 這種方法適合於嚴重遮擋的場景，例如人群密集的場景。

**總結**：針對密集遮擋問題，常用的解決方法包括多尺度特徵、上下文信息、注意力機制和生成對抗網絡等。這些方法可以從不同的角度來提高遮擋場景下的檢測效果。

### 4. “狹長形狀”目標檢測的合適方法

對於**狹長形狀（Elongated Shape）**的目標檢測，普通的目標檢測模型可能不太適合，因為狹長形狀的物體在圖像中的外觀特徵和其他形狀差異較大。針對這類目標的檢測方法有以下幾種：

#### 方法一：自適應錨框（Anchor Boxes）

- 錨框（Anchor Boxes）在常規模型中通常設計成固定的寬高比，但對於狹長目標，需要調整錨框的形狀和大小，使其更適合長條狀物體。
- 自適應錨框可以針對目標特徵進行調整，為狹長形目標提供更好的框選效果。
```
# 示例代碼：自適應錨框生成
import numpy as np

def generate_elongated_anchors(ratios=[0.1, 0.2, 0.5], scales=[64, 128, 256]):
    anchors = []
    for scale in scales:
        for ratio in ratios:
            width = scale * np.sqrt(ratio)
            height = scale / np.sqrt(ratio)
            anchors.append([width, height])
    return np.array(anchors)

# 結果示例
anchors = generate_elongated_anchors()
print("Generated Elongated Anchors:", anchors)

```

#### 方法二：方向敏感特徵（Orientation-sensitive Features）

- 針對狹長形物體的方向性，可以採用方向敏感的卷積或特徵提取方法，使得模型能夠識別目標的方向。
- SIFT（Scale-Invariant Feature Transform）特徵和霍夫變換（Hough Transform）可以幫助強化方向性特徵。

#### 方法三：旋轉邊界框（Rotated Bounding Box）

- 常規的邊界框是橫向或縱向的矩形，而針對狹長形狀目標，可以使用旋轉邊界框來更精確地包圍目標。
- 旋轉邊界框是指可以旋轉的矩形框，這樣能更好地覆蓋狹長目標，並提高檢測精度。
```
# 示例代碼：旋轉邊界框生成
import cv2

def draw_rotated_bbox(img, center, size, angle, color=(0, 255, 0)):
    rect = (center, size, angle)
    box = cv2.boxPoints(rect)
    box = np.int0(box)
    cv2.drawContours(img, [box], 0, color, 2)

# 使用示例：在圖像上畫旋轉邊界框
image = np.zeros((200, 200, 3), dtype=np.uint8)
draw_rotated_bbox(image, center=(100, 100), size=(50, 150), angle=30)
cv2.imshow("Rotated Bounding Box", image)
cv2.waitKey(0)
cv2.destroyAllWindows()

```

### 5. 如何解決動態目標檢測中 FPN 的作用

**動態目標檢測（Dynamic Object Detection）**面臨的挑戰主要在於物體的位置和形狀會隨時間變化。**特徵金字塔網絡（Feature Pyramid Network，FPN）**在此方面具有重要作用，因為它可以提供不同尺度下的特徵信息，幫助模型更好地識別動態場景中的多種物體。

#### FPN 的作用

1. **多尺度特徵提取（Multi-scale Feature Extraction）**
    - 動態目標檢測中，目標的大小和形狀可能會隨距離變化而改變。FPN 可以從不同尺度的特徵圖中提取信息，以便在各種尺度上識別物體。
2. **增強小物體的檢測效果**
    - 小物體在動態場景中易被忽略，FPN 將不同層次的特徵圖進行融合，有助於增強小物體的表徵，提高其檢測效果。
3. **保持高效運算（Efficient Computation）**
    - FPN 通過特徵金字塔的設計，有效提高了多尺度檢測的計算效率，特別適合於動態目標檢測中需要實時性要求的應用。

#### FPN 代碼示例

FPN 通常在 Backbone 網絡之上實現，比如在 ResNet 或 MobileNet 等特徵提取網絡上添加 FPN 結構：
```
import torch
import torch.nn as nn
import torchvision.models as models

class FPN(nn.Module):
    def __init__(self, backbone):
        super(FPN, self).__init__()
        self.backbone = backbone
        self.lat_layers = nn.ModuleList([nn.Conv2d(in_channels, 256, 1) for in_channels in [256, 512, 1024, 2048]])
        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')
    
    def forward(self, x):
        features = self.backbone(x)
        for i in range(len(features) - 1, 0, -1):
            features[i - 1] += self.upsample(features[i])
        return features

```

### 6. 為什麼 FPN 融合效果比 Pyramidal Feature Hierarchy 更好

**特徵融合（Feature Fusion）**在 FPN 中的應用使得其檢測效果優於僅僅使用金字塔特徵層次結構（Pyramidal Feature Hierarchy），原因如下：

#### 原因一：信息互補性（Information Complementarity）

- 在 FPN 中，特徵圖的上層和下層進行融合，每層的特徵互相補充。低層特徵圖擁有更豐富的空間細節信息，而高層特徵圖具有更強的語義信息。
- 通過融合這些特徵，FPN 可以兼顧小物體的細節和大物體的全局特徵。

#### 原因二：提升小物體檢測（Improved Small Object Detection）

- 金字塔特徵層次結構僅使用每層的單一特徵圖，而不進行上下層信息的整合，因此無法很好地對小物體進行檢測。
- FPN 將低層次的細節信息與高層次的語義信息結合，顯著提高了小物體的檢測效果。

#### 原因三：增強模型的泛化能力（Enhanced Generalization Capability）

- FPN 的融合過程通過自頂向下傳遞，使得每一層特徵包含了上下層的豐富信息，從而提升了模型在不同尺度下的泛化能力。
- 相比於金字塔層次結構，FPN 通常在處理複雜場景時更具魯棒性（Robustness）。

#### 代碼示例：自頂向下融合

以下是實現自頂向下融合的示例，展示了如何在不同層的特徵圖之間進行融合：
```
import torch
import torch.nn as nn

class SimpleFPN(nn.Module):
    def __init__(self):
        super(SimpleFPN, self).__init__()
        # 假設已提取的 backbone 特徵圖
        self.lat_layers = nn.ModuleList([nn.Conv2d(in_channels, 256, 1) for in_channels in [256, 512, 1024, 2048]])
        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')
    
    def forward(self, features):
        for i in range(len(features) - 1, 0, -1):
            features[i - 1] += self.upsample(features[i])
        return features

# 假設輸入的特徵圖
features = [torch.rand(1, 256, 64, 64), torch.rand(1, 512, 32, 32), torch.rand(1, 1024, 16, 16), torch.rand(1, 2048, 8, 8)]
fpn = SimpleFPN()
output = fpn(features)

```

### 總結

- **狹長形目標檢測**：通過調整錨框、自適應旋轉邊界框和方向敏感特徵來更好地檢測狹長形狀目標。
- **動態目標檢測中的 FPN 作用**：FPN 提供多尺度特徵支持、增強小物體檢測能力，適合動態目標場景。
- **FPN 融合優於金字塔層次**：通過特徵圖的互補性和融合策略，使 FPN 對小物體檢測和模型泛化能力的提升效果明顯，適合各種場景下的目標檢測。

### 7. FPN 在 RPN 中的應用如何解決小目標識別問題

**特徵金字塔網絡（Feature Pyramid Network, FPN）** 通過自頂向下的特徵融合方法，能夠在不同尺度上更好地檢測物體。這種設計非常適合解決 **小目標識別（Small Object Detection）** 問題，特別是在 RPN（Region Proposal Network） 中應用時，能顯著提升小目標的檢測能力。

#### 原理解析

1. **多尺度特徵融合（Multi-scale Feature Fusion）**：
    
    - FPN 能夠從 Backbone 中提取多層特徵圖，例如 ResNet 的不同層次，低層次包含更多空間細節，高層次包含更多語義信息。
    - FPN 將不同層的特徵圖通過上採樣和融合處理，這樣小目標在較低層的特徵圖中可以更好地表現出來，而大目標則可以從更高層的特徵中得到更好的表示。
2. **候選區域生成（Region Proposal Generation）**：
    
    - 在 RPN 中，FPN 將不同層次的特徵圖提供給 RPN 用於生成候選框（Anchors）。這樣一來，對於小目標的候選框可以基於低層特徵圖生成，確保小物體不會因為特徵層次過高而被忽略。
    - 通過這種多層次的候選框生成方式，FPN 在 RPN 中能夠更精確地捕捉小物體。

#### 實現示例

以下是一個簡化的 FPN 應用於 RPN 的代碼示例：
```
import torch
import torch.nn as nn

class FPN_RPN(nn.Module):
    def __init__(self, backbone):
        super(FPN_RPN, self).__init__()
        self.backbone = backbone
        self.lat_layers = nn.ModuleList([nn.Conv2d(in_channels, 256, 1) for in_channels in [256, 512, 1024, 2048]])
        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')
        self.rpn_head = nn.Conv2d(256, 9 * 2, 1)  # 假設每個像素9個anchors，每個anchor有2個分類（物體/背景）

    def forward(self, x):
        features = self.backbone(x)
        # 特徵金字塔融合
        for i in range(len(features) - 1, 0, -1):
            features[i - 1] += self.upsample(features[i])
        
        # 應用 RPN 頭
        proposals = [self.rpn_head(f) for f in features]
        return proposals

# 假設輸入的特徵圖
features = [torch.rand(1, 256, 64, 64), torch.rand(1, 512, 32, 32), torch.rand(1, 1024, 16, 16), torch.rand(1, 2048, 8, 8)]
fpn_rpn = FPN_RPN(backbone=None)  # 此處僅做示例，backbone 應為完整的 CNN 模型
output = fpn_rpn(features)

```
### 8. 介紹目標檢測 RCNN 系列和 YOLO 系列的區別

**RCNN 系列**和**YOLO 系列**是兩種不同的目標檢測方法，各自有不同的優缺點和應用場景。

#### RCNN 系列（Region-based CNN）

1. **基於區域的檢測方法（Region-based Detection）**：
    - RCNN 系列屬於 **兩階段檢測方法（Two-stage Detection）**。第一階段會生成候選區域（即 Region Proposal），第二階段對候選區域進行分類和邊界框回歸。
2. **演變歷程**：
    - **RCNN**：最初的 RCNN 方法使用選擇性搜索（Selective Search）來生成候選區域，每個候選區域單獨進行 CNN 處理，速度慢且資源消耗大。
    - **Fast RCNN**：Fast RCNN 改進了 RCNN，通過在整張圖像上進行卷積，然後對候選區域進行 RoI Pooling，顯著提高了速度。
    - **Faster RCNN**：Faster RCNN 則引入了 RPN（Region Proposal Network），由神經網絡生成候選框，進一步提升了速度。
3. **精度較高但速度較慢**：
    - RCNN 系列的精度通常高於 YOLO 系列，尤其是對於小物體和復雜場景的檢測表現更好。但由於其兩階段特性，推理速度相對較慢。

#### YOLO 系列（You Only Look Once）

1. **基於網格的檢測方法（Grid-based Detection）**：
    
    - YOLO 系列屬於 **一階段檢測方法（One-stage Detection）**，直接在整張圖片上進行分類和回歸操作，不進行候選區域生成。
    - YOLO 將整個圖像分為固定網格，每個網格直接預測該區域是否包含物體，以及物體的類別和位置。
2. **演變歷程**：
    
    - **YOLOv1**：最初的 YOLO 方法直接使用網格進行預測，但存在定位誤差和小物體檢測不佳的問題。
    - **YOLOv3 & YOLOv4**：改進了網格分辨率，增強了多尺度檢測的能力，大大提高了 YOLO 在小物體上的檢測性能。
    - **YOLOv5**及以後：進一步優化了模型的效率和精度，支持各種輕量化變體，適合在嵌入式設備上部署。
3. **速度快但精度稍低**：
    
    - YOLO 系列因為是一階段模型，推理速度非常快，特別適合於對實時性要求較高的應用。然而，由於其直接預測整圖，對小物體的檢測效果通常不如 RCNN 系列。

#### 總結

- **RCNN 系列**：兩階段檢測，精度較高，適合小物體和復雜場景，但速度較慢。
- **YOLO 系列**：一階段檢測，速度極快，適合實時性要求高的場景，但小物體檢測效果稍差。

### 9. YOLO 和 SSD 的區別

**YOLO（You Only Look Once）**和**SSD（Single Shot MultiBox Detector）**都是一階段目標檢測方法，但在結構設計和檢測方式上有所不同。

#### YOLO 的特點

1. **整體圖像作為輸入（Global Image as Input）**：
    - YOLO 將整個圖像分成固定的網格，每個網格直接預測物體的類別和邊界框。這種方法簡化了模型結構並加快了計算速度。
2. **單一尺度預測（Single-scale Prediction）**：
    - 在 YOLO 中，預測過程大多基於固定大小的網格進行，僅適合在圖像的單一尺度上進行物體檢測。
3. **速度快但對小物體效果差**：
    - YOLO 的單一尺度預測導致其對小物體的檢測效果稍差，特別是與背景融合度較高的小物體。

#### SSD 的特點

1. **多尺度特徵圖（Multi-scale Feature Maps）**：
    - SSD 使用了多層特徵圖進行檢測，即在低層特徵圖上檢測小物體，並在高層特徵圖上檢測大物體。這樣的設計使得 SSD 在不同尺度上都能檢測物體。
2. **多尺度錨框（Multi-scale Anchor Boxes）**：
    - SSD 為每一個特徵圖分配了不同大小和長寬比的錨框（Anchor Boxes），這樣能夠有效地檢測不同大小的物體。
3. **較高的精度和實時性**：
    - SSD 在檢測速度和精度之間取得了平衡，特別是在小物體檢測方面優於 YOLO，同時也具有較好的實時性。

#### 實現代碼示例

以下是 YOLO 和 SSD 的簡化代碼示例，展示了各自的網格和錨框設計方式。

##### YOLO 的網格預測代碼示例
```
import torch
import torch.nn as nn

class YOLO(nn.Module):
    def __init__(self, grid_size=7, num_classes=20, num_boxes=2):
        super(YOLO, self).__init__()
        self.grid_size = grid_size
        self.num_classes = num_classes
        self.num_boxes = num_boxes
        self.fc = nn.Linear(1024, grid_size * grid_size * (num_boxes * 5 + num_classes))

    def forward(self, x):
        x = self.fc(x)
        x = x.view(-1, self.grid_size, self.grid_size, self.num_boxes * 5 + self.num_classes)
        return x

```

##### SSD 的多尺度錨框代碼示例
```
import torch
import torch.nn as nn

class SSD(nn.Module):
    def __init__(self, num_classes=21):
        super(SSD, self).__init__()
        # 每層生成不同大小的特徵圖，並應用多尺度錨框
        self.num_classes = num_classes
        self.feature_maps = nn.ModuleList([
            nn.Conv2d(512, 1024, kernel_size=3, padding=1),
            nn.Conv2d(1024, 512, kernel_size=3, padding=1),
            nn.Conv2d(512, 256, kernel_size=3, padding=1)
        ])
        self.pred_layers = nn.ModuleList([nn.Conv2d(256, num_classes * 4, 3, padding=1) for _ in range(3)])

    def forward(self, x):
        predictions = []
        for feature_map, pred_layer in zip(self.feature_maps, self.pred_layers):
            x = feature_map(x)
            predictions.append(pred_layer(x))
        return predictions

```
#### 總結

- **YOLO**：使用單一尺度網格檢測，速度極快但對小物體檢測效果較差。
- **SSD**：利用多尺度特徵圖和錨框，檢測不同大小的物體，精度高於 YOLO，且仍能達到實時性。

### 10. 前景背景樣本不均衡解決方案：Focal Loss、GHM 與 PISA

在目標檢測任務中，**前景背景樣本不均衡（Foreground-Background Class Imbalance）**問題普遍存在，因為背景區域佔據了大量的樣本，而前景（即物體）區域相對較少。為了解決這個問題，通常會採用 **Focal Loss**、**GHM（Gradient Harmonizing Mechanism）** 和 **PISA（Positive Instance Sampling Algorithm）**。

#### Focal Loss

**Focal Loss** 是針對樣本不均衡問題而設計的一種損失函數，主要用於減少簡單樣本（如背景）的影響，並專注於難分類的樣本（如前景）。

- **公式**：
    
    Focal Loss=−α(1−pt)γlog⁡(pt)\text{Focal Loss} = -\alpha (1 - p_t)^\gamma \log(p_t)Focal Loss=−α(1−pt​)γlog(pt​)
    
    其中， ptp_tpt​ 是模型對正確類別的預測概率，α\alphaα 是平衡係數，γ\gammaγ 是焦點係數（通常取 2），可以控制簡單樣本的損失貢獻。
    
- **作用**：隨著 ptp_tpt​ 接近 1，(1−pt)γ(1 - p_t)^\gamma(1−pt​)γ 的值趨近於 0，從而降低簡單樣本的權重，使模型更關注難樣本。
    
```
import torch
import torch.nn as nn

class FocalLoss(nn.Module):
    def __init__(self, alpha=1, gamma=2):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma

    def forward(self, inputs, targets):
        BCE_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        pt = torch.exp(-BCE_loss)  # 計算pt
        focal_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss
        return focal_loss.mean()

```

#### GHM（Gradient Harmonizing Mechanism）

**GHM** 是一種通過調節梯度的大小來平衡樣本的方法。GHM 將樣本按梯度大小分成不同的 bin（區間），並根據 bin 中樣本的數量對每個樣本進行權重調整。

- **原理**：梯度小的樣本通常是簡單樣本，梯度大的樣本則是難樣本。GHM 會給予簡單樣本較低的權重，而對難樣本給予較高的權重，從而達到平衡。

```
import torch
import torch.nn as nn

class GHMLoss(nn.Module):
    def __init__(self, bins=10):
        super(GHMLoss, self).__init__()
        self.bins = bins

    def forward(self, logits, targets):
        gradients = torch.abs(logits - targets)  # 計算梯度大小
        max_grad = gradients.max()
        bin_width = max_grad / self.bins
        weights = torch.zeros_like(gradients)

        # 分配樣本權重
        for i in range(self.bins):
            mask = (gradients >= i * bin_width) & (gradients < (i + 1) * bin_width)
            weights[mask] = 1 / (mask.sum() + 1e-5)
        loss = nn.functional.binary_cross_entropy_with_logits(logits, targets, reduction='none') * weights
        return loss.mean()

```

#### PISA（Positive Instance Sampling Algorithm）

**PISA** 是一種通過採樣來平衡樣本的方法，它主要針對正樣本（前景）進行調整，優先選擇對模型學習有價值的正樣本。

- **原理**：PISA 會根據樣本的重要性分數（Importance Score）來選擇前景樣本，這樣可以過濾掉冗餘的正樣本，專注於難樣本。
- **應用場景**：PISA 適合於那些前景樣本數量過多的情況，能幫助模型更有效地學習正樣本的特徵。

---

### 11. 如何解決訓練數據樣本過少的問題

當訓練數據量不足時，模型往往難以學習到足夠的特徵，容易出現過擬合（Overfitting）問題。解決樣本過少的常見方法有以下幾種：

#### 方法一：數據增強（Data Augmentation）

數據增強通過對原始數據進行各種隨機變換（如旋轉、縮放、翻轉、裁剪等），生成新的數據樣本，從而增加數據量。
```
import torchvision.transforms as transforms

transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),
    transforms.ToTensor()
])

```

#### 方法二：遷移學習（Transfer Learning）

遷移學習通過使用在大規模數據集上預訓練的模型，並對其進行微調（Fine-tuning），可以在小數據集上獲得良好的效果。
```
import torchvision.models as models
import torch.nn as nn

model = models.resnet50(pretrained=True)
for param in model.parameters():
    param.requires_grad = False  # 凍結預訓練層
model.fc = nn.Linear(model.fc.in_features, num_classes)  # 替換分類層

```

#### 方法三：生成對抗網絡（Generative Adversarial Network, GAN）

GAN 可以生成類似於訓練數據的合成數據，用於擴充數據集，特別適合圖像數據增強。

#### 方法四：使用外部數據集

在條件允許的情況下，可以考慮使用外部相似數據集，並通過微調適配目標任務。這種方法適合於圖像分類等相對通用的任務。

---

### 12. 如何解決類別不平衡問題

在樣本中，如果某些類別的數量遠大於其他類別，則稱為**類別不平衡（Class Imbalance）**問題。這會導致模型偏向於多數類別，影響少數類別的預測準確性。

#### 方法一：調整損失函數（Loss Function）

- **Weighted Cross Entropy**：根據每個類別的樣本數量給損失函數加權，對少數類別給予更高的權重。
```
import torch.nn as nn

class_weights = torch.tensor([0.1, 0.9])  # 假設類別1佔多數
criterion = nn.CrossEntropyLoss(weight=class_weights)

```
- **Focal Loss**：Focal Loss 可以通過降低多數類別簡單樣本的損失權重，減少多數類別對模型的影響，參見上文代碼。

#### 方法二：重採樣（Resampling）

- **過採樣（Oversampling）**：通過複製少數類別樣本，增加其數量。
- **欠採樣（Undersampling）**：隨機去除部分多數類別樣本，平衡類別分佈。
```
from imblearn.over_sampling import RandomOverSampler

X_resampled, y_resampled = RandomOverSampler().fit_resample(X_train, y_train)

```

#### 方法三：生成樣本（Synthetic Sample Generation）

- **SMOTE（Synthetic Minority Over-sampling Technique）**：使用插值方法生成新的少數類別樣本，適合於數據不平衡問題。
```
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

```

#### 方法四：集成學習（Ensemble Learning）

通過不同類別的子模型組合，針對少數類別進行專門訓練，最終集成得到更平衡的預測結果。例如，**Bagging** 或 **Boosting** 方法可通過調整權重的方式平衡類別影響。

---

### 總結

- **前景背景樣本不均衡解決方案**：可以採用 Focal Loss、GHM 和 PISA，其中 Focal Loss 通過損失加權減少簡單樣本的影響，GHM 透過梯度加權平衡難樣本，PISA 則優化正樣本選擇。
- **訓練數據樣本過少的解決方案**：通過數據增強、遷移學習、生成對抗網絡和使用外部數據集來擴充數據。
- **類別不平衡的解決方案**：調整損失函數權重、進行重採樣、使用 SMOTE 生成新樣本，或採用集成學習方法來平衡各類別樣本的影響。

### 13. 手撕代碼 IOU（Intersection over Union）

**IOU（Intersection over Union）** 是衡量兩個邊界框（Bounding Boxes）重疊程度的指標，通常用於目標檢測的評估。IOU 的計算方式為交集面積除以聯合集面積，範圍在 0 到 1 之間。

#### IOU 計算公式

IOU=Intersection AreaUnion Area\text{IOU} = \frac{\text{Intersection Area}}{\text{Union Area}}IOU=Union AreaIntersection Area​

- **Intersection Area**：兩個邊界框的重疊區域面積。
- **Union Area**：兩個邊界框的總面積，去掉重疊部分。

#### 手撕代碼 IOU 示例
```
def iou(box1, box2):
    # box = [x1, y1, x2, y2]
    x1_inter = max(box1[0], box2[0])
    y1_inter = max(box1[1], box2[1])
    x2_inter = min(box1[2], box2[2])
    y2_inter = min(box1[3], box2[3])
    
    # 計算交集區域面積
    inter_area = max(0, x2_inter - x1_inter) * max(0, y2_inter - y1_inter)
    
    # 計算聯合集面積
    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union_area = box1_area + box2_area - inter_area
    
    # 計算 IOU
    iou_value = inter_area / union_area if union_area != 0 else 0
    return iou_value

```

---

### 14. 手撕代碼 NMS（Non-Maximum Suppression）

**NMS（Non-Maximum Suppression）** 是一種用於去除重疊檢測框的算法，在目標檢測中，用於保留最有代表性的邊界框。NMS 根據預測置信度（Confidence）和 IOU 計算，刪除那些重疊率較高且置信度較低的邊界框。

#### NMS 實現步驟

1. 根據置信度對檢測框排序。
2. 取出置信度最高的檢測框作為基準框，計算其他框與該框的 IOU。
3. 刪除 IOU 高於閾值的框，重複此過程直到篩選完成。

#### 手撕代碼 NMS 示例
```
def nms(boxes, scores, iou_threshold=0.5):
    # boxes = [[x1, y1, x2, y2], ...]
    # scores = [score1, score2, ...]
    indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)
    selected_boxes = []
    
    while indices:
        curr_index = indices.pop(0)
        selected_boxes.append(boxes[curr_index])
        
        # 篩選出與當前框 IOU 小於閾值的索引
        indices = [
            i for i in indices 
            if iou(boxes[curr_index], boxes[i]) < iou_threshold
        ]
    
    return selected_boxes

```

---

### 15. NMS 的改進思路

傳統的 NMS 存在一定的局限性，特別是在重疊率較高的情況下，可能會刪除掉一些有價值的邊界框。以下是常見的 NMS 改進方法：

#### 1. Soft-NMS

**Soft-NMS** 通過根據 IOU 動態調整每個邊界框的置信度，而不是直接刪除重疊框。這樣做可以保留部分重疊率較高但置信度也較高的邊界框。

- **核心公式**：對於每個候選框 BiB_iBi​，更新置信度 SiS_iSi​：
    
    Si=Si×e−IOU(B,Bi)2σS_i = S_i \times e^{-\frac{{\text{IOU}(B, B_i)^2}}{\sigma}}Si​=Si​×e−σIOU(B,Bi​)2​
    
    其中，σ\sigmaσ 是一個控制置信度衰減的參數。
    
- **代碼實現示例**：
    
```
import math

def soft_nms(boxes, scores, iou_threshold=0.5, sigma=0.5):
    indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)
    selected_boxes = []
    
    while indices:
        curr_index = indices.pop(0)
        selected_boxes.append(boxes[curr_index])
        
        for i in indices:
            iou_val = iou(boxes[curr_index], boxes[i])
            if iou_val > iou_threshold:
                scores[i] *= math.exp(-(iou_val ** 2) / sigma)
        
        # 重新排序索引以選取下一個最高置信度
        indices = sorted(indices, key=lambda i: scores[i], reverse=True)
    
    return selected_boxes

```

#### 2. Adaptive NMS

**Adaptive NMS** 允許根據物體的密集程度動態調整 IOU 閾值，密集的區域使用較低的 IOU 閾值，稀疏區域使用較高的閾值。

#### 3. Weighted NMS

**Weighted NMS** 將重疊框的位置信息合併成一個框，而不是直接刪除重疊的框。這種方法對於多次檢測到同一目標的情況，能夠生成更精確的框。

---

### 16. IOU 相關優化（GIOU, DIOU, CIOU）

在目標檢測中，IOU 被廣泛用於評估邊界框質量，但 IOU 本身也有局限性，尤其在兩個框不重疊的情況下 IOU = 0，無法衡量兩個框的相對位置。為了改進 IOU，提出了以下優化方法：

#### 1. GIOU（Generalized Intersection over Union）

**GIOU** 是 IOU 的改進版，旨在解決 IOU 對於不重疊框無法衡量的問題。GIOU 在 IOU 的基礎上增加了兩個框之間的最小封閉框面積差。

- **公式**： GIOU=IOU−C−UC\text{GIOU} = \text{IOU} - \frac{C - U}{C}GIOU=IOU−CC−U​ 其中，CCC 是包含兩個框的最小封閉框的面積，UUU 是兩個框的聯合集面積。
```
def giou(box1, box2):
    iou_value = iou(box1, box2)
    
    # 最小封閉框
    x1_c = min(box1[0], box2[0])
    y1_c = min(box1[1], box2[1])
    x2_c = max(box1[2], box2[2])
    y2_c = max(box1[3], box2[3])
    c_area = (x2_c - x1_c) * (y2_c - y1_c)
    
    giou_value = iou_value - (c_area - (box1_area + box2_area - inter_area)) / c_area
    return giou_value

```

#### 2. DIOU（Distance Intersection over Union）

**DIOU** 引入兩個框的中心點距離，解決了 GIOU 無法準確反映兩框中心點距離的問題。

- **公式**： DIOU=IOU−d2c2\text{DIOU} = \text{IOU} - \frac{d^2}{c^2}DIOU=IOU−c2d2​ 其中，ddd 是兩個框的中心點距離，ccc 是最小封閉框的對角線距離。

```
import numpy as np

def diou(box1, box2):
    iou_value = iou(box1, box2)
    center1 = [(box1[0] + box1[2]) / 2, (box1[1] + box1[3]) / 2]
    center2 = [(box2[0] + box2[2]) / 2, (box2[1] + box2[3]) / 2]
    center_distance = np.sum((np.array(center1) - np.array(center2)) ** 2)
    
    x1_c = min(box1[0], box2[0])
    y1_c = min(box1[1], box2[1])
    x2_c = max(box1[2], box2[2])
    y2_c = max(box1[3], box2[3])
    c_diagonal = (x2_c - x1_c) ** 2 + (y2_c - y1_c) ** 2
    
    diou_value = iou_value - center_distance / c_diagonal
    return diou_value

```

#### 3. CIOU（Complete Intersection over Union）

**CIOU** 是在 DIOU 基礎上進一步加入了長寬比的懲罰項，從而加強了對邊界框形狀的考慮。

- **公式**： CIOU=IOU−d2c2−αv\text{CIOU} = \text{IOU} - \frac{d^2}{c^2} - \alpha vCIOU=IOU−c2d2​−αv 其中，vvv 是邊界框的長寬比懲罰項，α\alphaα 是一個權重系數。

```
def ciou(box1, box2):
    diou_value = diou(box1, box2)
    w1, h1 = box1[2] - box1[0], box1[3] - box1[1]
    w2, h2 = box2[2] - box2[0], box2[3] - box2[1]
    
    v = (4 / (np.pi ** 2)) * (np.arctan(w1 / h1) - np.arctan(w2 / h2)) ** 2
    alpha = v / (1 - diou_value + v)
    
    ciou_value = diou_value - alpha * v
    return ciou_value

```

---

### 總結

- **IOU** 是衡量兩個框重疊程度的指標，常用於目標檢測評估。
- **NMS** 用於去除多餘的重疊框，Soft-NMS、Adaptive NMS 和 Weighted NMS 是其改進版本。
- **GIOU、DIOU、CIOU** 是對 IOU 的優化，通過加入封閉框、中心點距離和長寬比懲罰項，解決了傳統 IOU 的不足。