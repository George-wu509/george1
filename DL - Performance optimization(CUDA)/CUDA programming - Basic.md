

|                                 |     |
| ------------------------------- | --- |
| [[### CUDA Programming è©³ç´°ä»‹ç´¹]]   |     |
| [[###CUDA Programming æ˜¯ä»€éº¼ï¼Ÿ]]    |     |
| [[###CUDA kernelåšlayer fusion]] |     |
|                                 |     |
|                                 |     |


### CUDA Programming è©³ç´°ä»‹ç´¹

CUDA (Compute Unified Device Architecture) æ˜¯ç”± NVIDIA é–‹ç™¼çš„ä¸€ç¨®ä¸¦è¡Œè¨ˆç®—å¹³å°å’Œ APIï¼Œå…è¨±é–‹ç™¼è€…åˆ©ç”¨ GPU ä¾†åŠ é€Ÿè¨ˆç®—å¯†é›†å‹æ‡‰ç”¨ã€‚CUDA ä¸»è¦åŸºæ–¼ **SIMT (Single Instruction, Multiple Thread)** æ¶æ§‹ï¼Œæ¯å€‹ GPU æ ¸å¿ƒå¯ä»¥åŒæ™‚åŸ·è¡Œç›¸åŒçš„æŒ‡ä»¤ï¼Œä½†å¯ä»¥é‡å°ä¸åŒçš„æ•¸æ“šé€²è¡Œæ“ä½œã€‚

CUDA çš„ä¸»è¦çµ„ä»¶åŒ…æ‹¬ï¼š

- **Threads (ç·šç¨‹)ï¼š** å–®å€‹ GPU æ ¸å¿ƒåŸ·è¡Œçš„æœ€å°å–®ä½ã€‚
- **Blocks (å€å¡Š)ï¼š** ä¸€çµ„ GPU ç·šç¨‹ã€‚
- **Grids (ç¶²æ ¼)ï¼š** ä¸€çµ„ Blocksï¼Œå®šç¾©äº† CUDA æ ¸å¿ƒçš„é‹è¡Œç¯„åœã€‚
- **Shared Memory (å…±äº«è¨˜æ†¶é«”)ï¼š** å¯åœ¨åŒä¸€å€‹ Block å…§å…±äº«æ•¸æ“šçš„è¨˜æ†¶é«”å€åŸŸï¼Œæ¯” Global Memory æ›´å¿«ã€‚
- **Global Memory (å…¨åŸŸè¨˜æ†¶é«”)ï¼š** æ‰€æœ‰ç·šç¨‹éƒ½èƒ½è¨ªå•çš„è¨˜æ†¶é«”ï¼Œä½†è¨ªå•é€Ÿåº¦è¼ƒæ…¢ã€‚

---

## å¦‚ä½•ä½¿ç”¨ CUDA Programming åŠ é€Ÿ AI Segmentation Model

å‡è¨­æˆ‘å€‘æœ‰ä¸€å€‹ AI Segmentation Modelï¼Œåœ¨æ¨ç†éç¨‹ä¸­ï¼Œæˆ‘å€‘å¸Œæœ›é€é CUDA æé«˜æ¨¡å‹é‹è¡Œé€Ÿåº¦ã€‚ä¸»è¦æœ‰ä»¥ä¸‹å¹¾ç¨®åŠ é€Ÿæ–¹å¼ï¼š

1. **ä½¿ç”¨ PyTorch CUDA Tensor åŠ é€Ÿæ¨ç†**
2. **ä½¿ç”¨ CUDA Kernel ç·¨å¯« GPU åŠ é€Ÿå‡½æ•¸**
3. **åˆ©ç”¨ cuDNN å’Œ TensorRT é€²ä¸€æ­¥å„ªåŒ–**

### **æ–¹æ³• 1ï¼šä½¿ç”¨ PyTorch CUDA Tensor åŠ é€Ÿæ¨ç†**

å¦‚æœä½ çš„æ¨¡å‹æ˜¯åŸºæ–¼ PyTorchï¼Œæœ€ç°¡å–®çš„æ–¹å¼å°±æ˜¯å°‡æ¨¡å‹å’Œæ•¸æ“šè½‰æ›åˆ° CUDAï¼š
```python
import torch
import torchvision.models as models

# åŠ è¼‰ AI segmentation model (ä»¥ DeepLabV3 ç‚ºä¾‹)
model = models.segmentation.deeplabv3_resnet50(pretrained=True)

# å°‡æ¨¡å‹ç§»å‹•åˆ° GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
model.eval()

# æ¸¬è©¦è¼¸å…¥åœ–åƒ
input_tensor = torch.randn(1, 3, 512, 512).to(device)  # éš¨æ©Ÿç”Ÿæˆä¸€å¼µåœ–ç‰‡
output = model(input_tensor)

# é¡¯ç¤º segmentation mask
segmentation_mask = output['out'].argmax(dim=1).squeeze().cpu().numpy()
print(segmentation_mask.shape)  # (512, 512)

```

é€™ç¨®æ–¹å¼å¯ä»¥è®“ PyTorch è‡ªå‹•å°‡é‹ç®—è½‰æ›ç‚º CUDA é‹è¡Œï¼Œé©åˆ **æ¨ç†åŠ é€Ÿ**ã€‚

---

### **æ–¹æ³• 2ï¼šä½¿ç”¨ CUDA Kernel ç·¨å¯« GPU åŠ é€Ÿå‡½æ•¸**

æœ‰æ™‚å€™ï¼Œæˆ‘å€‘éœ€è¦åœ¨ CUDA ä¸Šè‡ªå®šç¾©é‹ç®—ï¼Œä¾‹å¦‚åŠ é€Ÿå¾Œè™•ç†æˆ–æŸäº›è‡ªè¨‚é‹ç®—ã€‚é€™æ™‚å€™å¯ä»¥ä½¿ç”¨ `torch.cuda` ä¾†ç·¨å¯« CUDA æ ¸å¿ƒå‡½æ•¸ã€‚ä¾‹å¦‚ï¼Œæˆ‘å€‘å¯ä»¥ç·¨å¯«ä¸€å€‹ç°¡å–®çš„ **2D å·ç©æ“ä½œ**ï¼š
```python
import torch
import torch.nn.functional as F
from torch.utils.cpp_extension import load

# ä½¿ç”¨ CUDA Kernel é€²è¡Œå·ç©é‹ç®—
cuda_code = """
extern "C" __global__ void conv2d_cuda(float* input, float* kernel, float* output, 
                                       int H, int W, int KH, int KW) {
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;

    if (x >= H || y >= W) return;

    float sum = 0.0;
    for (int i = 0; i < KH; ++i) {
        for (int j = 0; j < KW; ++j) {
            int xi = x + i - KH / 2;
            int yj = y + j - KW / 2;
            if (xi >= 0 && xi < H && yj >= 0 && yj < W) {
                sum += input[xi * W + yj] * kernel[i * KW + j];
            }
        }
    }
    output[x * W + y] = sum;
}
"""

# ç·¨è­¯ CUDA å…§æ ¸å‡½æ•¸
cuda_module = load(name="conv2d_cuda", sources=[], extra_cuda_sources=[cuda_code])

# æ¸¬è©¦ CUDA å…§æ ¸å‡½æ•¸
H, W = 512, 512
KH, KW = 3, 3
input_tensor = torch.randn(H, W, device="cuda")
kernel = torch.ones(KH, KW, device="cuda") / (KH * KW)  # å¹³å‡æ± åŒ–
output_tensor = torch.zeros(H, W, device="cuda")

# ä½ˆå±€ CUDA åŸ·è¡Œåƒæ•¸
threads_per_block = (16, 16)
blocks_per_grid = ((H + threads_per_block[0] - 1) // threads_per_block[0],
                   (W + threads_per_block[1] - 1) // threads_per_block[1])

# èª¿ç”¨ CUDA æ ¸å¿ƒå‡½æ•¸
cuda_module.conv2d_cuda(input_tensor.data_ptr(), kernel.data_ptr(), output_tensor.data_ptr(),
                        H, W, KH, KW, block=threads_per_block, grid=blocks_per_grid)

# å–å›è¼¸å‡º
output_tensor = output_tensor.cpu().numpy()
print(output_tensor.shape)  # (512, 512)

```

é€™ç¨®æ–¹å¼é©ç”¨æ–¼ **è‡ªå®šç¾© CUDA é‹ç®—**ï¼Œå¯ä»¥åŠ é€Ÿç‰¹å®šçš„æ•¸å­¸é‹ç®—ï¼Œä¾‹å¦‚å·ç©ã€æ± åŒ–æˆ–å¾Œè™•ç†æ­¥é©Ÿã€‚

---

### **æ–¹æ³• 3ï¼šä½¿ç”¨ cuDNN å’Œ TensorRT é€²ä¸€æ­¥å„ªåŒ–**

å¦‚æœä½ å¸Œæœ›é€²ä¸€æ­¥å„ªåŒ– AI segmentation modelï¼ŒNVIDIA æä¾› **cuDNN (CUDA Deep Neural Network Library)** å’Œ **TensorRT** ä¾†åŠ é€Ÿæ¨ç†ã€‚

ä½¿ç”¨ TensorRT å¯ä»¥è®“ PyTorch æ¨¡å‹åŠ é€Ÿï¼Œä¾‹å¦‚ï¼š
```python
import torch
import torch_tensorrt

# è½‰æ› PyTorch æ¨¡å‹ç‚º TensorRT åŠ é€Ÿç‰ˆæœ¬
trt_model = torch_tensorrt.compile(model, inputs=[torch_tensorrt.Input((1, 3, 512, 512), dtype=torch.float32)], enabled_precisions={torch.float16})

# é€²è¡Œæ¨ç†
output_trt = trt_model(input_tensor)
print(output_trt['out'].shape)  # (1, 21, 512, 512) for DeepLabV3 segmentation

```

é€™ç¨®æ–¹æ³•é©ç”¨æ–¼ **ç”Ÿç”¢ç’°å¢ƒçš„ AI æ¨ç†åŠ é€Ÿ**ï¼Œå¯ä»¥ç²å¾—æ¥µè‡´æ•ˆèƒ½ã€‚

---

## **å…¶ä»–å…©å€‹ä¾‹å­**

### **ä¾‹å­ 1ï¼šä½¿ç”¨ CUDA åŠ é€ŸçŸ©é™£ä¹˜æ³•**
```python
import torch

# å‰µå»ºå…©å€‹éš¨æ©ŸçŸ©é™£
A = torch.randn(1000, 1000).cuda()
B = torch.randn(1000, 1000).cuda()

# åœ¨ GPU ä¸Šè¨ˆç®—çŸ©é™£ä¹˜æ³•
C = torch.matmul(A, B)

print(C.shape)  # (1000, 1000)

```

é€™ç¨®æ–¹å¼é©ç”¨æ–¼ **å¤§å‹æ•¸æ“šè™•ç†**ï¼Œå¦‚ç¥ç¶“ç¶²è·¯å‰å‘å‚³æ’­è¨ˆç®—ã€‚

---

### **ä¾‹å­ 2ï¼šä½¿ç”¨ CUDA åŠ é€Ÿå½±åƒéŠ³åŒ–**
```python
import torch
import torch.nn.functional as F

# å®šç¾©å½±åƒéŠ³åŒ–çš„å·ç©æ ¸
sharpen_kernel = torch.tensor([[-1, -1, -1],
                               [-1,  9, -1],
                               [-1, -1, -1]], dtype=torch.float32, device="cuda").unsqueeze(0).unsqueeze(0)

# è¼‰å…¥å½±åƒä¸¦è½‰æ›ç‚º Tensor
image = torch.randn(1, 1, 512, 512, device="cuda")  # å‡è¨­ç‚ºç°éšå½±åƒ

# åŸ·è¡Œå·ç©
sharpened_image = F.conv2d(image, sharpen_kernel, padding=1)

print(sharpened_image.shape)  # (1, 1, 512, 512)

```

é€™ç¨®æ–¹å¼é©ç”¨æ–¼ **å½±åƒè™•ç†åŠ é€Ÿ**ï¼Œå¦‚é‚Šç·£æª¢æ¸¬æˆ–å½±åƒå¢å¼·ã€‚

---

## **çµè«–**

ä½¿ç”¨ CUDA å¯ä»¥é¡¯è‘—æå‡ AI segmentation model çš„æ¨ç†é€Ÿåº¦ï¼Œæ–¹æ³•åŒ…æ‹¬ï¼š

1. **PyTorch CUDA Tensor åŠ é€Ÿ**
2. **è‡ªå®šç¾© CUDA Kernel**
3. **TensorRT é€²ä¸€æ­¥å„ªåŒ–**

ä¸¦ä¸” CUDA ä¹Ÿé©ç”¨æ–¼çŸ©é™£è¨ˆç®—ã€å½±åƒè™•ç†ç­‰å„ç¨®æ‡‰ç”¨å ´æ™¯ã€‚ğŸš€




### CUDA Programming æ˜¯ä»€éº¼ï¼Ÿ

CUDA Programming **ä¸åƒ…åƒ…** æŒ‡ CUDA Kernel ç·¨å¯« GPU åŠ é€Ÿå‡½æ•¸ï¼Œé‚„åŒ…æ‹¬ä½¿ç”¨ CUDA ç›¸é—œ APIï¼ˆå¦‚ `CUDA C/C++`ã€`cuDNN`ã€`TensorRT`ï¼‰ä¾†å„ªåŒ– GPU é‹ç®—ã€‚  
PyTorch CUDA Tensor åŠ é€Ÿæ¨ç† **ç¢ºå¯¦ä¹Ÿæ˜¯ CUDA Programming çš„ä¸€éƒ¨åˆ†**ï¼Œä½†å®ƒçš„åº•å±¤ä»ç„¶ä¾è³´æ–¼ CUDA Kernelã€‚

---

## **PyTorch CUDA Tensor åŠ é€Ÿæ¨ç†æ˜¯å¦å…§éƒ¨åŸç†ä¹Ÿæ˜¯ç”¨ CUDA Kernelï¼Ÿ**

### âœ… **æ˜¯çš„ï¼ŒPyTorch CUDA Tensor å…§éƒ¨ç¢ºå¯¦ä½¿ç”¨ CUDA Kernel**

ç•¶ä½ ä½¿ç”¨ PyTorch çš„ `model.to("cuda")` æˆ– `tensor.to("cuda")` æ™‚ï¼ŒPyTorch æœƒï¼š

1. **å°‡ Tensor å­˜æ”¾åˆ° GPU è¨˜æ†¶é«”**ï¼ˆGlobal Memoryï¼‰
2. **èª¿ç”¨å·²å„ªåŒ–çš„ CUDA æ ¸å¿ƒå‡½æ•¸**ï¼ˆé€™äº›å‡½æ•¸ä¾†è‡ª NVIDIA cuBLASã€cuDNN æˆ– PyTorch æœ¬èº«çš„ CUDA Kernelï¼‰
3. **åœ¨ GPU ä¸ŠåŸ·è¡ŒçŸ©é™£é‹ç®—ã€å·ç©ç­‰æ“ä½œ**
4. **å°‡çµæœè¿”å› CPU æˆ–ç¹¼çºŒåœ¨ GPU è™•ç†**

ä½ å¯ä»¥æª¢æŸ¥ PyTorch å…§éƒ¨æ˜¯å¦ä½¿ç”¨ CUDAï¼š

python

è¤‡è£½ç·¨è¼¯

`import torch print(torch.cuda.is_available())  # True è¡¨ç¤º PyTorch å·²æ”¯æ´ CUDA print(torch.backends.cudnn.enabled)  # True è¡¨ç¤º cuDNN å·²å•Ÿç”¨`

èˆ‰ä¾‹ä¾†èªªï¼Œç•¶ä½ é€™æ¨£å¯«ï¼š

python

è¤‡è£½ç·¨è¼¯

`A = torch.randn(1000, 1000).cuda() B = torch.randn(1000, 1000).cuda() C = torch.matmul(A, B)  # é€™è£¡ PyTorch æœƒè‡ªå‹•èª¿ç”¨ CUDA æ ¸å¿ƒ`

PyTorch **åº•å±¤æœƒèª¿ç”¨ cuBLAS (CUDA Basic Linear Algebra Subprograms) ä¾†åŸ·è¡ŒçŸ©é™£ä¹˜æ³•**ï¼Œé€™æ˜¯ NVIDIA æä¾›çš„é«˜æ•ˆèƒ½ CUDA æ ¸å¿ƒå‡½æ•¸ã€‚

ä½ å¯ä»¥é–‹å•Ÿ **CUDA Profiling** ä¾†æŸ¥çœ‹ PyTorch å¯¦éš›ä½¿ç”¨çš„ CUDA æ ¸å¿ƒï¼š

python

è¤‡è£½ç·¨è¼¯

`import torch with torch.autograd.profiler.profile(use_cuda=True) as prof:     C = torch.matmul(A, B) print(prof)  # é€™è£¡æœƒåˆ—å‡º CUDA Kernel èª¿ç”¨è¨˜éŒ„`

ç¸½çµä¾†èªªï¼š

- **PyTorch CUDA Tensor** æ˜¯ **å°è£å¥½çš„ CUDA æ ¸å¿ƒ**ï¼Œä½ ä¸éœ€è¦è‡ªå·±å¯« CUDA Kernelï¼Œå°±èƒ½è®“é‹ç®—åœ¨ GPU ä¸ŠåŸ·è¡Œã€‚
- **PyTorch å…§éƒ¨æœƒæ ¹æ“šé‹ç®—é¡å‹èª¿ç”¨æœ€ä½³åŒ–çš„ CUDA Kernel**ï¼ˆä¾‹å¦‚ `cuDNN` ç”¨æ–¼å·ç©ï¼Œ`cuBLAS` ç”¨æ–¼çŸ©é™£é‹ç®—ï¼‰ã€‚

---

## **é™¤äº† CUDA Kernelï¼ŒCUDA é‚„æœ‰å“ªäº›åŠ é€Ÿæ–¹å¼ï¼Ÿ**

CUDA æä¾›è¨±å¤š **ä¸åŒå±¤ç´š** çš„åŠ é€ŸæŠ€è¡“ï¼Œä¸åªæ˜¯è‡ªå·±å¯« CUDA Kernelï¼Œé‚„å¯ä»¥ç”¨ CUDA Libraryã€Graphã€TensorRTã€Multi-GPU ç­‰æŠ€è¡“ä¾†é€²ä¸€æ­¥å„ªåŒ–ï¼š

### **1. ä½¿ç”¨ cuBLAS/cuDNN ç­‰ CUDA Library åŠ é€Ÿ**

NVIDIA æä¾›äº†è¨±å¤š CUDA å„ªåŒ–çš„å‡½å¼åº«ï¼š

- **cuBLAS**ï¼šç”¨æ–¼çŸ©é™£è¨ˆç®—ï¼ˆå¦‚ `torch.matmul`ï¼‰ã€‚
- **cuDNN**ï¼šç”¨æ–¼æ·±åº¦å­¸ç¿’çš„å·ç©ã€æ± åŒ–ç­‰é‹ç®—ï¼ˆå¦‚ `torch.nn.Conv2d`ï¼‰ã€‚
- **cuSPARSE**ï¼šç”¨æ–¼ç¨€ç–çŸ©é™£è¨ˆç®—ã€‚
- **cuFFT**ï¼šç”¨æ–¼ FFT è®Šæ›ã€‚

ä½ å¯ä»¥ç›´æ¥èª¿ç”¨é€™äº› CUDA Libraryï¼Œæˆ–è€…é€é PyTorch ä¾†ä½¿ç”¨ï¼š

python

è¤‡è£½ç·¨è¼¯

`import torch  # é€™è£¡ PyTorch å…§éƒ¨æœƒèª¿ç”¨ cuDNN ä¾†åŠ é€Ÿ Conv2D conv = torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3).cuda() input_tensor = torch.randn(1, 3, 224, 224).cuda() output = conv(input_tensor)`

---

### **2. CUDA Graph (æé«˜è¨ˆç®—æ•ˆç‡)**

- é©ç”¨æ–¼ **é‡è¤‡æ€§è¨ˆç®—**ï¼Œå¦‚ batch inferenceï¼Œèƒ½å¤  **æ¸›å°‘ Kernel Launch é–‹éŠ·**ã€‚
- **åŸç†**ï¼šå°‡é‹ç®—è®Šæˆ **è¨ˆç®—åœ– (Graph Execution)**ï¼Œè®“ GPU ä¸ç”¨æ¯æ¬¡éƒ½é‡æ–°è§£æ Kernel ä¾è³´é—œä¿‚ã€‚

python

è¤‡è£½ç·¨è¼¯

`import torch  # æº–å‚™æ¨¡å‹å’Œè¼¸å…¥ model = torch.nn.Conv2d(3, 64, 3).cuda() input_tensor = torch.randn(1, 3, 224, 224, device="cuda")  # è¨˜éŒ„è¨ˆç®—åœ– stream = torch.cuda.Stream() with torch.cuda.stream(stream):     g = torch.cuda.CUDAGraph()     with torch.cuda.graph(g):         output = model(input_tensor)`

é€™å¯ä»¥ **é¡¯è‘—æ¸›å°‘æ¨ç†å»¶é²**ã€‚

---

### **3. ä½¿ç”¨ TensorRT é€²ä¸€æ­¥åŠ é€Ÿæ¨ç†**

TensorRT æ˜¯ NVIDIA æä¾›çš„ **æ·±åº¦å­¸ç¿’æ¨ç†å„ªåŒ–æ¡†æ¶**ï¼Œå¯ä»¥å°‡ PyTorch æ¨¡å‹è½‰æ›æˆé«˜æ•ˆèƒ½ CUDA Kernelï¼š

python

è¤‡è£½ç·¨è¼¯

`import torch import torch_tensorrt  model = torch.nn.Conv2d(3, 64, 3).cuda() input_tensor = torch.randn(1, 3, 224, 224, device="cuda")  trt_model = torch_tensorrt.compile(model, inputs=[torch_tensorrt.Input(input_tensor.shape, dtype=torch.float32)]) output = trt_model(input_tensor)`

TensorRT æœƒ **è‡ªå‹•èåˆé‹ç®— (Operator Fusion)**ï¼Œä¾‹å¦‚æŠŠ Conv2D å’Œ BatchNorm åˆä½µæˆä¸€å€‹ Kernelï¼ŒåŠ é€Ÿè¨ˆç®—ã€‚

---

### **4. Multi-GPU åˆ†ä½ˆå¼é‹ç®—**

ç•¶ GPU è¨˜æ†¶é«”ä¸å¤ ç”¨æ™‚ï¼Œå¯ä»¥ç”¨ **Data Parallel (DP)** æˆ– **Distributed Data Parallel (DDP)**ï¼š

python

è¤‡è£½ç·¨è¼¯

`from torch.nn.parallel import DistributedDataParallel as DDP model = DDP(model, device_ids=[0,1])`

é€™ç¨®æ–¹æ³•èƒ½å¤  **åŒæ™‚ä½¿ç”¨å¤šå¼µ GPU åŠ é€Ÿè¨ˆç®—**ã€‚

---

### **5. æ··åˆç²¾åº¦ (FP16) è¨ˆç®—**

å¯ä»¥ä½¿ç”¨ **AMP (Automatic Mixed Precision)** ä¾†æ¸›å°‘è¨˜æ†¶é«”ä½”ç”¨ä¸¦åŠ é€Ÿè¨ˆç®—ï¼š

python

è¤‡è£½ç·¨è¼¯

`scaler = torch.cuda.amp.GradScaler()  with torch.cuda.amp.autocast():     output = model(input_tensor)  scaler.scale(loss).backward() scaler.step(optimizer) scaler.update()`

é€™å¯ä»¥ **æå‡æ¨ç†é€Ÿåº¦ 2 å€**ï¼Œç‰¹åˆ¥é©ç”¨æ–¼ Transformer æˆ– CNN æ¨¡å‹ã€‚

---

### **6. ä½¿ç”¨ Shared Memory (å…±äº«è¨˜æ†¶é«”) æé«˜è¨ˆç®—æ•ˆç‡**

ç•¶ä½ å¯« CUDA Kernel æ™‚ï¼Œå¯ä»¥ä½¿ç”¨ **å…±äº«è¨˜æ†¶é«” (Shared Memory)** ä¾†åŠ é€Ÿè®€å–ï¼š

cpp

è¤‡è£½ç·¨è¼¯

`__global__ void shared_memory_example(float *input, float *output) {     __shared__ float smem[1024];  // å…±äº«è¨˜æ†¶é«”      int tid = threadIdx.x;     smem[tid] = input[tid];  // æŠŠæ•¸æ“šæ”¾å…¥å…±äº«è¨˜æ†¶é«”      __syncthreads();  // åŒæ­¥æ‰€æœ‰ç·šç¨‹      output[tid] = smem[tid] * 2;  // è¨ˆç®—ä¸¦è¼¸å‡º }`

é€™æ¨£ **è¨˜æ†¶é«”å­˜å–æœƒæ¯” Global Memory æ›´å¿«**ï¼Œæå‡è¨ˆç®—æ•ˆèƒ½ã€‚

---

## **ç¸½çµ**

CUDA Programming ä¸åƒ…åƒ…æ˜¯è‡ªå·±å¯« CUDA Kernelï¼Œé‚„åŒ…æ‹¬ï¼š

|**æŠ€è¡“**|**æ˜¯å¦å…§éƒ¨ç”¨ CUDA Kernel**|**å„ªé»**|
|---|---|---|
|**PyTorch CUDA Tensor**|âœ…|ç›´æ¥ä½¿ç”¨ GPU é‹ç®—|
|**CUDA Kernel (è‡ªå®šç¾©é‹ç®—)**|âœ…|å®Œå…¨æ§åˆ¶ CUDA é‹ç®—|
|**cuBLAS/cuDNN (NVIDIA å‡½å¼åº«)**|âœ…|å…§å»ºå„ªåŒ–çš„é«˜æ•ˆèƒ½ Kernel|
|**CUDA Graph**|âœ…|æ¸›å°‘ Kernel å•Ÿå‹•é–‹éŠ·|
|**TensorRT**|âœ…|è‡ªå‹•å„ªåŒ–æ¨ç†|
|**Multi-GPU (DDP)**|âœ…|æé«˜è¨ˆç®—è¦æ¨¡|
|**AMP æ··åˆç²¾åº¦ (FP16)**|âœ…|é™ä½è¨˜æ†¶é«”ä½¿ç”¨ï¼ŒåŠ é€Ÿæ¨ç†|
|**CUDA Shared Memory**|âœ…|æ›´å¿«çš„è¨˜æ†¶é«”å­˜å–|

é€™äº›æŠ€è¡“ **éƒ½å±¬æ–¼ CUDA Programming**ï¼Œä½ å¯ä»¥æ ¹æ“šè‡ªå·±çš„éœ€æ±‚é¸æ“‡åˆé©çš„æ–¹æ³•ä¾†æå‡ segmentation model çš„æ•ˆèƒ½ ğŸš€ã€‚



Reference:
CUDAå¿«é€Ÿå…¥é—¨-01-åŸºç¡€è¯­æ³•æ¦‚å¿µ - é¦™è‰ç¾äººçš„æ–‡ç«  - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/16641687170


### CUDA kernelåšlayer fusion

é€™æ˜¯ä¸€å€‹ä½¿ç”¨ CUDA Kernel å¯¦ç¾ CNN ä¸­å·ç©å±¤ (Convolution)ã€æ‰¹æ¬¡æ¨™æº–åŒ–å±¤ (Batch Normalization) å’Œ ReLU æ¿€æ´»å‡½æ•¸å±¤èåˆçš„å…·é«”ç¯„ä¾‹ã€‚æˆ‘å°‡æä¾›å®Œæ•´çš„ PyTorch ç«¯ç¨‹å¼ç¢¼ã€CUDA/C++ ç›¸é—œç¨‹å¼ç¢¼ï¼Œä¸¦ç”¨ç¹é«”ä¸­æ–‡è©³ç´°è§£é‡‹ã€‚

é€™å€‹ç¯„ä¾‹çš„ç›®æ¨™æ˜¯å°‡ä»¥ä¸‹é †åºçš„æ“ä½œèåˆåˆ°ä¸€å€‹å–®ä¸€çš„ CUDA Kernel ä¸­ï¼š `Input -> Conv2d -> BatchNorm2d -> ReLU -> Output`

é€™æ¨£åšçš„ä¸»è¦å„ªé»æ˜¯ï¼š

1. **æ¸›å°‘ Kernel å•Ÿå‹•é–‹éŠ·ï¼š** å¤šå€‹æ“ä½œåˆä½µæˆä¸€å€‹ï¼Œåªéœ€è¦å•Ÿå‹•ä¸€æ¬¡ CUDA Kernelï¼Œè€Œä¸æ˜¯ä¸‰æ¬¡ã€‚
2. **æ¸›å°‘è¨˜æ†¶é«”è®€å¯«ï¼š** ä¸­é–“çµæœ (Conv2d çš„è¼¸å‡ºã€BatchNorm2d çš„è¼¸å‡º) ä¸éœ€è¦å¯«å›å…¨åŸŸè¨˜æ†¶é«” (Global Memory) å†è®€å‡ºä¾†ï¼Œå¯ä»¥åœ¨ GPU çš„æš«å­˜å™¨ (Registers) æˆ–å…±äº«è¨˜æ†¶é«” (Shared Memory) ä¸­ç›´æ¥å‚³éï¼Œå¤§å¹…é™ä½è¨˜æ†¶é«”é »å¯¬çš„å£“åŠ›ã€‚

---

**æ•´é«”æ¶æ§‹**

1. **PyTorch ç«¯ (`fused_layer.py`)**:
    - å®šç¾©ä¸€å€‹ç¹¼æ‰¿è‡ª `torch.autograd.Function` çš„é¡ï¼Œç”¨æ–¼é€£æ¥ PyTorch çš„è‡ªå‹•å¾®åˆ†ç³»çµ±å’Œæˆ‘å€‘çš„è‡ªè¨‚ CUDA æ“ä½œã€‚é€™å€‹é¡éœ€è¦å¯¦ä½œ `forward` å’Œ `backward` éœæ…‹æ–¹æ³•ã€‚
    - å®šç¾©ä¸€å€‹ç¹¼æ‰¿è‡ª `torch.nn.Module` çš„é¡ (`FusedLayer`)ï¼Œä½œç‚ºä½¿ç”¨è€…æ¥å£ã€‚å®ƒæœƒåˆå§‹åŒ–æ‰€éœ€çš„åƒæ•¸ (æ¬Šé‡ã€åç½®ã€BatchNorm åƒæ•¸ç­‰) ä¸¦åœ¨ `forward` æ–¹æ³•ä¸­èª¿ç”¨ `torch.autograd.Function`ã€‚
2. **CUDA/C++ ç«¯ (`fused_layer_cuda.cu`)**:
    - å¯¦ä½œ CUDA Kernel (`fused_conv_bn_relu_kernel`)ï¼ŒåŸ·è¡Œèåˆå¾Œçš„è¨ˆç®—é‚è¼¯ã€‚
    - å¯¦ä½œ C++ å‡½æ•¸ (`fused_layer_forward`, `fused_layer_backward`) ä½œç‚º PyTorch å’Œ CUDA Kernel ä¹‹é–“çš„æ©‹æ¨‘ï¼Œè² è²¬æª¢æŸ¥è¼¸å…¥ã€æº–å‚™æ•¸æ“šæŒ‡é‡ã€è¨ˆç®— Kernel å•Ÿå‹•é…ç½®ä¸¦å•Ÿå‹• Kernelã€‚
3. **ç·¨è­¯è¨­å®š (`setup.py`)**:
    - ä½¿ç”¨ `torch.utils.cpp_extension` ä¾†ç·¨è­¯ C++/CUDA ç¨‹å¼ç¢¼ï¼Œç”Ÿæˆ PyTorch å¯ä»¥è¼‰å…¥çš„ Python æ¨¡çµ„ã€‚

---

**1. PyTorch ç«¯ç¨‹å¼ç¢¼ (`fused_layer.py`)**

Python

```
import torch
import torch.nn as nn
from torch.autograd import Function
from torch.nn.modules.utils import _pair
import math

# è¼‰å…¥ç·¨è­¯å¥½çš„ C++/CUDA æ“´å±•
# å‡è¨­ç·¨è­¯å¾Œç”Ÿæˆçš„æ¨¡çµ„åç¨±ç‚º 'fused_layer_cpp'
# æˆ‘å€‘ç¨å¾Œæœƒç”¨ setup.py ä¾†ç·¨è­¯
try:
    import fused_layer_cpp
except ImportError:
    print("ç„¡æ³•å°å…¥ fused_layer_cpp æ¨¡çµ„ã€‚è«‹å…ˆç·¨è­¯ C++/CUDA ç¨‹å¼ç¢¼ã€‚")
    # æä¾›ä¸€å€‹å‡çš„ä½”ä½ç¬¦ï¼Œä»¥ä¾¿ç¨‹å¼ç¢¼çµæ§‹å®Œæ•´ï¼Œä½†ç„¡æ³•å¯¦éš›é‹è¡Œ
    class FakeCppModule:
        def forward(self, *args):
            raise NotImplementedError("CUDA extension not compiled.")
        def backward(self, *args):
            raise NotImplementedError("CUDA extension not compiled.")
    fused_layer_cpp = FakeCppModule()


# å®šç¾©é€£æ¥ PyTorch è‡ªå‹•å¾®åˆ†å’Œ CUDA Kernel çš„ Function
class FusedLayerFunction(Function):

    @staticmethod
    def forward(ctx, input, weight, bias, # Conv params
                running_mean, running_var, gamma, beta, # BN params
                eps, stride, padding): # Other params
        """
        å‰å‘å‚³æ’­å‡½æ•¸
        Args:
            ctx: Context object to save information for backward pass.
            input (Tensor): Input tensor (N, C_in, H_in, W_in).
            weight (Tensor): Convolution weights (C_out, C_in, kH, kW).
            bias (Tensor): Convolution bias (C_out,). Can be None.
            running_mean (Tensor): BatchNorm running mean (C_out,).
            running_var (Tensor): BatchNorm running variance (C_out,).
            gamma (Tensor): BatchNorm weight (C_out,).
            beta (Tensor): BatchNorm bias (C_out,).
            eps (float): BatchNorm epsilon.
            stride (tuple): Convolution stride (sH, sW).
            padding (tuple): Convolution padding (pH, pW).

        Returns:
            Tensor: Output tensor (N, C_out, H_out, W_out).
        """
        # ç¢ºä¿è¼¸å…¥åœ¨ CUDA ä¸Š
        assert input.is_cuda, "Input tensor must be on CUDA"
        assert weight.is_cuda, "Weight tensor must be on CUDA"
        # ... (å¯ä»¥åŠ å…¥æ›´å¤šæª¢æŸ¥)

        # ç¢ºä¿ bias æ˜¯æ­£ç¢ºçš„å½¢ç‹€æˆ– None
        if bias is not None:
            assert bias.is_cuda, "Bias tensor must be on CUDA"
            assert bias.ndim == 1 and bias.size(0) == weight.size(0)
        else:
            # å¦‚æœ bias æ˜¯ Noneï¼Œå‰µå»ºä¸€å€‹å…¨é›¶çš„ tensor å‚³éçµ¦ CUDA (æˆ–åœ¨ CUDA å…§éƒ¨è™•ç† None)
            # é€™è£¡æˆ‘å€‘é¸æ“‡å‰µå»ºä¸€å€‹å…¨é›¶ tensorï¼Œç°¡åŒ– CUDA ç«¯çš„è™•ç†
             bias = torch.zeros(weight.size(0), device=input.device, dtype=input.dtype)

        # èª¿ç”¨ C++/CUDA å¯¦ç¾çš„ forward å‡½æ•¸
        output = fused_layer_cpp.forward(input, weight, bias,
                                         running_mean, running_var,
                                         gamma, beta, eps,
                                         stride[0], stride[1],
                                         padding[0], padding[1])

        # --- ä¿å­˜åå‘å‚³æ’­æ‰€éœ€çš„å¼µé‡å’Œåƒæ•¸ ---
        # æ³¨æ„ï¼šé€™è£¡ä¿å­˜çš„æ˜¯ forward è¨ˆç®— *ä¹‹å¾Œ* çš„ outputï¼Œå› ç‚º ReLU çš„åå‘å‚³æ’­éœ€è¦å®ƒ
        # åŒæ™‚ä¹Ÿéœ€è¦ input, weight, gamma ç­‰ä¾†è¨ˆç®—æ¢¯åº¦
        # ç‚ºäº†ç°¡åŒ–ç¯„ä¾‹ï¼Œåå‘å‚³æ’­éƒ¨åˆ†æœƒéå¸¸ç²¾ç°¡ï¼Œåƒ…ä½œçµæ§‹æ¼”ç¤º
        # å¯¦éš›å®Œæ•´çš„åå‘å‚³æ’­éå¸¸è¤‡é›œ
        ctx.save_for_backward(input, weight, bias, running_mean, running_var, gamma, beta, output)
        ctx.stride = stride
        ctx.padding = padding
        ctx.eps = eps

        return output

    @staticmethod
    def backward(ctx, grad_output):
        """
        åå‘å‚³æ’­å‡½æ•¸ (ç°¡åŒ–ç‰ˆ)
        Args:
            ctx: Context object with saved tensors.
            grad_output (Tensor): Gradient of the loss with respect to the output of this layer.

        Returns:
            tuple: Gradients with respect to each input of the forward function.
                   Order must match the input order of forward().
                   Gradients for non-tensor inputs or inputs that don't require grad should be None.
        """
        # æª¢æŸ¥æ¢¯åº¦æ˜¯å¦åœ¨ CUDA ä¸Š
        assert grad_output.is_cuda, "Gradient tensor must be on CUDA"

        # å–å‡ºä¿å­˜çš„å¼µé‡
        input, weight, bias, running_mean, running_var, gamma, beta, output = ctx.saved_tensors
        stride = ctx.stride
        padding = ctx.padding
        eps = ctx.eps

        # --- èª¿ç”¨ C++/CUDA å¯¦ç¾çš„ backward å‡½æ•¸ ---
        # è­¦å‘Šï¼šä¸‹é¢é€™å€‹ C++ backward å‡½æ•¸æ˜¯æ¥µåº¦ç°¡åŒ–çš„ï¼Œ
        # å¯¦éš›çš„ Conv+BN+ReLU åå‘å‚³æ’­éå¸¸è¤‡é›œï¼Œæ¶‰åŠéˆå¼æ³•å‰‡çš„å¤šé‡å¾®åˆ†ã€‚
        # é€™å€‹ç¯„ä¾‹ä¸»è¦å±•ç¤ºçµæ§‹ï¼Œä¸å¯¦ç¾å®Œæ•´ä¸”æ­£ç¢ºçš„åå‘å‚³æ’­ã€‚
        # ä¸€å€‹çœŸå¯¦çš„å¯¦ç¾éœ€è¦è¨ˆç®— grad_input, grad_weight, grad_bias, grad_gamma, grad_betaã€‚
        # é€™è£¡çš„ CUDA backward å¯èƒ½åªç°¡å–®å¯¦ç¾äº† grad_output é€šé ReLU åå‘çš„éƒ¨åˆ†ã€‚
        grad_input, grad_weight, grad_bias, grad_gamma, grad_beta = fused_layer_cpp.backward(
            grad_output, input, weight, bias, running_mean, running_var, gamma, beta, output,
            eps, stride[0], stride[1], padding[0], padding[1]
        )

        # è¿”å›å°æ‡‰ forward è¼¸å…¥çš„æ¢¯åº¦
        # å°æ–¼ä¸éœ€è¦æ¢¯åº¦çš„è¼¸å…¥ (running_mean, running_var) æˆ–éå¼µé‡è¼¸å…¥ (eps, stride, padding) è¿”å› None
        return (grad_input, grad_weight, grad_bias,
                None, None, grad_gamma, grad_beta, # BN mean/var ä¸éœ€è¦æ¢¯åº¦, gamma/beta éœ€è¦
                None, None, None) # eps, stride, padding ä¸éœ€è¦æ¢¯åº¦


# å®šç¾©ä½¿ç”¨è€…æ¥å£çš„ Module
class FusedLayer(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True, eps=1e-5):
        """
        åˆå§‹åŒ–èåˆå±¤
        Args:
            in_channels (int): è¼¸å…¥é€šé“æ•¸
            out_channels (int): è¼¸å‡ºé€šé“æ•¸ (å·ç©æ ¸æ•¸é‡)
            kernel_size (int or tuple): å·ç©æ ¸å¤§å°
            stride (int or tuple): å·ç©æ­¥é•·
            padding (int or tuple): å·ç©å¡«å……
            bias (bool): å·ç©å±¤æ˜¯å¦ä½¿ç”¨åç½® (BatchNorm çš„ beta å¯¦éš›ä¸Šæœƒèµ·åˆ°é¡ä¼¼ä½œç”¨)
            eps (float): BatchNorm çš„ epsilonï¼Œé˜²æ­¢é™¤ä»¥é›¶
        """
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = _pair(kernel_size)
        self.stride = _pair(stride)
        self.padding = _pair(padding)
        self.eps = eps

        # --- åˆå§‹åŒ–åƒæ•¸ ---
        # 1. å·ç©å±¤åƒæ•¸
        self.weight = nn.Parameter(torch.empty(out_channels, in_channels, *self.kernel_size))
        if bias:
            self.bias = nn.Parameter(torch.empty(out_channels))
        else:
            self.register_parameter('bias', None) # è¨»å†Šç‚º None

        # 2. BatchNorm å±¤åƒæ•¸ (éœ€è¦è¨»å†Šç‚º Parameter æˆ– Buffer)
        self.gamma = nn.Parameter(torch.empty(out_channels)) # weight in BN
        self.beta = nn.Parameter(torch.empty(out_channels))  # bias in BN
        # running_mean å’Œ running_var æ˜¯ Bufferï¼Œå®ƒå€‘åœ¨è¨“ç·´ä¸­æ›´æ–°ï¼Œä½†åœ¨æ¨æ–·ä¸­ä½¿ç”¨å›ºå®šå€¼ï¼Œ
        # ä¸¦ä¸”é€šå¸¸ä¸éœ€è¦è¨ˆç®—æ¢¯åº¦ã€‚
        self.register_buffer('running_mean', torch.zeros(out_channels))
        self.register_buffer('running_var', torch.ones(out_channels))
        # self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long)) # BN å…§éƒ¨è¿½è¹¤ç”¨ï¼Œé€™è£¡å¯ä»¥çœç•¥

        # --- åˆå§‹åŒ–åƒæ•¸å€¼ ---
        self.reset_parameters()

    def reset_parameters(self):
        # ä½¿ç”¨ Kaiming He åˆå§‹åŒ–å·ç©æ¬Šé‡
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / math.sqrt(fan_in)
            nn.init.uniform_(self.bias, -bound, bound)
        # åˆå§‹åŒ– BatchNorm åƒæ•¸
        nn.init.ones_(self.gamma)
        nn.init.zeros_(self.beta)
        nn.init.zeros_(self.running_mean)
        nn.init.ones_(self.running_var)

    def forward(self, input):
        # ç¢ºä¿ Module åœ¨è¨“ç·´æ¨¡å¼ä¸‹ BN åƒæ•¸èƒ½è¢«æ­£ç¢ºè™•ç† (é›–ç„¶æˆ‘å€‘é€™è£¡æ²’å¯¦ç¾è¨“ç·´æ™‚çš„ BN é‚è¼¯)
        # åœ¨æ¨æ–·æ¨¡å¼ (eval()), running_mean/var è¢«ä½¿ç”¨
        # åœ¨è¨“ç·´æ¨¡å¼ (train()), ç†æƒ³æƒ…æ³ä¸‹æ‡‰ç”¨ä½¿ç”¨ç•¶å‰ batch çš„ mean/var æ›´æ–° running_mean/var
        # ä½†æˆ‘å€‘çš„ CUDA Kernel ç‚ºäº†ç°¡åŒ–ï¼Œç›®å‰åªä½¿ç”¨äº† running_mean/var (é¡ä¼¼æ¨æ–·æ¨¡å¼)
        # ä¸€å€‹å®Œæ•´çš„å¯¦ç¾éœ€è¦åœ¨ CUDA ä¸­è™•ç† training vs eval çš„ä¸åŒé‚è¼¯

        # ä½¿ç”¨ FusedLayerFunction åŸ·è¡Œèåˆæ“ä½œ
        return FusedLayerFunction.apply(input, self.weight, self.bias,
                                         self.running_mean, self.running_var,
                                         self.gamma, self.beta, self.eps,
                                         self.stride, self.padding)

    def extra_repr(self):
        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'
             ', stride={stride}, padding={padding}, bias={bias}, eps={eps}')
        return s.format(**self.__dict__, bias=self.bias is not None)

# --- æ¸¬è©¦ç¯„ä¾‹ ---
if __name__ == '__main__':
    # æª¢æŸ¥ CUDA æ˜¯å¦å¯ç”¨
    if not torch.cuda.is_available():
        print("CUDA is not available. Exiting.")
        exit()

    # åƒæ•¸è¨­å®š
    N, C_in, H_in, W_in = 4, 16, 32, 32  # Batch size, Input channels, Input height, Input width
    C_out = 32                           # Output channels
    kH, kW = 3, 3                        # Kernel size
    sH, sW = 1, 1                        # Stride
    pH, pW = 1, 1                        # Padding

    # å‰µå»ºè¼¸å…¥å¼µé‡ (æ”¾åˆ° CUDA ä¸Š)
    input_tensor = torch.randn(N, C_in, H_in, W_in, dtype=torch.float32, device='cuda')
    input_tensor.requires_grad_() # éœ€è¦è¨ˆç®—æ¢¯åº¦

    # --- æ–¹æ³•ä¸€ï¼šæ¨™æº– PyTorch å±¤ ---
    print("--- Standard PyTorch Layers ---")
    conv_std = nn.Conv2d(C_in, C_out, (kH, kW), stride=(sH, sW), padding=(pH, pW), bias=True).cuda()
    bn_std = nn.BatchNorm2d(C_out, eps=1e-5).cuda()
    relu_std = nn.ReLU(inplace=True).cuda() # æ³¨æ„ inplace å¯èƒ½å½±éŸ¿æ¢¯åº¦æª¢æŸ¥

    # è¨­ç½®ç‚ºè©•ä¼°æ¨¡å¼ï¼Œé€™æ¨£ BN æœƒä½¿ç”¨ running mean/var
    conv_std.eval()
    bn_std.eval()
    relu_std.eval()

    # å‰å‘å‚³æ’­
    output_conv = conv_std(input_tensor)
    # print("Conv output shape:", output_conv.shape)
    # print("BN running mean (before):", bn_std.running_mean[:5])
    # print("BN running var (before):", bn_std.running_var[:5])
    output_bn = bn_std(output_conv)
    # print("BN running mean (after):", bn_std.running_mean[:5]) # æ‡‰è©²ä¸è®Šå› ç‚º eval()
    # print("BN running var (after):", bn_std.running_var[:5])   # æ‡‰è©²ä¸è®Šå› ç‚º eval()
    output_std = relu_std(output_bn)
    print("Standard Output Shape:", output_std.shape)

    # è¨ˆç®—æ¨™æº–è¼¸å‡ºçš„æ¢¯åº¦ (ç”¨æ–¼æ¯”è¼ƒ)
    grad_output_std = torch.randn_like(output_std)
    output_std.backward(grad_output_std)
    grad_input_std = input_tensor.grad.clone() # è¤‡è£½æ¢¯åº¦
    input_tensor.grad.zero_() # æ¸…é›¶æ¢¯åº¦ä»¥ä¾¿ä¸‹æ¬¡è¨ˆç®—


    # --- æ–¹æ³•äºŒï¼šèåˆ CUDA Kernel å±¤ ---
    print("\n--- Fused CUDA Kernel Layer ---")
    fused_layer = FusedLayer(C_in, C_out, (kH, kW), stride=(sH, sW), padding=(pH, pW), bias=True, eps=1e-5).cuda()

    # å°‡æ¨™æº–å±¤çš„åƒæ•¸è¤‡è£½åˆ°èåˆå±¤ä¸­ï¼Œä»¥ç¢ºä¿è¨ˆç®—çµæœä¸€è‡´
    fused_layer.weight.data.copy_(conv_std.weight.data)
    if conv_std.bias is not None:
        fused_layer.bias.data.copy_(conv_std.bias.data)
    fused_layer.gamma.data.copy_(bn_std.weight.data) # BN weight is gamma
    fused_layer.beta.data.copy_(bn_std.bias.data)    # BN bias is beta
    fused_layer.running_mean.copy_(bn_std.running_mean)
    fused_layer.running_var.copy_(bn_std.running_var)

    # è¨­ç½®ç‚ºè©•ä¼°æ¨¡å¼ (é›–ç„¶æˆ‘å€‘çš„ Kernel ç›®å‰åªå¯¦ç¾äº†é¡ä¼¼è©•ä¼°æ¨¡å¼çš„é‚è¼¯)
    fused_layer.eval()

    # å‰å‘å‚³æ’­
    output_fused = fused_layer(input_tensor)
    print("Fused Output Shape:", output_fused.shape)

    # æ¯”è¼ƒå‰å‘è¼¸å‡ºçµæœ
    forward_diff = torch.max(torch.abs(output_std - output_fused)).item()
    print(f"Max absolute difference in forward pass: {forward_diff:.6f}")
    # ç”±æ–¼æµ®é»æ•¸è¨ˆç®—çš„å¾®å°å·®ç•°ï¼Œé€™è£¡çš„å·®å€¼ä¸ä¸€å®šç‚º 0ï¼Œä½†åœ¨ 1e-6 æˆ– 1e-7 ç¯„åœå…§é€šå¸¸æ˜¯å¯æ¥å—çš„

    # è¨ˆç®—èåˆå±¤è¼¸å‡ºçš„æ¢¯åº¦
    # æ³¨æ„ï¼šç”±æ–¼æˆ‘å€‘çš„ backward å¯¦ç¾æ˜¯ç°¡åŒ–çš„ï¼Œæ¢¯åº¦æ¯”è¼ƒå¯èƒ½ä¸æœƒåŒ¹é…
    print("\n--- Backward Pass Comparison (Simplified) ---")
    grad_output_fused = grad_output_std.clone() # ä½¿ç”¨ç›¸åŒçš„è¼¸å‡ºæ¢¯åº¦
    output_fused.backward(grad_output_fused)
    grad_input_fused = input_tensor.grad.clone()

    # æ¯”è¼ƒè¼¸å…¥æ¢¯åº¦
    backward_diff = torch.max(torch.abs(grad_input_std - grad_input_fused)).item()
    print(f"Max absolute difference in input gradient: {backward_diff:.6f}")
    print("Note: Backward difference might be large due to simplified CUDA backward implementation.")

    # æª¢æŸ¥å…¶ä»–åƒæ•¸çš„æ¢¯åº¦ (åœ¨ç°¡åŒ–ç‰ˆ backward ä¸­å¯èƒ½ç‚º None æˆ– 0)
    print("Gradient w.r.t. fused weight (sample):", fused_layer.weight.grad[0, 0, 0, :5] if fused_layer.weight.grad is not None else None)
    print("Gradient w.r.t. fused bias (sample):", fused_layer.bias.grad[:5] if fused_layer.bias is not None and fused_layer.bias.grad is not None else None)
    print("Gradient w.r.t. fused gamma (sample):", fused_layer.gamma.grad[:5] if fused_layer.gamma.grad is not None else None)
    print("Gradient w.r.t. fused beta (sample):", fused_layer.beta.grad[:5] if fused_layer.beta.grad is not None else None)

```

---

**2. CUDA/C++ ç«¯ç¨‹å¼ç¢¼ (`fused_layer_cuda.cu`)**

é€™å€‹æ–‡ä»¶åŒ…å« CUDA Kernel å’Œ C++ æ¥å£å‡½æ•¸ã€‚

C++

```
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <vector>
#include <cmath> // For std::sqrt

// Helper function for CUDA error checking
#define CUDA_CHECK(call)                                               \
do {                                                                   \
    cudaError_t err = call;                                            \
    if (err != cudaSuccess) {                                          \
        fprintf(stderr, "CUDA error in %s:%d: %s\n", __FILE__, __LINE__, \
                cudaGetErrorString(err));                              \
        /* More robust error handling might be needed here */          \
        throw std::runtime_error(cudaGetErrorString(err));             \
    }                                                                  \
} while (0)

// CUDA Kernel for Fused Conv2d + BatchNorm + ReLU (Forward Pass)
// Computes one output element (n, c_out, h_out, w_out) per thread
__global__ void fused_conv_bn_relu_kernel_forward(
    const float* __restrict__ input, // Input tensor data (N, C_in, H_in, W_in)
    const float* __restrict__ weight, // Weight tensor data (C_out, C_in, kH, kW)
    const float* __restrict__ bias,   // Bias tensor data (C_out,)
    const float* __restrict__ running_mean, // BN running mean (C_out,)
    const float* __restrict__ running_var,  // BN running variance (C_out,)
    const float* __restrict__ gamma,  // BN gamma (weight) (C_out,)
    const float* __restrict__ beta,   // BN beta (bias) (C_out,)
    float* __restrict__ output, // Output tensor data (N, C_out, H_out, W_out)
    const int N, const int C_in, const int H_in, const int W_in,
    const int C_out, const int kH, const int kW,
    const int sH, const int sW, const int pH, const int pW,
    const int H_out, const int W_out,
    const float eps)
{
    // Calculate the global thread index
    const int index = blockIdx.x * blockDim.x + threadIdx.x;

    // Calculate the total number of output elements
    const int num_outputs = N * C_out * H_out * W_out;

    // Check if the thread index is within bounds
    if (index >= num_outputs) {
        return;
    }

    // Decompose the index into (n, c_out, h_out, w_out)
    const int w_out = index % W_out;
    const int h_out = (index / W_out) % H_out;
    const int c_out = (index / (W_out * H_out)) % C_out;
    const int n = index / (W_out * H_out * C_out);

    // Calculate the corresponding top-left corner in the input feature map
    const int h_in_start = h_out * sH - pH;
    const int w_in_start = w_out * sW - pW;

    // --- Convolution Part ---
    float conv_sum = 0.0f;

    // Iterate over input channels (c_in)
    for (int c_in = 0; c_in < C_in; ++c_in) {
        // Iterate over kernel height (kh)
        for (int kh = 0; kh < kH; ++kh) {
            const int h_in = h_in_start + kh;
            // Iterate over kernel width (kw)
            for (int kw = 0; kw < kW; ++kw) {
                const int w_in = w_in_start + kw;

                // Check for padding boundaries
                if (h_in >= 0 && h_in < H_in && w_in >= 0 && w_in < W_in) {
                    // Calculate flat indices for input and weight tensors
                    // Input index: n * C_in * H_in * W_in + c_in * H_in * W_in + h_in * W_in + w_in
                    const int input_idx = n * C_in * H_in * W_in +
                                          c_in * H_in * W_in +
                                          h_in * W_in + w_in;

                    // Weight index: c_out * C_in * kH * kW + c_in * kH * kW + kh * kW + kw
                    const int weight_idx = c_out * C_in * kH * kW +
                                           c_in * kH * kW +
                                           kh * kW + kw;

                    // Accumulate the convolution result
                    conv_sum += input[input_idx] * weight[weight_idx];
                }
                // Padded values are treated as 0, so no need to add if out of bounds
            }
        }
    }

    // Add bias (if provided)
    // Bias index: c_out
    conv_sum += bias[c_out];

    // --- Batch Normalization Part ---
    // Using running mean and variance (inference mode)
    // BN Formula: y = gamma * (x - mean) / sqrt(var + eps) + beta
    const float mean = running_mean[c_out];
    const float var = running_var[c_out];
    const float inv_stddev = 1.0f / std::sqrt(var + eps); // Precompute inverse standard deviation
    // const float inv_stddev = rsqrtf(var + eps); // Potentially faster using CUDA's rsqrtf

    const float bn_output = gamma[c_out] * (conv_sum - mean) * inv_stddev + beta[c_out];

    // --- ReLU Part ---
    // ReLU Formula: y = max(0, x)
    const float final_output = fmaxf(0.0f, bn_output); // Use fmaxf for float max

    // Write the final result to the output tensor
    output[index] = final_output;
}


// CUDA Kernel for Fused Conv2d + BatchNorm + ReLU (Backward Pass - Simplified Placeholder)
// WARNING: This backward kernel is highly simplified and likely incorrect for full training.
// It primarily demonstrates passing the gradient through the ReLU activation.
// A correct implementation is significantly more complex.
__global__ void fused_conv_bn_relu_kernel_backward(
    const float* __restrict__ grad_output, // Gradient from the next layer (N, C_out, H_out, W_out)
    const float* __restrict__ output,      // Output of the forward pass (needed for ReLU backward)
    float* __restrict__ grad_input_prop, // Gradient to propagate back towards input (before BN/Conv)
    // Add other pointers needed for full gradient calculation (input, weight, gamma etc.)
    const int N, const int C_out, const int H_out, const int W_out
    // Add other necessary dimensions (C_in, H_in, W_in, kH, kW, etc.)
    )
{
    const int index = blockIdx.x * blockDim.x + threadIdx.x;
    const int num_elements = N * C_out * H_out * W_out;

    if (index >= num_elements) {
        return;
    }

    // --- ReLU Backward Part ---
    // Gradient of ReLU: grad_output if output > 0, else 0
    const float grad_relu = (output[index] > 0.0f) ? grad_output[index] : 0.0f;

    // --- Simplified Propagation ---
    // This step should involve the backward pass of BatchNorm and then Convolution.
    // For this simplified example, we just assign the ReLU gradient.
    // In a real scenario, grad_relu would be the input to the BN backward pass.
    grad_input_prop[index] = grad_relu;

    // TODO: Implement full backward pass for BN and Conv to compute:
    // - grad_input (gradient w.r.t. the original input tensor)
    // - grad_weight (gradient w.r.t. convolution weights)
    // - grad_bias (gradient w.r.t. convolution bias)
    // - grad_gamma (gradient w.r.t. BN gamma)
    // - grad_beta (gradient w.r.t. BN beta)
}


// C++ interface function for the forward pass
torch::Tensor fused_layer_forward_cuda(
    torch::Tensor input,
    torch::Tensor weight,
    torch::Tensor bias,
    torch::Tensor running_mean,
    torch::Tensor running_var,
    torch::Tensor gamma,
    torch::Tensor beta,
    double eps,
    int stride_h, int stride_w,
    int padding_h, int padding_w)
{
    // Input shape: (N, C_in, H_in, W_in)
    // Weight shape: (C_out, C_in, kH, kW)
    // Bias shape: (C_out,)
    // BN params shape: (C_out,)

    // Get tensor dimensions
    const int N = input.size(0);
    const int C_in = input.size(1);
    const int H_in = input.size(2);
    const int W_in = input.size(3);

    const int C_out = weight.size(0);
    const int kH = weight.size(2);
    const int kW = weight.size(3);

    // Calculate output dimensions
    const int H_out = (H_in + 2 * padding_h - kH) / stride_h + 1;
    const int W_out = (W_in + 2 * padding_w - kW) / stride_w + 1;

    // Create output tensor of the correct size on the same device as input
    auto output = torch::empty({N, C_out, H_out, W_out}, input.options());

    // Check tensor contiguity (important for direct pointer access)
    TORCH_CHECK(input.is_contiguous(), "Input tensor must be contiguous");
    TORCH_CHECK(weight.is_contiguous(), "Weight tensor must be contiguous");
    TORCH_CHECK(bias.is_contiguous(), "Bias tensor must be contiguous");
    TORCH_CHECK(running_mean.is_contiguous(), "Running mean tensor must be contiguous");
    TORCH_CHECK(running_var.is_contiguous(), "Running variance tensor must be contiguous");
    TORCH_CHECK(gamma.is_contiguous(), "Gamma tensor must be contiguous");
    TORCH_CHECK(beta.is_contiguous(), "Beta tensor must be contiguous");
    // Output tensor created by torch::empty is typically contiguous

    // Calculate grid and block dimensions for the CUDA kernel
    const int total_outputs = N * C_out * H_out * W_out;
    const int threads_per_block = 256; // Common block size, can be tuned
    // Equivalent to ceil(total_outputs / threads_per_block)
    const int num_blocks = (total_outputs + threads_per_block - 1) / threads_per_block;

    // Launch the CUDA kernel
    fused_conv_bn_relu_kernel_forward<<<num_blocks, threads_per_block>>>(
        input.data_ptr<float>(),
        weight.data_ptr<float>(),
        bias.data_ptr<float>(),
        running_mean.data_ptr<float>(),
        running_var.data_ptr<float>(),
        gamma.data_ptr<float>(),
        beta.data_ptr<float>(),
        output.data_ptr<float>(),
        N, C_in, H_in, W_in,
        C_out, kH, kW,
        stride_h, stride_w, padding_h, padding_w,
        H_out, W_out,
        (float)eps
    );

    // Check for kernel launch errors (asynchronous)
    CUDA_CHECK(cudaGetLastError());
    // Optional: Synchronize to wait for completion (for debugging or timing)
    // CUDA_CHECK(cudaDeviceSynchronize());

    return output;
}


// C++ interface function for the backward pass (Simplified)
std::vector<torch::Tensor> fused_layer_backward_cuda(
    torch::Tensor grad_output,
    torch::Tensor input, // Needed for grad_weight calculation
    torch::Tensor weight, // Needed for grad_input calculation
    torch::Tensor bias,   // Not usually needed directly for backward, but good practice
    torch::Tensor running_mean, // Needed for BN backward
    torch::Tensor running_var,  // Needed for BN backward
    torch::Tensor gamma,      // Needed for BN backward & grad_beta/gamma
    torch::Tensor beta,       // Needed for BN backward
    torch::Tensor output,     // Output from forward (needed for ReLU backward)
    double eps,
    int stride_h, int stride_w,
    int padding_h, int padding_w)
{
    // Get dimensions
    const int N = grad_output.size(0);
    const int C_out = grad_output.size(1);
    const int H_out = grad_output.size(2);
    const int W_out = grad_output.size(3);
    // ... get other dimensions (C_in, H_in, W_in, kH, kW) from input/weight

    // --- Prepare Output Tensors for Gradients ---
    // We need to return gradients for: input, weight, bias, gamma, beta
    auto grad_input = torch::zeros_like(input);
    auto grad_weight = torch::zeros_like(weight);
    auto grad_bias = torch::zeros_like(bias);
    auto grad_gamma = torch::zeros_like(gamma);
    auto grad_beta = torch::zeros_like(beta);

    // --- Simplified Backward Implementation ---
    // 1. Allocate temporary space for gradient after ReLU backward (before BN/Conv backward)
    auto grad_after_relu = torch::empty_like(grad_output);

    // 2. Launch the simplified backward kernel (only does ReLU backward)
    const int total_elements = N * C_out * H_out * W_out;
    const int threads_per_block = 256;
    const int num_blocks = (total_elements + threads_per_block - 1) / threads_per_block;

    fused_conv_bn_relu_kernel_backward<<<num_blocks, threads_per_block>>>(
        grad_output.data_ptr<float>(),
        output.data_ptr<float>(),       // Forward output needed for ReLU grad
        grad_after_relu.data_ptr<float>(), // Output of this simplified kernel
        N, C_out, H_out, W_out
    );
    CUDA_CHECK(cudaGetLastError());

    // 3. --- Placeholder for Full Backward Computation ---
    // At this point, 'grad_after_relu' contains the gradient *after* the ReLU backward step.
    // A full implementation would now take 'grad_after_relu' and compute:
    //    a) BatchNorm Backward: Calculate grad_gamma, grad_beta, and the gradient *before* BN.
    //    b) Convolution Backward: Using the gradient from BN backward, calculate
    //       grad_input, grad_weight, and grad_bias.
    // This involves complex operations like transposed convolution (for grad_input)
    // and correlation (for grad_weight).

    // Since this is a simplified example, we will return zero gradients for most parts,
    // except potentially showing how the structure would look.
    // For demonstration, let's imagine we somehow computed the gradients.
    // In a real scenario, you'd likely call separate kernels or a more complex fused backward kernel here.

    // Example: Assigning grad_beta (often sum of grad_after_relu * gamma / sqrt(var+eps) over N, H_out, W_out)
    // Example: Assigning grad_gamma (...)
    // Example: Calling a Conv_backward_input kernel...
    // Example: Calling a Conv_backward_weight kernel...

    // Return the computed gradients (mostly zeros in this placeholder)
    return {grad_input, grad_weight, grad_bias, grad_gamma, grad_beta};
}


// Binding the C++ functions to Python module using pybind11
PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("forward", &fused_layer_forward_cuda, "Fused Conv-BN-ReLU forward (CUDA)");
    m.def("backward", &fused_layer_backward_cuda, "Fused Conv-BN-ReLU backward (CUDA - Simplified)");
}
```

---

**3. ç·¨è­¯è¨­å®š (`setup.py`)**

é€™å€‹æ–‡ä»¶å‘Šè¨´ Python å¦‚ä½•ä½¿ç”¨ `torch.utils.cpp_extension` ä¾†ç·¨è­¯ä½ çš„ C++/CUDA ç¨‹å¼ç¢¼ã€‚

Python

```
from setuptools import setup
from torch.utils.cpp_extension import BuildExtension, CUDAExtension

setup(
    name='fused_layer_cpp', # Python è¼‰å…¥æ™‚ä½¿ç”¨çš„æ¨¡çµ„åç¨±
    ext_modules=[
        CUDAExtension(
            name='fused_layer_cpp', # å¿…é ˆå’Œä¸Šé¢ name ä»¥åŠ PYBIND11_MODULE å…§çš„åç¨±ä¸€è‡´
            sources=['fused_layer_cuda.cu'],
            # å¯é¸ï¼šæ·»åŠ é¡å¤–çš„ç·¨è­¯å™¨åƒæ•¸
            # extra_compile_args={'cxx': ['-g'], # C++ ç·¨è­¯å™¨åƒæ•¸ (ä¾‹å¦‚èª¿è©¦ç¬¦è™Ÿ)
            #                     'nvcc': ['-O3']} # NVCC ç·¨è­¯å™¨åƒæ•¸ (ä¾‹å¦‚å„ªåŒ–ç´šåˆ¥)
        ),
    ],
    cmdclass={
        'build_ext': BuildExtension
    })

```

---

**å¦‚ä½•ç·¨è­¯å’Œé‹è¡Œ**

1. **ç’°å¢ƒè¦æ±‚**:
    - å®‰è£ PyTorch (éœ€æ”¯æ´ CUDA)ã€‚
    - å®‰è£ CUDA Toolkit (ç‰ˆæœ¬éœ€èˆ‡ PyTorch ç›¸å®¹)ã€‚
    - å®‰è£ C++ ç·¨è­¯å™¨ (å¦‚ GCC/G++)ã€‚
    - å®‰è£ Python (setuptools é€šå¸¸å·²å…§å»º)ã€‚
2. **å„²å­˜æ–‡ä»¶**: å°‡ä¸Šé¢ä¸‰å€‹ç¨‹å¼ç¢¼å¡Šåˆ†åˆ¥å„²å­˜ç‚º `fused_layer.py`, `fused_layer_cuda.cu`, å’Œ `setup.py` åˆ°åŒä¸€å€‹è³‡æ–™å¤¾ä¸­ã€‚
3. **ç·¨è­¯**: åœ¨è©²è³‡æ–™å¤¾æ‰“é–‹çµ‚ç«¯æˆ–å‘½ä»¤æç¤ºå­—å…ƒï¼Œé‹è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
    
    Bash
    
    ```
    python setup.py install
    # æˆ–è€…ï¼Œå¦‚æœä½ ä¸æƒ³å®‰è£åˆ° Python ç’°å¢ƒï¼Œåªæƒ³åœ¨ç•¶å‰ç›®éŒ„ç·¨è­¯ï¼š
    # python setup.py build_ext --inplace
    ```
    
    é€™æœƒèª¿ç”¨ NVCC (CUDA ç·¨è­¯å™¨) å’Œ C++ ç·¨è­¯å™¨ä¾†ç·¨è­¯ `fused_layer_cuda.cu`ï¼Œä¸¦å°‡å…¶é€£çµæˆä¸€å€‹ Python å¯ä»¥å°å…¥çš„å…±äº«åº« (ä¾‹å¦‚ `fused_layer_cpp.cpython-38-x86_64-linux-gnu.so` æˆ–é¡ä¼¼åç¨±)ã€‚
4. **é‹è¡Œ**: ç·¨è­¯æˆåŠŸå¾Œï¼Œç›´æ¥é‹è¡Œ `fused_layer.py`:
    
    Bash
    
    ```
    python fused_layer.py
    ```
    
    å¦‚æœä¸€åˆ‡é †åˆ©ï¼Œä½ å°‡çœ‹åˆ°æ¨™æº– PyTorch å±¤å’Œèåˆ CUDA å±¤çš„å‰å‘è¼¸å‡ºæ¯”è¼ƒçµæœï¼Œä»¥åŠé—œæ–¼åå‘å‚³æ’­æ¯”è¼ƒçš„èªªæ˜ã€‚

---

**è©³ç´°è§£é‡‹**

1. **`fused_layer.py`**:
    
    - `FusedLayerFunction`: é€™æ˜¯æ ¸å¿ƒçš„æ©‹æ¨‘ã€‚`forward` éœæ…‹æ–¹æ³•æ¥æ”¶ PyTorch å¼µé‡å’Œå…¶ä»–åƒæ•¸ï¼Œèª¿ç”¨ç·¨è­¯å¥½çš„ C++ å‡½æ•¸ `fused_layer_cpp.forward`ï¼Œä¸¦ä½¿ç”¨ `ctx.save_for_backward` ä¿å­˜åå‘å‚³æ’­æ‰€éœ€çš„æ•¸æ“šã€‚`backward` éœæ…‹æ–¹æ³•æ¥æ”¶ä¸Šä¸€å±¤å‚³ä¾†çš„æ¢¯åº¦ `grad_output`ï¼Œå–å‡ºä¿å­˜çš„æ•¸æ“šï¼Œèª¿ç”¨ C++ å‡½æ•¸ `fused_layer_cpp.backward` ä¾†è¨ˆç®—æ¢¯åº¦ï¼Œä¸¦æŒ‰ `forward` è¼¸å…¥çš„é †åºè¿”å›æ¢¯åº¦ã€‚å°æ–¼ä¸éœ€è¦æ¢¯åº¦çš„è¼¸å…¥ï¼ˆå¦‚ `stride`, `eps` æˆ–BatchNormçš„ `running_mean/var`ï¼‰ï¼Œè¿”å› `None`ã€‚
    - `FusedLayer`: é€™æ˜¯ä¸€å€‹æ¨™æº–çš„ `nn.Module`ï¼Œè®“ä½¿ç”¨è€…åƒä½¿ç”¨æ™®é€š PyTorch å±¤ä¸€æ¨£ä½¿ç”¨æˆ‘å€‘çš„èåˆå±¤ã€‚å®ƒåœ¨ `__init__` ä¸­åˆå§‹åŒ–å·ç©å’Œ BatchNorm çš„åƒæ•¸ (`nn.Parameter` å’Œ `register_buffer`)ï¼Œä¸¦åœ¨ `forward` æ–¹æ³•ä¸­èª¿ç”¨ `FusedLayerFunction.apply(...)` ä¾†åŸ·è¡Œå¯¦éš›è¨ˆç®—ã€‚
    - `if __name__ == '__main__':` éƒ¨åˆ†æ˜¯æ¸¬è©¦ä»£ç¢¼ï¼Œå‰µå»ºäº†æ¨™æº– PyTorch å±¤ï¼ˆConv2d, BatchNorm2d, ReLUï¼‰å’Œæˆ‘å€‘çš„ `FusedLayer`ï¼Œè¤‡è£½åƒæ•¸ä»¥ç¢ºä¿ä¸€è‡´æ€§ï¼Œç„¶å¾Œæ¯”è¼ƒå…©è€…çš„å‰å‘è¼¸å‡ºå’Œï¼ˆç°¡åŒ–çš„ï¼‰åå‘æ¢¯åº¦ã€‚
2. **`fused_layer_cuda.cu`**:
    
    - **`#include <torch/extension.h>`**: å¼•å…¥ PyTorch C++ æ“´å±•æ‰€éœ€çš„é ­æ–‡ä»¶ï¼ŒåŒ…å«è™•ç†å¼µé‡ã€èˆ‡ Python äº¤äº’ç­‰çš„å·¥å…·ã€‚
    - **`fused_conv_bn_relu_kernel_forward` (`__global__` Kernel)**:
        - `__global__` è¡¨ç¤ºé€™æ˜¯ä¸€å€‹å¯ä»¥åœ¨ GPU ä¸Šç”±å¤šå€‹ç·šç¨‹ä¸¦è¡ŒåŸ·è¡Œçš„å‡½æ•¸ï¼ˆKernelï¼‰ã€‚
        - `const float* __restrict__ ...`: `const` è¡¨ç¤ºå‡½æ•¸ä¸æœƒä¿®æ”¹é€™äº›è¼¸å…¥æ•¸æ“šï¼Œ`__restrict__` æ˜¯çµ¦ç·¨è­¯å™¨çš„æç¤ºï¼Œè¡¨æ˜é€™äº›æŒ‡é‡æŒ‡å‘çš„è¨˜æ†¶é«”å€åŸŸä¸æœƒé‡ç–Šï¼Œæœ‰åŠ©æ–¼å„ªåŒ–ã€‚
        - **ç·šç¨‹ç´¢å¼•è¨ˆç®—**: `const int index = blockIdx.x * blockDim.x + threadIdx.x;` æ˜¯ CUDA ä¸­è¨ˆç®—å…¨å±€å”¯ä¸€ç·šç¨‹ ID çš„æ¨™æº–æ–¹å¼ã€‚æ¯å€‹ç·šç¨‹è² è²¬è¨ˆç®—è¼¸å‡ºå¼µé‡ä¸­çš„ä¸€å€‹å…ƒç´ ã€‚
        - **é‚Šç•Œæª¢æŸ¥**: `if (index >= num_outputs) return;` ç¢ºä¿ç·šç¨‹ä¸æœƒè™•ç†è¶…å‡ºè¼¸å‡ºå¼µé‡ç¯„åœçš„æ•¸æ“šã€‚
        - **ç´¢å¼•åˆ†è§£**: å°‡ä¸€ç¶­çš„ `index` è½‰æ›å›å››ç¶­çš„ `(n, c_out, h_out, w_out)` ç´¢å¼•ï¼Œä»¥ç¢ºå®šç•¶å‰ç·šç¨‹è² è²¬è¨ˆç®—å“ªå€‹è¼¸å‡ºé»ã€‚
        - **å·ç©è¨ˆç®—**:
            - è¨ˆç®—è¼¸å…¥ç‰¹å¾µåœ–å°æ‡‰çš„æ„Ÿå—é‡å·¦ä¸Šè§’åº§æ¨™ `(h_in_start, w_in_start)`ã€‚
            - ä½¿ç”¨ä¸‰å±¤åµŒå¥—å¾ªç’°éæ­·è¼¸å…¥é€šé“ `c_in` å’Œå·ç©æ ¸ `kH`, `kW`ã€‚
            - æª¢æŸ¥ç•¶å‰è¨ˆç®—çš„è¼¸å…¥åº§æ¨™ `(h_in, w_in)` æ˜¯å¦åœ¨æœ‰æ•ˆçš„è¼¸å…¥ç¯„åœå…§ï¼ˆè™•ç† Paddingï¼‰ã€‚
            - å¦‚æœæœ‰æ•ˆï¼Œè¨ˆç®—è¼¸å…¥æ•¸æ“š `input[...]` å’Œæ¬Šé‡ `weight[...]` åœ¨å…¶ä¸€ç¶­å…§å­˜ä½ˆå±€ä¸­çš„ç´¢å¼•ã€‚
            - åŸ·è¡Œä¹˜åŠ æ“ä½œ `conv_sum += input[...] * weight[...]`ã€‚
            - åŠ ä¸Šåç½® `bias[c_out]`ã€‚
        - **BatchNorm è¨ˆç®—**:
            - è®€å–å°æ‡‰è¼¸å‡ºé€šé“ `c_out` çš„ `running_mean`, `running_var`, `gamma`, `beta`ã€‚
            - å¥—ç”¨ BatchNorm å…¬å¼ï¼š`y = gamma * (x - mean) / sqrt(var + eps) + beta`ã€‚é€™è£¡ `x` æ˜¯ `conv_sum`ã€‚ä½¿ç”¨äº† `rsqrtf` æˆ– `1.0f / std::sqrt()` ä¾†è¨ˆç®—æ¨™æº–å·®çš„å€’æ•¸ã€‚
        - **ReLU è¨ˆç®—**:
            - æ‡‰ç”¨ ReLU å‡½æ•¸ï¼š`final_output = fmaxf(0.0f, bn_output)`ã€‚`fmaxf` æ˜¯ CUDA æä¾›çš„æµ®é»æ•¸ max å‡½æ•¸ã€‚
        - **å¯«å…¥è¼¸å‡º**: å°‡æœ€çµ‚çµæœ `final_output` å¯«å…¥è¼¸å‡ºå¼µé‡ `output[index]`ã€‚
    - **`fused_conv_bn_relu_kernel_backward` (`__global__` Kernel - Simplified)**:
        - é€™å€‹ Kernel å±•ç¤ºäº†åå‘å‚³æ’­çš„çµæ§‹ï¼Œä½†åŠŸèƒ½æ¥µå…¶ç°¡åŒ–ã€‚
        - å®ƒæ¥æ”¶ `grad_output` (ä¾†è‡ªä¸‹ä¸€å±¤çš„æ¢¯åº¦) å’Œ `output` (å‰å‘å‚³æ’­çš„è¼¸å‡º)ã€‚
        - å®ƒåªè¨ˆç®—äº† ReLU çš„åå‘å‚³æ’­ï¼šå¦‚æœå‰å‘è¼¸å‡ºçš„ `output[index] > 0`ï¼Œå‰‡æ¢¯åº¦é€šé (`grad_relu = grad_output[index]`)ï¼Œå¦å‰‡æ¢¯åº¦ç‚º 0ã€‚
        - **é‡è¦**: å®ƒæ²’æœ‰å¯¦ç¾ BatchNorm å’Œ Convolution çš„åå‘å‚³æ’­ã€‚å®ƒåªæ˜¯å°‡ ReLU çš„æ¢¯åº¦å¯«å› `grad_input_prop`ã€‚ä¸€å€‹å®Œæ•´çš„å¯¦ç¾æœƒéå¸¸è¤‡é›œã€‚
    - **`fused_layer_forward_cuda` (C++ function)**:
        - é€™æ˜¯ PyTorch `FusedLayerFunction.forward` èª¿ç”¨çš„ C++ å‡½æ•¸ã€‚
        - å®ƒæ¥æ”¶ `torch::Tensor` å°è±¡ã€‚
        - ç²å–è¼¸å…¥å¼µé‡çš„ç¶­åº¦ã€‚
        - è¨ˆç®—è¼¸å‡ºå¼µé‡çš„ç¶­åº¦ `H_out`, `W_out`ã€‚
        - ä½¿ç”¨ `torch::empty(...)` å‰µå»ºè¼¸å‡ºå¼µé‡ã€‚
        - ä½¿ç”¨ `TORCH_CHECK` æª¢æŸ¥è¼¸å…¥å¼µé‡æ˜¯å¦æ˜¯é€£çºŒçš„ (contiguous)ï¼Œé€™å°æ–¼ç›´æ¥ç²å– `data_ptr()` ä¸¦åœ¨ CUDA ä¸­å®‰å…¨ä½¿ç”¨å¾ˆé‡è¦ã€‚
        - è¨ˆç®— CUDA Kernel å•Ÿå‹•æ‰€éœ€çš„ç¶²æ ¼ (Grid) å’Œå¡Š (Block) ç¶­åº¦ã€‚`threads_per_block` é€šå¸¸è¨­ç‚º 128, 256, 512 ç­‰ï¼Œ`num_blocks` å‰‡æ ¹æ“šç¸½è¼¸å‡ºå…ƒç´ æ•¸é‡å’Œå¡Šå¤§å°è¨ˆç®—å¾—å‡ºã€‚
        - ä½¿ç”¨ `<<<num_blocks, threads_per_block>>>` èªæ³•å•Ÿå‹• `fused_conv_bn_relu_kernel_forward` Kernelï¼Œä¸¦å°‡å¼µé‡çš„æ•¸æ“šæŒ‡é‡ (`data_ptr<float>()`) å’Œå…¶ä»–åƒæ•¸å‚³éçµ¦ Kernelã€‚
        - ä½¿ç”¨ `CUDA_CHECK(cudaGetLastError())` æª¢æŸ¥ Kernel å•Ÿå‹•æ˜¯å¦å‡ºéŒ¯ã€‚
        - è¿”å›å‰µå»ºçš„ `output` å¼µé‡ã€‚
    - **`fused_layer_backward_cuda` (C++ function - Simplified)**:
        - é€™æ˜¯ PyTorch `FusedLayerFunction.backward` èª¿ç”¨çš„ C++ å‡½æ•¸ã€‚
        - æ¥æ”¶ `grad_output` å’Œå‰å‘å‚³æ’­æ™‚ä¿å­˜çš„å…¶ä»–å¼µé‡ã€‚
        - å‰µå»ºç”¨æ–¼å„²å­˜æ¢¯åº¦çš„å¼µé‡ (`grad_input`, `grad_weight`, etc.)ï¼Œåˆå§‹ç‚ºé›¶ã€‚
        - å•Ÿå‹•ç°¡åŒ–çš„ `fused_conv_bn_relu_kernel_backward` Kernelï¼Œè¨ˆç®— ReLU çš„åå‘æ¢¯åº¦ã€‚
        - **èªªæ˜äº†ç¼ºå¤±çš„éƒ¨åˆ†**: å¼·èª¿äº†å®Œæ•´çš„ BN å’Œ Conv åå‘å‚³æ’­æ²’æœ‰è¢«å¯¦ç¾ã€‚
        - è¿”å›åŒ…å«ï¼ˆå¤§éƒ¨åˆ†ç‚ºé›¶çš„ï¼‰æ¢¯åº¦çš„ `std::vector<torch::Tensor>`ï¼Œé †åºå¿…é ˆèˆ‡ `FusedLayerFunction.backward` çš„è¿”å›å€¼å°æ‡‰ã€‚
    - **`PYBIND11_MODULE`**: ä½¿ç”¨ Pybind11 åº«å°‡ C++ å‡½æ•¸ `fused_layer_forward_cuda` å’Œ `fused_layer_backward_cuda` ç¶å®šåˆ° Pythonï¼Œä½¿å®ƒå€‘å¯ä»¥è¢« Python ä»£ç¢¼ (å³ `fused_layer.py`) ä½œç‚º `fused_layer_cpp.forward` å’Œ `fused_layer_cpp.backward` èª¿ç”¨ã€‚`TORCH_EXTENSION_NAME` æ˜¯ä¸€å€‹å®ï¼Œæœƒè¢« `setup.py` ä¸­çš„ `name` æ›¿æ›ã€‚
3. **`setup.py`**:
    
    - ä½¿ç”¨ `torch.utils.cpp_extension` ä¸­çš„ `BuildExtension` å’Œ `CUDAExtension`ã€‚
    - `name='fused_layer_cpp'` æŒ‡å®šäº†ç·¨è­¯å¾Œç”Ÿæˆçš„ Python æ¨¡çµ„çš„åç¨±ã€‚
    - `sources=['fused_layer_cuda.cu']` æŒ‡å®šäº†éœ€è¦ç·¨è­¯çš„æºæ–‡ä»¶ã€‚
    - `cmdclass={'build_ext': BuildExtension}` å‘Šè¨´ setuptools ä½¿ç”¨ PyTorch æä¾›çš„æ“´å±•ç·¨è­¯å‘½ä»¤ã€‚

---

**é‡è¦æ³¨æ„äº‹é …èˆ‡æ½›åœ¨æ”¹é€²**

1. **åå‘å‚³æ’­è¤‡é›œæ€§**: é€™å€‹ç¯„ä¾‹ä¸­çš„åå‘å‚³æ’­æ˜¯**æ¥µåº¦ç°¡åŒ–**çš„ã€‚å¯¦ç¾ä¸€å€‹å®Œæ•´ä¸”é«˜æ•ˆçš„ Conv + BN + ReLU èåˆå±¤çš„åå‘å‚³æ’­éå¸¸è¤‡é›œï¼Œéœ€è¦ä»”ç´°æ¨å°éˆå¼æ³•å‰‡ä¸‹çš„æ‰€æœ‰æ¢¯åº¦ï¼ˆ`grad_input`, `grad_weight`, `grad_bias`, `grad_gamma`, `grad_beta`ï¼‰ï¼Œä¸¦åœ¨ CUDA ä¸­é«˜æ•ˆå¯¦ç¾ã€‚é€™é€šå¸¸æ¶‰åŠåˆ° Transposed Convolution (è¨ˆç®— `grad_input`) å’ŒåŸºæ–¼è¼¸å…¥/æ¢¯åº¦è¼¸å‡ºçš„ Correlation (è¨ˆç®— `grad_weight`) ç­‰æ“ä½œã€‚å°æ–¼ BatchNorm çš„åå‘å‚³æ’­ä¹Ÿéœ€è¦é¡å¤–çš„è¨ˆç®—ã€‚
2. **è¨“ç·´ vs. æ¨æ–·**: ç¯„ä¾‹ä¸­çš„ BatchNorm éƒ¨åˆ†ä¸»è¦ä½¿ç”¨äº† `running_mean` å’Œ `running_var`ï¼Œé€™é¡ä¼¼æ–¼æ¨æ–·æ¨¡å¼ (`eval()`)ã€‚è¦åœ¨è¨“ç·´æ¨¡å¼ (`train()`) ä¸‹æ­£ç¢ºå·¥ä½œï¼Œéœ€è¦åœ¨ Kernel ä¸­è¨ˆç®—ç•¶å‰ mini-batch çš„å‡å€¼å’Œæ–¹å·®ï¼Œç”¨å®ƒå€‘ä¾†é€²è¡Œæ¨™æº–åŒ–ï¼Œä¸¦æ›´æ–° `running_mean` å’Œ `running_var`ã€‚é€™æœƒå¢åŠ  Kernel çš„è¤‡é›œæ€§ï¼Œéœ€è¦é¡å¤–çš„æ­¸ç´„ (Reduction) æ“ä½œä¾†è¨ˆç®—å‡å€¼å’Œæ–¹å·®ã€‚
3. **æ€§èƒ½å„ªåŒ–**:
    - **å…±äº«å…§å­˜ (Shared Memory)**: å°æ–¼å·ç©æ“ä½œï¼Œå¯ä»¥å°‡è¼¸å…¥ç‰¹å¾µåœ–çš„ Tile å’Œå·ç©æ ¸çš„æ¬Šé‡åŠ è¼‰åˆ°å…±äº«å…§å­˜ä¸­ï¼Œä»¥æ¸›å°‘å°å…¨å±€å…§å­˜çš„è¨ªå•æ¬¡æ•¸ï¼Œå¾è€Œæé«˜æ€§èƒ½ã€‚
    - **å¯„å­˜å™¨ä½¿ç”¨ (Register Usage)**: æ¯å€‹ç·šç¨‹è¨ˆç®—ä¸€å€‹è¼¸å‡ºé»æ¯”è¼ƒç›´è§€ï¼Œä½†å¯èƒ½æœƒä½¿ç”¨è¼ƒå¤šå¯„å­˜å™¨ã€‚å¯ä»¥æ¢ç´¢è®“æ¯å€‹ç·šç¨‹è¨ˆç®—å¤šå€‹è¼¸å‡ºé»ï¼Œæˆ–è€…è®“ä¸€å€‹ç·šç¨‹å¡Š (Thread Block) å”ä½œè¨ˆç®—è¼¸å‡ºçš„ Tileã€‚
    - **æŒ‡ä»¤ç´šä¸¦è¡Œ**: ä½¿ç”¨ `float2` æˆ– `float4` æ•¸æ“šé¡å‹ä¸€æ¬¡è™•ç†å¤šå€‹æµ®é»æ•¸ï¼Œåˆ©ç”¨ GPU çš„å‘é‡è™•ç†èƒ½åŠ›ã€‚
    - **Kernel èª¿å„ª**: æ ¹æ“šå…·é«”çš„ GPU æ¶æ§‹èª¿æ•´ `threads_per_block` çš„å¤§å°ã€‚
    - **ç®—æ³•é¸æ“‡**: å°æ–¼æŸäº›å·ç©å°ºå¯¸ï¼ŒWinograd æˆ– FFT-based å·ç©å¯èƒ½æ¯”ç›´æ¥å·ç©æ›´å¿«ï¼Œä½†èåˆå®ƒå€‘æœƒæ›´è¤‡é›œã€‚
4. **æ•¸å€¼ç©©å®šæ€§**: åœ¨ BatchNorm ä¸­ï¼Œ`eps` éå¸¸é‡è¦ã€‚åå‘å‚³æ’­ä¸­ä¹Ÿå¯èƒ½éœ€è¦æ³¨æ„æ•¸å€¼ç©©å®šæ€§å•é¡Œã€‚
5. **æ•¸æ“šé¡å‹**: ç¯„ä¾‹ä½¿ç”¨ `float` (FP32)ã€‚å°æ–¼ç¾ä»£ GPUï¼Œå¯ä»¥è€ƒæ…®ä½¿ç”¨ `half` (FP16) æˆ– `bfloat16` (BF16) é…åˆ Tensor Cores ä¾†ç²å¾—æ›´é«˜çš„ååé‡ï¼Œä½†éœ€è¦è™•ç†æ½›åœ¨çš„ç²¾åº¦æå¤±å’Œæ•¸å€¼ç¯„åœå•é¡Œã€‚
6. **å¯ç¶­è­·æ€§**: é›–ç„¶èåˆ Kernel å¯èƒ½æ›´å¿«ï¼Œä½†å®ƒå€‘é€šå¸¸æ¯”æ¨™æº–å±¤æ›´é›£ç·¨å¯«ã€èª¿è©¦å’Œç¶­è­·ã€‚

é€™å€‹ç¯„ä¾‹æä¾›äº†ä¸€å€‹èåˆå±¤çš„åŸºæœ¬æ¡†æ¶å’Œæ¦‚å¿µé©—è­‰ã€‚è¦åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­ä½¿ç”¨ï¼Œç‰¹åˆ¥æ˜¯éœ€è¦è¨“ç·´æ¨¡å‹æ™‚ï¼Œå¿…é ˆæŠ•å…¥æ›´å¤šç²¾åŠ›ä¾†å¯¦ç¾å®Œæ•´ä¸”ç¶“éå„ªåŒ–çš„åå‘å‚³æ’­ï¼Œä¸¦ä»”ç´°è™•ç†è¨“ç·´/æ¨æ–·æ¨¡å¼çš„å·®ç•°ã€‚