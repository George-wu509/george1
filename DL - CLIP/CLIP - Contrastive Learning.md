
ref: [用 10 分鐘搭建萬物識別的 Live Demo](https://u9534056.medium.com/%E7%94%A8-10-%E5%88%86%E9%90%98%E6%90%AD%E5%BB%BA%E8%90%AC%E7%89%A9%E8%AD%98%E5%88%A5%E7%9A%84-live-demo-a80edcf0effb)
## _Contrastive Learning 對比學習_

[_Contrastive Learning_](https://lilianweng.github.io/lil-log/2021/05/31/contrastive-representation-learning.html) 是 _Self-Supervised Learning_ 的其中一個分支，特點就是跟無監督學習 (Unsupervised) 方法一樣，不需要多餘的標注數據就能訓練。但需要注意的是，**對比學習不屬於無監督學習**，因為它還是需要標注數據的，只是它的標注數據就是「數據自己」。

對比學習的著名方法包含[_MoCo_](https://arxiv.org/abs/1911.05722)、[_SimCLR_](https://arxiv.org/abs/2002.05709)。以 _SimCLR 為例，_他利用「資料擴增」的方式，將一張圖變為 N 張圖，然後讓機器學習下列問題：

### 1. **對比學習（Contrastive Learning）**詳細解釋

**對比學習（Contrastive Learning）**是一種自監督學習（Self-supervised Learning）的技術，旨在學習數據的表徵（representation），以便能夠在沒有標籤的情況下進行有效的特徵提取。其核心思想是讓模型學會區分正樣本和負樣本，通過**相似對的最大化（positive pair maximization）**和**不相似對的最小化（negative pair minimization）**來學習特徵表示。這在沒有標籤數據的情況下，尤其是在無監督學習中，顯得非常有效。

對比學習的具體過程：

- **正樣本（Positive Pairs）**：指的是語義上相關的數據對。例如，一張圖像及其進行數據增強後的版本被視為一個正樣本對。
- **負樣本（Negative Pairs）**：指的是語義上不相關的數據對，如一張貓的照片與一張狗的照片。

在訓練過程中，模型會嘗試讓**正樣本對的特徵向量更加接近**，而讓**負樣本對的特徵向量盡量分開**。這樣，模型能夠學習到有意義的表徵，即便在無監督學習的情況下，也能在下游任務中有較好的表現。

### 2. **常用著名的對比學習方法**

1. **MoCo（Momentum Contrast for Unsupervised Visual Representation Learning）**
    
    - **MoCo** 是一種自監督對比學習技術，使用動量更新來維持一個**動量編碼器（momentum encoder）**，從而能夠動態生成對比學習所需的負樣本。
    - **作用**：MoCo的優點在於它使用一個**動量緩存隊列（momentum queue）**來存儲大量的負樣本，解決了小批量訓練時負樣本不足的問題，並且保持了正負樣本之間的穩定性。
    - **應用**：在無監督圖像表徵學習中取得了良好的效果。
2. **SimCLR（Simple Framework for Contrastive Learning of Visual Representations）**
    
    - **SimCLR** 是一種簡單且有效的對比學習框架，通過**數據增強（Data Augmentation）**來創建正樣本對，並使用**批量內（in-batch）對比學習**來處理負樣本。
    - **作用**：SimCLR 強調不同形式的數據增強（如裁剪、旋轉等），並將增強後的圖像當作正樣本對進行學習。SimCLR 並不需要像 MoCo 那樣額外維護一個動量編碼器。
    - **應用**：在無標籤的圖像分類、檢索、表示學習中廣泛應用。
3. **BYOL（Bootstrap Your Own Latent）**
    
    - **BYOL** 是一種自監督學習方法，與大多數對比學習方法不同，BYOL **不依賴於負樣本對**。它通過兩個網絡進行訓練：**在線網絡（online network）**和**目標網絡（target network）**。
    - **作用**：BYOL 通過不斷更新的在線網絡與靜態的目標網絡之間的對比來進行學習，達到了在沒有負樣本的情況下也能學習到有意義的特徵。
    - **應用**：在無監督學習中提供了另一種思路，特別適合於沒有大量負樣本的情況下使用。
4. **SimSiam（Exploring Simple Siamese Representation Learning）**
    
    - **SimSiam** 是一種自監督對比學習方法，與 BYOL 類似，它也不需要負樣本。SimSiam 使用**Siamese Network（孿生網絡）**來進行學習，兩個網絡結構相同，通過對比學習正樣本對來進行訓練。
    - **作用**：SimSiam 將關鍵點放在學習穩定的語義表徵上，並在簡化對比學習架構的同時仍然保持了良好的性能。
    - **應用**：用於學習圖像、視頻中的有意義的表徵。
5. **SwAV（Swapped Assignment between Views）**
    
    - **SwAV** 是一種創新的自監督對比學習方法，它的主要創新點在於**將不同視角（views）下的數據進行交換分配（swapped assignment）**，並讓模型學習如何將不同視角的圖像嵌入到相似的表徵空間中。
    - **作用**：SwAV 使用了一個特別的群集（clustering）步驟來強化表徵學習，並且不需要明確的負樣本。
    - **應用**：SwAV 能夠有效地處理圖像表示學習問題，尤其在大規模無監督圖像分類中表現出色。

### 3. **無監督學習、半監督學習與對比學習等相關概念**

#### a. **無監督學習（Unsupervised Learning）**

- **定義**：無監督學習是指模型在訓練過程中不依賴於標註數據，只使用未標註的數據進行學習。模型的目標是從數據中發現內部的結構和規律，例如聚類（clustering）和降維（dimensionality reduction）。
- **關聯**：對比學習是一種無監督學習方法，因為它不需要標籤數據，只需通過對正樣本和負樣本的區分來進行學習。

#### b. **半監督學習（Semi-supervised Learning）**

- **定義**：半監督學習使用標註數據和未標註數據的結合來訓練模型。這在標註數據有限的情況下非常有用。
- **關聯**：在半監督學習中，對比學習可以用於無標註數據的表徵學習，然後與標註數據進行結合，從而提升模型性能。

#### c. **對比學習（Contrastive Learning）**

- **定義**：對比學習是一種無監督或自監督學習方法，通過讓模型學會區分正樣本對和負樣本對來學習數據的語義表示。
- **關聯**：對比學習是無監督學習的一種常用方法，尤其在表徵學習和自監督學習中應用廣泛。

#### d. **零樣本學習（Zero-shot Learning, ZSL）**

- **定義**：零樣本學習指的是模型在沒有見過某些類別的數據時，也能正確地進行分類或推理。這通常依賴於模型能夠學習到數據的泛化特徵，並基於已有的知識進行推理。
- **關聯**：對比學習的表徵學習能力使得模型能夠在沒有標註數據的情況下學習到泛化的特徵，這樣就可以應用於零樣本學習，從而對未見過的類別進行推理。

#### e. **一次學習（One-shot Learning, OSL）**

- **定義**：一次學習指的是模型只需要少量標註數據（通常是每個類別只有一個樣本）進行訓練，並且能夠在新類別上進行推理。這依賴於模型能夠有效地泛化。
- **關聯**：對比學習可以幫助模型在沒有大量標註數據的情況下學習到有意義的表徵，這為一次學習提供了良好的基礎。

#### f. **基於解釋的學習（Explanation-based Learning, EBL）**

- **定義**：基於解釋的學習是一種符號學習技術，模型會基於先前的知識和推理規則來解釋新的數據，並生成通用的規則。這種方法強調模型對數據的解釋能力。
- **關聯**：雖然 EBL 與對比學習並非直接相關，但對比學習中學到的數據語義表示能夠幫助模型更好地理解數據的語義結構，從而支持模型的解釋能力。

### 4. **總結**

- **對比學習（Contrastive Learning）** 是一種強大的無監督學習技術，通過對比正樣本和負樣本學習到有意義的特徵表示。
- 常用的對比學習方法包括 **MoCo**、**SimCLR**、**BYOL**、**SimSiam** 和 **SwAV**，這些方法在無標註數據的表徵學習中發揮了重要作用。
- **無監督學習、半監督學習、零樣本學習、一次學習** 等技術與對比學習密切相關，都旨在利用有限的數據或無標註數據進行有效學習。