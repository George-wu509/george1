
## ETH-XGaze 詳細解析：深入探討凝視估計技術

在人機互動、虛擬實境和心理學研究等領域，精準地判斷一個人的視線方向，即凝視估計（Gaze Estimation），扮演著至關重要的角色。ETH-XGaze 的出現，為解決這個領域中長期存在的挑戰，特別是在極端頭部姿態下的凝視估計，提供了一個大規模、高品質的數據集和一個強而有力的基準模型。本文將深入探討 ETH-XGaze 的所有技術細節，從輸入、輸出、模型架構、訓練過程，到實際應用案例，並與其他主流模型進行比較。

輸入: 單張人臉圖像 (224x224)
輸入: 俯仰角 (Pitch), 偏航角 (Yaw)  [ pitch, yaw ]
網路架構: ResNet



### 核心概念：解決極端姿態下的凝視估計難題

傳統的凝視估計模型在面對使用者大幅度的頭部轉動時，其準確度會顯著下降。這主要是因為現有的大部分數據集缺乏在此類極端角度下的標註樣本。ETH-XGaze 的主要貢獻便在於此，它提供了一個包含超過一百萬張高解析度圖像的數據集，涵蓋了來自110位參與者的廣泛頭部姿態和視線方向。這些數據的採集過程極其嚴謹，使用了18個高階數位單眼相機和專業的照明設備，確保了數據的品質和多樣性。

---

### 技術細節剖析

#### 1. 輸入 (Input)

ETH-XGaze 的基準模型採用的是**單張人臉圖像（Single Face Image）** 作為輸入，而非影片序列。具體來說，輸入是經過前處理的**人臉裁切圖（Cropped Face Patch）**。

- **圖像來源**: 原始數據集中的高解析度圖像。
- **前處理**:
    1. **人臉偵測 (Face Detection)**: 首先，需要一個標準的人臉偵測器（例如 MTCNN 或 Dlib）來定位出圖像中的人臉位置，得到一個**檢測框（Detection Box）**。
    2. **圖像裁切與標準化**: 根據偵測到的人臉關鍵點（如眼睛、鼻子、嘴角的位置），對人臉區域進行裁切。為了消除頭部滾轉（roll a rotation）對模型造成的影響，通常會對圖像進行**標準化（Normalization）**，即將雙眼連線調整至水平。
    3. **尺寸調整**: 裁切並標準化後的人臉圖像會被統一縮放至固定的尺寸，在官方的基準模型中，通常是 **224x224** 或 **448x448** 像素
因此，模型的直接輸入是一個固定大小、已經對齊過的人臉方形圖像。
#### 2. 輸出 (Output)
模型的輸出是預測出的三維空間中的視線方向，通常用兩個角度來表示
- **俯仰角 (Pitch)**: 代表視線的上下方向。正值通常表示向上看，負值表示向下看。
- **偏航角 (Yaw)**: 代表視線的左右方向。正值通常表示向右看，負值表示向左看。
這兩個角度值定義了在以頭部為中心的座標系中的一個**三維單位向量 (3D Unit Vector)**，這個向量即代表了凝視的方向。輸出格式為一個包含兩個浮點數的向量 `[pitch, yaw]`。在程式碼中，可以很輕易地將這兩個角度轉換為一個三維向量 `(x, y, z)` 以便於在三維空間中進行視覺化或進一步的計算。

#### 3. 模型架構 (Model Architecture)

ETH-XGaze 官方提出的基準模型採用了在電腦視覺領域廣泛成功並被驗證過的**殘差網絡 (Residual Network, ResNet)**，具體來說是 **ResNet-50**。

- **骨幹網絡 (Backbone)**: 使用 ResNet-50 作為特徵提取器。ResNet 的核心優勢在於其“捷徑連接”（shortcut connections）能夠有效地解決深度神經網絡中的梯度消失問題，從而可以訓練更深、更複雜的網絡，提取出更豐富的圖像特徵。輸入的 224x224 人臉圖像會通過一系列的卷積層、殘差模塊和池化層。
    
- **頭部 (Head)**: 在 ResNet-50 的骨幹網絡提取出高維度的特徵圖（feature map）之後，會連接一個客製化的“頭部”來完成最終的迴歸任務。
    1. **全局平均池化 (Global Average Pooling, GAP)**: 將最後一層卷積層輸出的特徵圖進行全局平均池化，將其壓縮成一個一維的特徵向量。
    2. **全連接層 (Fully Connected Layers)**: 這個特徵向量會被送入一個或多個全連接層（也稱為線性層）。這些全連接層的作用是將從圖像中提取的複雜特徵映射到最終的輸出空間。
    3. **輸出層**: 最後一個全連接層的輸出維度為 **2**，分別對應預測的俯仰角（pitch）和偏航角（yaw）。

整個模型的設計遵循了深度學習在圖像迴歸任務上的標準實踐，即利用一個強大的卷積神經網絡作為主幹來學習圖像表示，再透過幾個全連接層來預測連續的數值。

#### 4. 訓練過程 (Training)

模型的訓練是一個監督式學習的過程，目標是讓模型預測出的凝視角度與數據集中的“真實”標註（ground truth）盡可能接近。

- **數據集**: 使用 ETH-XGaze 提供的龐大數據集進行訓練。數據集會被劃分為訓練集、驗證集和測試集，以確保模型的泛化能力。
    
- **損失函數 (Loss Function)**: 由於凝視估計本質上是一個迴歸問題，最常用的損失函數是**均方誤差 (Mean Squared Error, MSE)**。然而，在凝視估計領域，更常用的是直接計算角度誤差。因此，一種更直觀且有效的損失函數是**平均角度誤差 (Mean Angular Error)** 的變體，或是將其作為評估指標。在實作上，通常會使用 `L1Loss`（平均絕對誤差）或 `MSELoss` 來計算預測的 `[pitch, yaw]` 向量與真實標註之間的差異。
    
    Loss=N1​i=1∑N​∥gpred,i​−gtrue,i​∥22​
    
    其中，N 是批次大小（batch size），g 是 `[pitch, yaw]` 向量。
    
- **優化器 (Optimizer)**: 採用 **Adam** 優化器。Adam 是一種結合了 Momentum 和 RMSprop 優點的自適應學習率優化算法，在實際應用中表現穩健且高效。
    
- **學習率 (Learning Rate)**: 初始學習率通常設置在一個較小的值，例如 `1e-4`。訓練過程中可能會使用**學習率調度器 (Learning Rate Scheduler)**，例如，當驗證集上的損失不再下降時，動態地降低學習率，以幫助模型在訓練後期更精細地收斂到最優解。
    
- **訓練週期 (Epochs)**: 模型會被訓練多個週期，直到在驗證集上的性能達到飽和。在 ETH-XGaze 的基準模型訓練中，通常會訓練25個週期或更多。
    

---

### 主要應用與具體案例說明

ETH-XGaze 的主要應用涵蓋了需要理解使用者意圖和注意力的各種場景。

**主要應用領域:**

- **人機互動 (Human-Computer Interaction)**: 實現“眼動控制”，例如用眼睛來移動滑鼠、選擇圖標或滾動頁面，特別適用於身障人士。
    
- **駕駛員監控系統 (Driver Monitoring Systems)**: 檢測駕駛員是否疲勞或分心。如果系統偵測到駕駛員的視線長時間偏離前方道路，可以發出警報。
    
- **市場研究與廣告分析**: 分析消費者在觀看廣告或瀏覽貨架時的視覺注意力分佈，從而優化產品佈局和廣告設計。
    
- **虛擬實境 (VR) 與擴增實境 (AR)**: 在虛擬世界中，使用者的視線可以作為一種自然的互動方式，例如用眼神與虛擬角色交流，或在需要高渲染品質的中心凹區域（foveated rendering）進行優化。
    
- **醫療診斷**: 在自閉症或注意力不足過動症（ADHD）等疾病的輔助診斷中，分析患者的視覺掃描路徑。
    

**具體案例：基於 ETH-XGaze 的無觸控介面互動**

假設我們要開發一個應用，讓使用者可以透過眼神來選擇電腦螢幕上的四個按鈕之一。

**步驟一：設定環境與載入模型**

1. **影像擷取**: 使用網路攝影機（Webcam）即時擷取使用者的臉部影像。
    
2. **載入預訓練模型**: 載入一個在 ETH-XGaze 數據集上預先訓練好的凝視估計模型（例如官方提供的 ResNet-50 基準模型）。同時，也需要載入一個人臉偵測模型。
    

**步驟二：即時影像處理與凝視預測**

1. **偵測人臉**: 對每一幀影像，執行人臉偵測算法，找到人臉的邊界框。
    
2. **裁切與標準化**: 根據偵測結果，裁切出人臉區域，並進行標準化處理（旋轉校正、尺寸調整為 224x224 像素）。
    
3. **模型推論**: 將處理好的人臉圖像輸入到 ETH-XGaze 模型中，模型會輸出一對 `[pitch, yaw]` 值。
    

**步驟三：將凝視向量轉換為螢幕座標**

這一步是將抽象的凝視方向轉換為具體的螢幕點位。

1. **座標系轉換**: 模型輸出的凝視向量是基於頭部座標系的。我們需要將其轉換到攝影機的世界座標系。這一步通常需要估計頭部在攝影機前的三維姿態（Head Pose Estimation），可以透過人臉關鍵點來實現。
    
2. **視線與螢幕平面求交**: 假設我們知道攝影機相對於螢幕的位置和方向（這一步可以透過一次性的校準程序來完成）。我們可以從眼睛的位置（可近似為頭部中心）發射出一條由凝視向量所定義的射線。計算這條射線與螢幕所在平面的交點，這個交點的二維座標 `(x, y)` 就是使用者在螢幕上的注視點。
    

**步驟四：互動邏輯**

1. **按鈕區域判定**: 判斷計算出的螢幕注視點 `(x, y)` 是否落在了四個按鈕中任何一個的區域內。
    
2. **停留觸發 (Dwell to Click)**: 為了避免視線的自然晃動導致的誤觸發，可以設定一個“停留時間”閾值。當使用者的注視點在某個按鈕區域內停留超過（例如）1秒鐘，系統就判定為一次點擊事件，並執行該按鈕對應的功能。
    

透過以上步驟，便可以實現一個完整、流暢的眼動操控系統，而 ETH-XGaze 模型在此過程中扮演了提供精準、可靠的底層凝視方向數據的核心角色。

---

### 與其他主流模型的比較

|特性|ETH-XGaze (基準模型)|Gaze360|MPIIFaceGaze|iTracker|
|---|---|---|---|---|
|**主要貢獻**|解決極端頭部姿態問題的大規模數據集和基準|360度全方位凝視估計數據集與模型|較早期的自然場景凝視估計數據集|端到端的深度學習模型，同時利用雙眼和人臉|
|**輸入**|單張標準化人臉圖像 (224x224)|單張人臉圖像|單張人臉圖像|左眼、右眼、全臉圖像及人臉網格|
|**輸出**|2D 凝視角 (Pitch, Yaw)|3D 凝視向量|2D 凝視角 (Pitch, Yaw)|2D 螢幕座標 (x, y)|
|**模型架構**|ResNet-50|分支化的 ResNet-50，分別處理中心和周邊視覺|VGG-16|三個獨立的 CNN 分支，最後融合|
|**數據集規模**|> 1,000,000 張圖像|> 238,000 張圖像|約 213,000 張圖像|約 2,500,000 張圖像|
|**頭部姿態範圍**|極廣 (水平達 ±70°)|全方位 (360°)|較廣|有限|
|**優勢**|在極端頭部姿態下表現魯棒，數據品質高|能夠處理背對鏡頭的視線方向|在自然互動場景下表現良好|直接輸出螢幕座標，端到端訓練|
|**劣勢**|基準模型架構相對簡單|數據集規模相對較小|對於極端姿態的樣本覆蓋不足|需要多個輸入，模型較為複雜|

匯出到試算表

總結而言，ETH-XGaze 不僅僅是一個模型，它更是一個強大的生態系統，其核心是一個高品質、大規模的數據集。它極大地推動了凝視估計領域的發展，使得開發出在真實世界複雜場景下依然能夠保持高準確度的模型成為可能。其提供的基準模型也為後續的研究者提供了一個堅實的起點，可以在此基礎上進行改進和創新。