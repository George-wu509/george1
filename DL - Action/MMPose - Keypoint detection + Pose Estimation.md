
MMPose [Github](https://github.com/open-mmlab/mmpose)


|                                                               |     |
| ------------------------------------------------------------- | --- |
| [[#### MMPose 全面詳解]]                                          |     |
| [[#### Pose estimation後續分析]]                                  |     |
| [[#### ST-GCN (Spatio-Temporal Graph Convolutional Network)]] |     |


```
請中文詳細解釋MMPose, 特別注重在keypoint detection model跟 pose estimation model的所有技術細節包括輸入(譬如是image or video或detection box), 輸出譬如是每個keypoint的座標or bbox, Model architecture, 如何進行訓練, 主要應用並用具體案例詳細一步步說明. 也請列表比較其他主流keypoint detection model跟 pose estimation models.
```

這就為您詳細解釋MMPose，並涵蓋您所要求的各個技術細節。

#### MMPose 全面詳解

MMPose 是一款基於 PyTorch 的開源姿態估計（Pose Estimation）工具箱，是 OpenMMLab 項目的一部分。它以其模組化設計、豐富的算法庫、高效能和靈活性而聞名，廣泛應用於學術研究和工業界。

MMPose 不僅僅是一個單一的模型，而是一個集成了數據處理、模型構建、訓練、評估和部署等完整流程的框架。它可以處理多種姿態估計(pose estimation)任務，包括單人/多人2D姿態估計、3D人體姿態估計、手部姿態估計、臉部關鍵點檢測、動物關鍵點檢測，甚至服裝和物體的關鍵點檢測。

---

### 1. 關鍵點檢測 (Keypoint Detection) 與姿態估計 (Pose Estimation) 模型技術細節

在MMPose的框架下，"Keypoint Detection Model" 和 "Pose Estimation Model" 通常指的是同一個模型，其核心任務就是從輸入中檢測出物體（如人體）的關鍵點（如關節）座標。整個流程可以細分為**「自頂向下 (Top-down)」**和**「自底向上 (Bottom-up)」**兩種主要方法。

#### **自頂向下 (Top-down) 方法詳解**

這是MMPose中最常用也最精準的方法。其核心思想是「先檢測，後估計」。

1. **輸入 (Input):**
    
    - **原始輸入:** 一張靜態圖片 (Image) 或影片中的一幀 (Video Frame)。
        
    - **第一階段 - 目標檢測:** 首先，需要一個獨立的目標檢測器 (Object Detector)，例如 YOLO, Faster R-CNN, 或 MMDetection 庫中的任何模型。這個檢測器會在圖片上識別人體，並輸出每個人的邊界框 (Bounding Box, bbox)，格式通常是 `[x_min, y_min, x_max, y_max]`。
        
    - **第二階段 - 姿態估計模型的真正輸入:** MMPose會將目標檢測器輸出的每個人體邊界框從原圖中裁剪 (crop) 出來，並將這些裁剪後的人體圖像縮放 (resize) 到一個固定的尺寸（例如 `256x192` 或 `384x288` 像素）。**因此，姿態估計模型的直接輸入是這些標準化後的人體圖像塊 (cropped and resized human image patches)。**
        
2. **模型架構 (Model Architecture):** MMPose中的姿態估計模型通常由三個核心部分組成：
    
    - **Backbone (骨幹網路):** 負責從輸入的人體圖像塊中提取特徵。這些通常是強大的卷積神經網絡 (CNN)。
        
        - **常用模型:** ResNet, HRNet (High-Resolution Net), MobileNet (輕量級), ResNeSt等。
            
        - **HRNet** 特別適合姿態估計，因為它能在整個過程中保持高解析度的特徵圖，這對於精確定位關鍵點至關重要。傳統的CNN（如ResNet）在深層時解析度會降低，丟失空間細節。
            
    - **Neck (頸部):** 雖然在某些簡單的模型中不存在，但在更複雜的模型中，Neck 用於融合和增強從Backbone不同層級輸出的特徵圖。例如，特徵金字塔網絡 (FPN)。
        
    - **Head (頭部):** 這是模型的最終輸出部分，負責根據提取到的特徵預測關鍵點的位置。
        
        - **主流設計 - 熱力圖 (Heatmap):** 大多數先進的模型（包括MMPose中的主流模型）不直接回歸 (regression) 關鍵點的 `(x, y)` 座標。而是為每個關鍵點（例如，鼻子、左肩、右膝）生成一張**熱力圖 (Heatmap)**。
            
        - 一張熱力圖的尺寸通常是輸入圖像的 `1/4`（例如 `64x48`），其上的每個像素值代表該位置是對應關鍵點的「可能性」或「置信度」。在理想情況下，熱力圖在真實關鍵點位置的像素值最高（通常用一個二維高斯分佈來表示），離得越遠值越低。
            
3. **輸出 (Output):**
    
    - **模型直接輸出:** 一個批次 (batch) 的熱力圖。如果模型要檢測 `K` 個關鍵點，那麼輸出就是 `K` 張熱力圖。例如，COCO數據集定義了17個人體關鍵點，模型就會輸出17張熱力圖。
        
    - **後處理 (Post-processing):** 為了得到最終的座標，MMPose會對每張熱力圖進行後處理：
        
        1. 找到熱力圖上響應值最高的像素點 `(u, v)`。
            
        2. 為了提高亞像素 (sub-pixel) 精度，通常會對最高點周圍的像素值進行微調（例如，二次插值或從高響應值向次高響應值偏移1/4像素）。
            
        3. 將計算出的熱力圖座標 `(u, v)` **映射回原始圖像的座標系**，得到最終的關鍵點座標 `(x, y)` 和一個置信度分數 (confidence score)，這個分數通常就是熱力圖上的最高響應值。
            
    - **最終輸出格式:** 對於輸入圖像中的每個人，輸出是一個包含 `K` 個關鍵點的列表，每個關鍵點的格式通常是 `[x, y, confidence]`。
        
4. **訓練過程 (Training):**
    
    - **標籤 (Ground Truth):** 在訓練時，我們需要有標註好的數據，即每個關鍵點的真實座標 `(x_gt, y_gt)`。
        
    - **生成目標熱力圖 (Target Heatmap):** 我們不能直接用座標來監督熱力圖的生成。因此，對於每個關鍵點，我們會手動創建一張「目標熱力圖」。具體做法是在 `(x_gt, y_gt)` 對應到熱力圖上的位置 `(u_gt, v_gt)` 為中心，生成一個二維的高斯核，作為模型的學習目標。
        
    - **損失函數 (Loss Function):** 訓練的目標是讓模型預測出的熱力圖與我們生成的目標熱力圖盡可能相似。最常用的損失函數是**均方誤差損失 (Mean Squared Error, MSE Loss)**，它會逐像素計算預測熱力圖和目標熱力圖之間的差異。
        
    - **數據增強 (Data Augmentation):** 為了提高模型的泛化能力，MMPose會使用大量的數據增強技術，包括：
        - 隨機翻轉 (Random Flip)
        - 隨機旋轉 (Random Rotation)
        - 隨機縮放 (Random Scale)
        - 仿射變換 (Affine Transformation)

#### **自底向上 (Bottom-up) 方法簡述**

這種方法與Top-down相反，它「先找關節，再組裝成人」。

1. **輸入:** 整張原始圖片，無需預先檢測人體。
    
2. **模型過程:**
    - 模型首先檢測出圖像中**所有**的關鍵點（例如，所有的手腕、所有的膝蓋）。
    - 同時，模型會學習預測**部件親和場 (Part Affinity Fields, PAFs)**，這是一種2D向量場，表示不同關節之間的連接方向和可能性。
        
3. **輸出與後處理:**
    - 在得到所有候選關鍵點和PAFs後，通過一個複雜的圖匹配算法（例如，貪心算法），將屬於同一個人的關節連接起來，組裝成完整的人體骨架。
        
4. **優缺點:**
    - **優點:** 速度快，因為只需對整張圖進行一次推斷，不受人數影響。
    - **缺點:** 擁擠場景中，精度通常不如Top-down方法，因為關節匹配容易出錯。
        

### 2. 主要應用與具體案例說明

MMPose的應用非常廣泛，涵蓋了需要理解人體、動物或物體姿態的各個領域。

#### **案例：健身動作姿態分析系統**

假設我們要開發一個深蹲 (Squat) 動作的輔助教練系統，以確保用戶動作標準。

**第一步：數據採集與準備**
- **輸入:** 來自手機或電腦攝影機的即時影片流 (Video Stream)。
- **數據處理:** 從影片流中逐幀讀取圖像。

**第二步：Top-down姿態估計流程**

1. **人體檢測:**
    - 將當前幀圖像輸入到一個預訓練的目標檢測模型（例如，YOLOv3）。
    - 該模型會返回偵測到的每個人的邊界框 `bbox`。如果畫面中只有一個人，就取置信度最高的那個 `bbox`。
2. **圖像預處理:**
    - 使用MMPose的數據處理流程，根據上一步得到的 `bbox` 從原始幀中裁剪出人體圖像。
    - 將裁剪出的圖像縮放到MMPose姿態估計模型（例如，HRNet-w48）所要求的輸入尺寸，比如 `256x192`。
3. **姿態估計推斷:**
    - 將預處理後的人體圖像輸入到加載好的MMPose模型中。
    - 模型輸出17張熱力圖（對應COCO數據集的17個關鍵點）。
4. **後處理與座標獲取:**
    - MMPose對熱力圖進行解碼，找到每個關鍵點（如腳踝、膝蓋、髖部、肩膀）在原始圖像中的 `(x, y)` 座標和置信度。
    - **輸出:** 我們得到一個包含17個 `[x, y, confidence]` 的列表，代表了這個人的完整骨架。

**第三步：應用層邏輯分析**

- **角度計算:** 利用得到的關節座標，我們可以計算出動作的關鍵角度。例如，通過髖部、膝蓋和腳踝三個點的座標，可以利用向量的夾角公式計算出膝蓋的彎曲角度。
        
- **規則判斷:**
    - **深度判斷:** 判斷深蹲的最低點時，髖關節是否低於膝關節。
    - **膝蓋位置:** 判斷膝蓋是否超過腳尖（可以通過比較膝關節和腳踝的水平座標）。
    - **背部姿態:** 判斷肩膀和髖部的連線是否保持相對挺直。
        
- **反饋與互動:**
    
    - 在畫面上即時繪製骨架和角度。
    - 當檢測到動作不標準時（例如，膝蓋彎曲角度不夠），系統可以發出語音或文字提示，如「請再蹲低一點」。
    - 完成一次有效的深蹲後，計數器加一。
        

**其他應用場景:**

- **人機交互:** 通過手勢姿態估計來控制虛擬物件或操作系統。
- **運動分析:** 分析運動員（如高爾夫球手、游泳運動員）的動作以優化表現。
- **安防監控:** 檢測異常行為，如跌倒檢測。
- **動畫與電影特效 (VFX):** 無標記點的動作捕捉 (Markerless Motion Capture)，將真人動作遷移到虛擬角色上。
- **醫療健康:** 康復訓練輔助、步態分析等。

---

### 3. 與其他主流模型的比較

|特性/模型|MMPose (框架)|OpenPose|AlphaPose|Detectron2|
|---|---|---|---|---|
|**主要方法**|包含 Top-down 和 Bottom-up，以 Top-down 為主|Bottom-up|Top-down|主要為 Top-down|
|**核心優勢**|**模組化、靈活性高、算法庫全面**，支持多種任務（人、手、臉、動物），易於擴展和研究|**開創性的 Bottom-up 方法**，在多人擁擠場景下速度快|**高精度**，引入了區域人體提議網絡 (SPPE) 和參數化姿態非極大值抑制 (PGNMS)|作為一個**通用的目標檢測/分割框架**，其姿態估計功能是其中一部分，與分割等任務結合緊密|
|**代表模型**|**HRNet**, ResNet, Lite-HRNet, MobileNet|VGG-based (原始模型)|ResNet-based|ResNet, FPN 等|
|**輸入方式**|**靈活**，可對接任何檢測器 (如MMDetection) 輸出的人體框，或直接處理圖像 (Bottom-up)|整張圖輸入|需要先進行人體檢測，自帶檢測器|內置檢測流程，先檢測後估計|
|**輸出**|關鍵點座標 `[x, y, conf]`|關鍵點座標 + 關聯信息 (JSON格式)|關鍵點座標 `[x, y, conf]` (JSON格式)|統一的數據結構，包含bbox、分割掩碼和關鍵點|
|**訓練友好度**|**非常高**，基於MMEngine，配置化驅動，訓練、評估、測試流程標準化|訓練過程相對複雜，不如MMPose模組化|提供了訓練腳本，但靈活性和擴展性不如MMPose|訓練流程清晰，但主要為Facebook生態服務，定製化可能需要更深入的了解|
|**社區與生態**|**非常活躍**，背靠強大的OpenMMLab生態，與MMDetection, MMClassification等庫無縫集成|影響力深遠，但近年來更新和擴展速度放緩|擁有穩定的用戶群體，在學術界有較高引用|**非常龐大**，是Meta (Facebook AI) 的官方庫，文檔和社區支持都很好|
|**適用場景**|學術研究、工業級項目開發、需要**高度定製化和多任務**的場景|需要**處理大量擁擠人群**且對實時性要求高的場景|追求**最高精度**的單人/多人姿態估計任務|當姿態估計是**一個更大的視覺理解系統**（如實例分割+姿態估計）的一部分時|

#### **總結比較:**

- 如果你是一個**研究者**或需要一個**靈活、強大、可擴展**的姿態估計框架來支持多種任務和模型，**MMPose是首選**。它的代碼質量和文檔都非常出色。
    
- **OpenPose** 更像是一個已經成型的、開箱即用的工具，它的 Bottom-up 方法在特定場景下（如舞台、廣場等多人密集場景）依然有其不可替代的優勢。
    
- **AlphaPose** 在精度上做了很多優化，如果你追求極致的姿-態估計準確率，它是一個非常強的競爭者。
    
- **Detectron2** 則是一個更宏大的框架，姿態估計只是其功能之一。當你需要同時處理檢測、分割、姿態估計等多個任務時，使用Detectron2可以避免在不同框架間切換。





#### Pose estimation後續分析

```
所以pose estimation或mmpose的最後結果是image or video每一frame的每個人的一組key point detection. 那要如何用這個結果連結到後續的application, 譬如action detection, Intention Inference, Gaze estimation等等. 有哪些常用的後續分析跟applications, 以及中文詳細解釋後續分析的個別model, 輸入輸出跟model architecture跟如何training. 並具體舉例說明
```

MMPose 或任何姿態估計模型的最終結果，就是**結構化的時間序列數據**——為影片中的每一幀、每一個人，提供一組描述其身體姿態的關鍵點座標。這一步完成了從**非結構化的像素數據**到**結構化的骨架數據**的轉換。譬如video的某個frame裡某人的17個keypoints的 [ x, y, confidence ]

這個結構化的骨架數據是後續所有高級應用的基石。它去除了背景、光照、衣著顏色等無關噪音，只保留了最核心的姿態和運動信息，使得後續分析變得更簡單、更高效、也更具魯棒性。

接下來，我們詳細探討如何利用這些骨架數據，以及後續分析模型的細節。

---

### 從關鍵點到高級應用的橋樑：特徵工程

直接使用原始的 `(x, y)` 座標序列有時效果不佳，因為它們受限於人在畫面中的位置和大小。因此，在送入後續模型前，通常會進行**特徵工程 (Feature Engineering)**：

1. **歸一化 (Normalization):**
    - **平移不變性:** 以某個中心點（如人體骨盆或軀幹中心）為原點 `(0,0)`，將所有其他關鍵點的座標進行相對位移。這樣，無論這個人站在畫面的哪個位置，其相對骨架姿態都是一樣的。
    - **尺度不變性:** 計算骨架的某個固定長度（如軀幹長度）作為單位長度 "1"，然後將所有關節點之間的距離按此比例縮放。這樣，無論這個人離鏡頭遠近，其標準化的骨架大小都是一致的。
        
2. **動態特徵提取:**
    - **位移 (Displacement):** 計算每個關節點在相鄰幀之間的移動向量（即速度）。
    - **加速度 (Acceleration):** 計算速度的變化。
    - **關節角度 (Joint Angles):** 計算相鄰骨骼之間的夾角，例如膝關節角度、肘關節角度。
    - **骨骼長度與方向 (Bone Length and Orientation):** 直接將骨骼表示為向量。

經過處理後，每一幀的輸入不再是2D座標，而是一個包含了位置、速度、角度等多維度信息的高維特徵向量。一個長度為 `T` 幀的影片片段，其輸入就是一個 `(T, K, C)` 的張量，其中 `T` 是時間序列長度，`K` 是關鍵點數量，`C` 是每個點的特徵維度。

---

### 後續分析模型與應用詳解

|                                                                                                                                        |                                                                                                                                                                                                                           |
| -------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 動作識別 (Action Recognition)<br><br>智慧安防（識別打鬥、跌倒）<br>運動分析（識別投籃、揮拍）<br>人機互動（識別揮手、鼓掌）                                                       | 模型輸入: 一個固定長度的骨架序列。例如，一個 (32, 17, 3) 的張量，代表32幀、17個關鍵點、每個點有 (x, y, confidence) 3個維度<br><br>模型輸出: 一個表示動作類別的標籤，例如 "走路", "跑步", "跌倒"<br><br>主流模型架構:<br>RNN / LSTM / GRU<br>ST-GCN (Spatio-Temporal Graph Convolutional Network) |
| 意圖推斷 / 軌跡預測 <br>(Intention Inference / Trajectory Prediction)<br><br>自動駕駛（預測行人是否會橫穿馬路）<br>智慧零售（預測顧客下一步會走向哪個貨架）<br>人機協作（預測工人的操作意圖以避免碰撞） | 模型輸入: 過去一段時間（例如過去2秒）的骨架序列和/或邊界框(bbox)位置序列。<br><br>模型輸出: 未來一段時間（例如未來3秒）的骨架姿態序列或bbox位置序列<br><br>主流模型架構 (Seq2Seq 模型):<br>Encoder-Decoder<br>Transformer<br>                                                                  |
| 視線估計 (Gaze Estimation)<br><br>注意力分析（分析駕駛員或學生的注意力是否集中）<br>輔助技術（幫助殘障人士用眼睛控制設備）<br>市場研究（分析顧客在看哪個廣告）                                       | 模型輸入: 高解析度的臉部關鍵點，特別是眼睛、瞳孔、眼角、鼻子和臉部輪廓的座標, （可選）裁剪出的雙眼區域的圖像塊 (image patch)<br><br>模型輸出: 一個三維向量 (v) 或兩個角度 (θ,ϕ)，代表視線在三維空間中的方向<br><br>主流模型架構 (Seq2Seq 模型):<br>CNN / MLP                                                        |
|                                                                                                                                        |                                                                                                                                                                                                                           |

#### 1. 動作識別 (Action Recognition)

這是最常見的後續應用，旨在識別一個人在一段時間內（一個影片片段）所做的動作是什麼。

- **具體應用:** 智慧安防（識別打鬥、跌倒）、運動分析（識別投籃、揮拍）、人機互動（識別揮手、鼓掌）。
- **模型輸入:** 一個固定長度的骨架序列。例如，一個 `(32, 17, 3)` 的張量，代表32幀、17個關鍵點、每個點有 `(x, y, confidence)` 3個維度。
- **模型輸出:** 一個表示動作類別的標籤，例如 "走路", "跑步", "跌倒"。
    
- **主流模型架構:**
    - **RNN (Recurrent Neural Networks) / LSTM / GRU:**
        - **架構:** 這些模型天生適合處理序列數據。它們有一個「記憶」單元，可以按時間步（逐幀）讀取骨架數據，並不斷更新其隱藏狀態 (hidden state)。在讀取完整個序列後，最終的隱藏狀態就包含了整個動作的時序信息，最後接一個全連接層進行分類。
        - **訓練:** 使用帶有動作標籤的骨架序列數據集（如 NTU-RGB+D, Kinetics-Skeleton）進行訓練。損失函數通常是**交叉熵損失 (Cross-Entropy Loss)**。
    - **ST-GCN (Spatio-Temporal Graph Convolutional Network):**
        - **核心思想:** 這是目前基於骨架的動作識別領域的**SOTA (State-of-the-art)** 方法。它將人體骨架看作一個**圖 (Graph)**，其中關節是**節點 (Node)**，骨骼是**邊 (Edge)**。
        - **架構:**
            1. **空間圖卷積 (Spatial Graph Convolution):** 在單一幀內，模型通過圖卷積操作來聚合相鄰關節的特徵。例如，「手」的特徵會從「手腕」和「手臂」的特徵中學習，模擬了卷積神經網絡(CNN)中感受野的概念，但作用於非歐幾里得的圖結構上。
            2. **時間卷積 (Temporal Convolution):** 在時間維度上，模型使用標準的1D卷積核來捕捉同一個關節在時間上的運動模式
            - 通過交替堆疊這兩種卷積層，ST-GCN可以同時學習身體部件的空間結構關係和它們隨時間變化的動態模式。
        - **訓練:** 同樣使用交叉熵損失進行分類訓練。ST-GCN的優勢在於它能顯式地利用人體的自然拓撲結構，比RNN更有效地學習局部肢體協同運動。

#### 2. 意圖推斷 / 軌跡預測 (Intention Inference / Trajectory Prediction)

預測一個人的短期未來行為或移動路徑。

- **具體應用:** 自動駕駛（預測行人是否會橫穿馬路）、智慧零售（預測顧客下一步會走向哪個貨架）、人機協作（預測工人的操作意圖以避免碰撞）
- **模型輸入:** 過去一段時間（例如過去2秒）的骨架序列和/或邊界框(bbox)位置序列
- **模型輸出:** 未來一段時間（例如未來3秒）的骨架姿態序列或bbox位置序列。
- **主流模型架構 (Seq2Seq 模型):**
    - **Encoder-Decoder 架構 (編碼器-解碼器):**
        - **Encoder (編碼器):** 通常是一個LSTM或GRU。它負責讀取並「理解」輸入的歷史軌跡序列，將其壓縮成一個固定長度的「上下文向量 (context vector)」，這個向量代表了對過去運動的總結。
        - **Decoder (解碼器):** 另一個LSTM或GRU。它接收編碼器輸出的上下文向量，然後逐個時間步地生成未來的姿態或位置。上一步的輸出會作為下一步的輸入，直到生成所需長度的未來序列。
    - **Transformer:** 近年來，基於自註意力機制的Transformer模型也在此類任務中表現出色，因為它能更好地捕捉序列中長距離的依賴關係。
        
- **訓練:**
    - **數據:** 需要大量的行人/物體運動軌跡數據集（如 a JAAD, PIE）。
    - **損失函數:** 訓練目標是讓模型預測的未來序列與真實的未來序列盡可能接近。因此，損失函數通常是**均方誤差 (Mean Squared Error, MSE)** 或 **L2損失**，計算預測座標和真實座標之間的歐氏距離。
#### 3. 視線估計 (Gaze Estimation)

這個應用比較特殊，它高度依賴臉部關鍵點，而不是全身關鍵點。MMPose同樣支持臉部關鍵點檢測
- **具體應用:** 注意力分析（分析駕駛員或學生的注意力是否集中）、輔助技術（幫助殘障人士用眼睛控制設備）、市場研究（分析顧客在看哪個廣告）。
- **模型輸入:**
    1. 高解析度的**臉部關鍵點**，特別是眼睛、瞳孔、眼角、鼻子和臉部輪廓的座標。
    2. （可選）裁剪出的雙眼區域的圖像塊 (image patch)。
        
- **模型輸出:** 一個三維向量
- **主流模型架構:**
    - **基於幾何的方法:** 早期方法，通過眼睛、瞳孔等關鍵點的幾何關係來估算視線方向。不夠魯棒。
    - **基於學習的方法:**
        - **CNN (卷積神經網絡):** 如果輸入包含眼睛圖像塊，通常會用一個輕量級的CNN（如ResNet-18的變體）來提取圖像特徵。
        - **MLP (多層感知機):** 將臉部關鍵點的**相對座標**（例如，瞳孔相對於眼角的座標）和CNN提取的圖像特徵拼接起來，送入一個MLP中，最終回歸出視線向量。
- **訓練:**
    - **數據:** 需要在嚴格控制的實驗室環境下採集的數據集，其中包含用戶頭部姿態、眼睛圖像和他們正在注視的屏幕上的真實點位（如MPIIGaze）。
    - **損失函數:** 通常是**角度損失 (Angular Loss)**，計算預測視線向量和真實視線向量之間的夾角差異，或者直接用MSE Loss計算向量的差異。

---

### 具體案例：基於骨架的智能安防跌倒檢測系統

我們以**跌倒檢測**為例，一步步說明整個流程。
**目標:** 當監控畫面中的老人發生跌倒時，系統自動發出警報。

**第一步：姿態數據流的獲取 (MMPose)**
- 系統持續接收監控攝影機的影片流。
- 對於每一幀，運行MMPose的Top-down流程（例如 YOLOv3 + HRNet）。
- **輸出:** 為畫面中的每個人生成一組17個關鍵點的座標 `[x, y, conf]`。這些數據被持續存儲起來。

**第二步：為動作識別模型準備輸入**
- **滑動窗口:** 系統維護一個長度為 `T`（例如`T=32`幀，約1-2秒）的滑動窗口。每當有新的一幀數據進來，就將其加入窗口，並移除最老的一幀。
- **數據預處理:**
    1. 從窗口中提取一個人的32幀骨架數據，形成一個 `(32, 17, 3)` 的張量。
    2. **歸一化:** 在每一幀中，找到髖關節中心點的座標 `(x_hip, y_hip)`，然後將所有17個關節點的座標減去這個中心點座標。這使得數據與該人在畫面中的絕對位置無關。
    3. **特徵選擇:** 為了簡化，我們只使用歸一化後的 `(x, y)` 座標，輸入張量變為 `(32, 17, 2)`

**第三步：使用ST-GCN模型進行推斷**
- **模型加載:** 系統預先加載一個已經在大型動作識別數據集（如NTU-RGB+D）上預訓練過，並在特定的跌倒數據集上進行了**微調 (Fine-tuning)** 的ST-GCN模型。該模型被訓練來區分三種動作："站立/行走", "坐/躺", "正在跌倒"。
- **模型輸入:** 將上一步準備好的 `(32, 17, 2)` 張量送入ST-GCN模型。
- **模型內部工作流程 (簡化):**
    1. 模型首先在空間維度上，通過圖卷積分析每一幀的姿態。例如，當膝蓋彎曲且身體重心急劇下降時，空間圖卷積層會捕捉到這種 "不穩定" 的身體結構特徵。
    2. 接著，在時間維度上，時間卷積層會分析這些特徵隨時間的劇烈變化。例如，它會識別出人體質心在垂直方向上的速度突然變得非常大，這正是跌倒的典型特徵。
- **模型輸出:** 模型會輸出一組概率值，例如：`{'站立/行走': 0.1, '坐/躺': 0.05, '正在跌倒': 0.85}`。

**第四步：觸發應用邏輯**
- **決策:** 系統設定一個閾值，例如0.8。
- 當 "正在跌倒" 類別的概率超過0.8時，系統判定發生了跌倒事件。
- **執行動作:** 系統立即觸發警報，例如向家庭成員或護理中心發送帶有事件截圖的通知

這個例子完整地展示了從MMPose的原始像素分析，到骨架數據提取，再到利用先進的時空模型（ST-GCN）進行高級行為理解的全過程。




#### ST-GCN (Spatio-Temporal Graph Convolutional Network)

這就為您詳細介紹ST-GCN，並涵蓋您所要求的各個方面，包括與GNN的比較、網路架構、與RNN的差異，並提供PyTorch程式碼範例。

![[Pasted image 20250904171038.png]]



---

### ST-GCN (Spatio-Temporal Graph Convolutional Network) 詳解


Reference:
ST-GCN论文精读，时空-图卷积神经网络，行为识别 - 除以七的文章 - 知乎
https://zhuanlan.zhihu.com/p/648017486

Youtube: https://www.youtube.com/watch?v=JJZUvhIuvOQ



ST-GCN 是由 Sijie Yan 等人在 2018 年發表的開創性論文《Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition》中提出的模型。它的核心目標是解決**基於骨架的動作識別**問題。

在ST-GCN出現之前，主流方法使用RNN或CNN來處理骨架序列，但這些方法都有缺陷：

- **RNN/LSTM:** 將骨架在每一幀的關節點座標展平為一個長向量，這破壞了人體骨架內在的**空間拓撲結構**。模型必須從零開始隱式地學習哪些關節是相連的。
    
- **CNN:** 將骨架序列轉換為一種「偽圖像 (pseudo-image)」，其中圖像的行代表關節，列代表幀。這種方法雖然能捕捉時序特徵，但骨架關節的排列是人為設定的，無法真正體現其自然的連接關係。
    

**ST-GCN的核心思想是：直接將人體骨架視為一個圖 (Graph)，並在此時空圖上進行卷積操作，從而能夠同時、顯式地學習身體的空間結構特徵和時間動態特徵。**

---

### ST-GCN 與 GNN (Graph Neural Network) 的相同與相異之處

這是一個「一般」與「特化」的關係。ST-GCN **是** GNN 的一種特化應用。

#### 相同之處：

1. **核心運算:** 兩者都基於圖神經網路的核心思想——**消息傳遞 (Message Passing)** 或 **鄰域聚合 (Neighbor Aggregation)**。一個節點（關節）的新特徵是通過聚合其相鄰節點的特徵來計算的。
    
2. **圖結構數據:** 兩者的處理對象都是圖結構數據，由節點 (mathcalV) 和邊 (mathcalE) 組成，即 mathcalG=(mathcalV,mathcalE)。
    

#### 相異之處：

|特性|通用 GNN (General GNN)|ST-GCN|
|---|---|---|
|**圖的結構**|處理**任意、不規則**的圖結構，如社交網路、分子結構、引用網路。圖的大小和連接方式都可以是變化的。|處理**固定、預定義**的圖結構，即人體骨架。所有樣本的圖拓撲（關節如何連接）都是完全相同的。|
|**數據維度**|通常處理單一的靜態圖，沒有時間維度。|專門設計用來處理**圖的序列**，即在模型中顯式地加入了**時間維度**。這是其名稱中 "Spatio-Temporal" 的來源。|
|**卷積實現**|GNN的卷積（如GCN, GAT）需要在任意圖上定義。例如，GCN的卷積操作是 H(l+1)=sigma(hatD−frac12hatAhatD−frac12H(l)W(l))。|由於圖結構固定，其空間圖卷積可以被簡化和優化。原論文提出了一種**分區策略**，將鄰域節點分為向心、離心和自身三組，並為每組分配不同的權重，實現了更細粒度的特徵學習。|
|**應用領域**|節點分類、圖分類、連結預測等廣泛的圖學習任務。|**高度特化**於特定任務，主要是基於骨架的動作識別，也可擴展到其他姿態序列分析任務。|

匯出到試算表

**總結：** 你可以將 GNN 視為一個處理圖數據的通用框架，而 ST-GCN 是這個框架在**人體骨架序列**這個特定問題上的一個高效、成功的特化實現。

---

### ST-GCN 網路架構 (Network Architecture)

#### 1. 輸入 (Input)

ST-GCN的輸入是一個代表骨架序列的張量 (Tensor)，其維度通常是 `(N, C, T, V, M)`：

- `N`: 批次大小 (Batch size)。
    
- `C`: 每個關節點的特徵維度。通常是3，即 `(x, y, confidence)`。
    
- `T`: 時間序列的長度（幀數）。
    
- `V`: 關節點的數量 (Vertices)。例如，COCO數據集為17，NTU-RGB+D為25。
    
- `M`: 一幀畫面中的人數。
    

#### 2. 核心構建塊：ST-GCN Unit

整個網路是由多個 ST-GCN Unit 堆疊而成的。每個 Unit 包含兩個關鍵部分：**空間圖卷積 (GCN)** 和 **時間卷積 (TCN)**。

- **a) 空間圖卷積層 (Spatial GCN Layer):**
    
    - **目標:** 在**單一幀**內，聚合關節的空間鄰域信息。
        
    - **運作方式:** 對於某一幀的某個關節 `v`，模型會聚合與 `v` 通過骨骼直接相連的關節的特徵。例如，「手腕」節點會聚合來自「手肘」和「手掌」節點的信息。
        
    - **數學表達 (簡化版):** f_out=sum_k(mathbfA_kodotmathbfM∗k)f∗inmathbfW_k
        
        - f_in: 輸入的特徵張量 `(C, T, V)`。
            
        - mathbfA∗k: **鄰接矩陣 (Adjacency Matrix)**。這是一個 `(V, V)` 的矩陣，如果關節 `i` 和 `j` 相連，則 A∗ij=1，否則為0。這是**圖結構的核心**。ST-GCN將其擴展為多個鄰接矩陣（分區策略），例如 mathbfA_0 代表節點自身，mathbfA_1 代表相鄰的向心節點等。
            
        - mathbfM_k: **可學習的權重遮罩**。這是一個 `(V, V)` 的矩陣，用於學習每條邊的重要性。
            
        - mathbfW_k: 標準的權重矩陣，類似於CNN中的卷積核，用於特徵變換。
            
        - odot: 逐元素相乘。
            
- **b) 時間卷積層 (Temporal Convolutional Layer):**
    
    - **目標:** 在**時間維度**上，聚合每個關節的時序動態信息。
        
    - **運作方式:** 在GCN層完成空間特徵聚合後，TCN層對每個關節的時間序列進行一維卷積（實際上是用一個 `kernel_size = (K_t, 1)` 的二維卷積實現）。例如，模型會觀察「手腕」節點在連續 `K_t` 幀內的運動模式（例如，來回擺動）。
        

#### 3. 整體架構

1. 輸入的骨架序列首先經過一個數據預處理層（批次標準化）。
    
2. 數據流經約9-10個堆疊的 **ST-GCN Unit**。每個Unit都包含 `GCN -> BatchNorm -> ReLU -> TCN -> BatchNorm -> ReLU` 的流程。
    
3. 通常會在某些Unit之間加入類似ResNet的**殘差連接 (Residual Connection)**，並使用步長 (stride) 為2的時間卷積來進行下採樣，以擴大感受野。
    
4. 經過所有ST-GCN Unit後，得到一個高維的特徵張量。
    
5. 使用**全局平均池化 (Global Average Pooling)** 將 `(C', T', V)` 的特徵圖在時間和空間維度上池化為一個 `(C')` 的特徵向量。
    
6. 最後將該向量送入一個**全連接層 (Softmax分類器)**，輸出每個動作類別的概率。
    

---

### 與 RNN 的差別

|特性|RNN / LSTM|ST-GCN|
|---|---|---|
|**數據處理方式**|**序列化 (Sequential)**：逐幀處理，隱藏狀態在時間步之間傳遞。|**並行化 (Parallel)**：一次性處理整個時空圖，類似於CNN。計算效率更高。|
|**空間結構利用**|**隱式學習**：將每幀的關節點展平為向量，丟失了顯式的拓撲結構，模型必須自行從數據中學習關節間的關係。|**顯式建模**：通過預定義的鄰接矩陣，直接將人體結構的先驗知識（哪個關節與哪個關節相連）融入模型，學習更高效、更具可解釋性。|
|**時間依賴性**|擅長捕捉**全局長時序依賴**，但可能受梯度消失/爆炸問題影響。|通過堆疊多層時間卷積來捕捉時序依賴。感受野是**局部**的，但可以通過堆疊層數來擴大，類似於CNN。|
|**模型感受野**|理論上是整個過去的序列。|感受野是**局部**的（由GCN的鄰居階數和TCN的核大小決定），但通過深層堆疊可以覆蓋整個時空圖。|

匯出到試算表

**核心差異：** ST-GCN最大的優勢在於**顯式地利用了人體的空間圖結構**，這是一個非常強的歸納偏置 (inductive bias)，使得模型不必浪費精力去學習這個固定的結構，而是可以專注於學習基於該結構的運動模式。

---

### PyTorch 程式碼範例

這裡提供一個通用GNN和一個簡化版ST-GCN的程式碼，以作對比。

#### 1. 通用 GNN 範例 (使用 `torch_geometric`)

首先，你需要安裝 `torch_geometric`: `pip install torch_geometric`

```Python
import torch
import torch.nn.functional as F
from torch_geometric.nn import GCNConv
from torch_geometric.data import Data

# 1. 定義圖數據
# 假設一個有4個節點的圖 (e.g., 一個分子)
edge_index = torch.tensor([[0, 1, 1, 2, 2, 3],  # 邊的起始節點
                           [1, 0, 2, 1, 3, 2]], # 邊的結束節點
                          dtype=torch.long)
# 每個節點有2個特徵
x = torch.randn(4, 2) 
data = Data(x=x, edge_index=edge_index)

# 2. 定義一個簡單的GNN模型
class SimpleGNN(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super().__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, out_channels)

    def forward(self, x, edge_index):
        # 第一層圖卷積
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        # 第二層圖卷積
        x = self.conv2(x, edge_index)
        return F.log_softmax(x, dim=1)

# 3. 使用模型
model = SimpleGNN(in_channels=2, hidden_channels=16, out_channels=4)
output = model(data.x, data.edge_index)

print("通用GNN輸出形狀:", output.shape) # torch.Size([4, 4]), 每個節點的分類概率
```

#### 2. 簡化版 ST-GCN 範例 (使用純 PyTorch)

```Python
import torch
import torch.nn as nn

class GraphConvolution(nn.Module):
    """
    簡化的空間圖卷積層
    """
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(out_channels, in_channels))

    def forward(self, x, A):
        # x shape: (N, C_in, T, V)
        # A shape: (V, V)
        # 實現 f_out = A * f_in * W
        x = torch.einsum('nctv,vw->nctw', x, A) # 鄰域聚合
        x = torch.einsum('nctw,oc->notw', x, self.weight) # 特徵變換
        return x.contiguous()

class STGCNBlock(nn.Module):
    """
    簡化的ST-GCN基本單元
    """
    def __init__(self, in_channels, out_channels, kernel_size_t):
        super().__init__()
        self.gcn = GraphConvolution(in_channels, out_channels)
        # 時間卷積, padding保證時間維度不變
        self.tcn = nn.Conv2d(out_channels, out_channels, 
                             kernel_size=(kernel_size_t, 1), 
                             padding=( (kernel_size_t - 1) // 2, 0) )
        self.relu = nn.ReLU()

    def forward(self, x, A):
        x = self.gcn(x, A)
        x = self.relu(x)
        x = self.tcn(x)
        x = self.relu(x)
        return x

class SimpleSTGCN(nn.Module):
    """
    一個非常簡化的ST-GCN模型
    """
    def __init__(self, num_classes, in_channels, num_joints):
        super().__init__()
        self.block1 = STGCNBlock(in_channels, 64, kernel_size_t=9)
        self.block2 = STGCNBlock(64, 128, kernel_size_t=9)
        
        # 全局平均池化
        self.gap = nn.AdaptiveAvgPool2d(1)
        # 分類器
        self.fc = nn.Linear(128, num_classes)

    def forward(self, x, A):
        # x shape: (N, C, T, V)
        # A shape: (V, V)
        x = self.block1(x, A)
        x = self.block2(x, A)
        
        x = self.gap(x) # shape: (N, 128, 1, 1)
        x = x.view(x.size(0), -1) # shape: (N, 128)
        
        x = self.fc(x)
        return x

# 3. 使用模型
# 模擬輸入數據
N, C, T, V = 16, 3, 50, 18 # 16個樣本, 3通道, 50幀, 18個關節
x = torch.randn(N, C, T, V)

# 創建一個簡單的鄰接矩陣 (這裡用單位矩陣代替，實際應為真實骨架連接)
# 加上自環
A = torch.eye(V) 

model = SimpleSTGCN(num_classes=10, in_channels=C, num_joints=V)
output = model(x, A)

print("ST-GCN輸出形狀:", output.shape) # torch.Size([16, 10]), 每個樣本的分類概率
```

---

### 具體舉例說明 ST-GCN 如何工作

**任務：** 識別 "揮手" 動作。

1. **輸入數據：** 一段2秒的影片（假設30fps，共60幀），MMPose提取出18個關節點。輸入張量 `x` 的形狀為 `(1, 3, 60, 18)`。同時，我們有一個固定的 `18x18` 的鄰接矩陣 `A`，它定義了「手」與「手腕」、「手腕」與「手肘」等關節的連接關係。
    
2. **進入第一個ST-GCN Unit：**
    
    - **空間GCN層：**
        
        - 對於第 `t` 幀的「手腕」節點，GCN層會查看鄰接矩陣 `A`，找到它的鄰居——「手肘」和「手掌」。
            
        - 它會將「手肘」和「手掌」的特徵（它們的 `(x, y, conf)` 值）聚合到「手腕」上，並通過可學習的權重 `W` 進行變換。
            
        - **結果：** 「手腕」節點的新特徵不僅包含了自己的位置信息，還**融合了整個手臂的結構信息**。模型學到了一個代表 "手臂姿態" 的局部特徵。
            
    - **時間TCN層：**
        
        - GCN處理完所有幀後，TCN層開始工作。它看的是**同一個關節**在時間上的變化。
            
        - 對於「手腕」這個節點，TCN的9幀卷積核會觀察它在 `t-4` 到 `t+4` 幀的運動模式。
            
        - **結果：** 如果手腕在做左右往復運動，TCN層會捕捉到這個**震盪的時序模式**，並輸出一個強烈的響應。
            
3. **進入更深的ST-GCN Unit：**
    
    - 後面的層會基於前面層提取的「局部時空特徵」進行更高級的組合。
        
    - 例如，深層的GCN可能會學到「手臂擺動」與「身體軀幹穩定」之間的協同關係。
        
    - 深層的TCN會學習這些組合特徵的更長時間的演變規律。
        
4. **最終分類：**
    
    - 在經過所有層後，模型已經提取了非常豐富的、能夠描述「揮手」這個動作的時空特徵。
        
    - 全局池化層將這些複雜的特徵總結成一個單一的特徵向量。
        
    - 最後，分類器看到這個特徵向量，並高概率地將其識別為 "揮手" 類別。
        

通過這種方式，ST-GCN 完美地結合了對身體結構的理解和對運動模式的捕捉，使其在基於骨架的動作識別任務上取得了巨大的成功。