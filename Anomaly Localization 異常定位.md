
|                                             |     |
| ------------------------------------------- | --- |
| [[#### 用五張照片新增異常所需的AI技術與模型]]                |     |
| [[#### 用AutoEncoder進行Anomaly Localization]] |     |
| [[#### 用DINOv2進行Anomaly Localization]]      |     |
|                                             |     |




#### 用五張照片新增異常所需的AI技術與模型

### **核心功能一：如何在「未知缺陷」上產生「物件偵測的邊界框」?**

這個功能的挑戰在於，傳統的物件偵測模型（如YOLO）是**監督式**的，它們只能找到**被明確教過的、已知的**缺陷類型。要讓模型在沒見過某種缺陷的情況下，不僅能識別出「這裡不對勁」，還能精確地**框出位置**，需要採用更先進的非監督式或自監督式方法。

其核心思想是：**讓AI深度學習「什麼是正常」的產品外觀，然後找出並定位所有「不正常」的區域。**

#### **方法一：基於重建 (Reconstruction-Based) 的方法**

這種方法利用模型重建輸入影像的能力來定位異常。

**AI 技術：**

- **自監督學習 (Self-Supervised Learning):** 模型在訓練階段只看大量的「正常」產品照片，學習如何從壓縮的特徵中完美地重建出這些正常照片。
    
- **像素級差異分析 (Pixel-level Difference Analysis):** 透過比較原始影像和模型重建後的影像，找出差異最大的區域。
    

**使用的AI 模型：**

- **自動編碼器 (Autoencoders, AE) / 變分自動編碼器 (VAE):**
    
    1. **訓練：** 使用數百到數千張**無缺陷的正常產品照片**來訓練一個Autoencoder模型。模型會學會一個高效的編碼和解碼過程，能夠將正常的輸入影像完美地重建出來。
        
    2. **推論與定位 (Inference & Localization):**
        
        - 當一張新的、待檢測的產品照片（可能包含未知缺陷）輸入模型時，模型會嘗試重建它。
            
        - 由於模型只學過「正常」的模式，當它遇到異常的區域（例如一個前所未見的凹痕），它將無法很好地重建這個區域。
            
        - **生成殘差圖 (Residual Map):** 將**原始輸入影像**和**模型重建的影像**進行像素對像素的相減，得到一張「殘差圖」或「熱力圖 (Heatmap)」。在這張圖上，重建得好的正常區域會接近黑色（差異小），而重建失敗的異常區域會呈現亮點（差異大）。
            
        - **產生邊界框：** 最後，對這張殘差圖使用傳統的影像處理技術，例如**二值化 (Thresholding)** 和 **輪廓檢測 (Contour Detection)**，就可以自動地找到亮點區域並為其生成一個最緊密的邊界框。
            
- **生成對抗網路 (GANs) 的變體 (如 AnoGAN):**
    
    - 原理與Autoencoder類似，但使用GAN來學習正常影像的分佈。在推論時，它會尋找一個最能生成接近輸入影像的潛在向量，並計算兩者之間的差異圖，同樣可以定位異常。
        

**優點：** 概念直觀，容易理解。 **缺點：** 有時重建的影像可能會過於模糊，或者模型過於強大以至於連異常也「學會」重建了，導致定位不夠精確。

#### **方法二：基於特徵嵌入 (Feature Embedding) 的方法**

這是目前最前沿、效果也最好的方法。它不是比較像素，而是比較高維度的「特徵」。

**AI 技術：**

- **自監督特徵學習 (Self-Supervised Feature Learning):** 利用在超大型數據集（如ImageNet）上預訓練好的深度神經網路（如ResNet, Vision Transformer）作為強大的**特徵提取器 (Feature Extractor)**。
    
- **度量學習 (Metric Learning):** 學習一個特徵空間，在這個空間中，所有「正常」的特徵都聚集在一起，而任何「異常」的特徵都會遠離這個集群。
    

**使用的AI 模型 (State-of-the-art Models):**

- **PatchCore:**
    
    1. **建立「正常特徵」記憶庫 (Memory Bank of Normal Features):**
        
        - 使用一個預訓練好的CNN模型（例如Wide-ResNet）作為特徵提取器。
            
        - 將所有正常的訓練圖片輸入模型，提取出每個區域（patch）的**特徵向量 (Feature Vector)**。
            
        - 將這些代表了「正常」的特徵向量儲存起來，形成一個龐大的「正常特徵記憶庫」。
            
    2. **推論與定位:**
        
        - 對於一張新的待測圖片，同樣將其每個區域的特徵提取出來。
            
        - 對於每個區域的特徵，計算它與記憶庫中**所有正常特徵的最近距離**。
            
        - 如果某個區域的特徵與記憶庫中所有正常特徵的距離都**非常遠**，那麼這個區域就被判定為**異常**。
            
        - **生成異常分數圖 (Anomaly Score Map):** 系統會生成一張熱力圖，圖上每個點的亮度代表了該區域的異常分數。
            
        - **產生邊界框：** 同樣地，對這張熱力圖進行閾值處理和輪廓檢測，即可框出異常位置。
            
- **PaDiM / SPADE:**
    
    - 這些模型與PatchCore思想類似，但不儲存所有正常特徵，而是為每個位置的正常特徵學習一個**多變量高斯分佈 (Multivariate Gaussian Distribution)**。推論時，計算待測特徵屬於這個正常分佈的機率（或馬氏距離），距離越遠越異常。
        

**優點：** 定位通常比重建法更精確，對微小異常更敏感，且效能極高。 **缺點：** 模型和概念較為複雜。

---

### **核心功能二：如何「僅用五張照片就新增加 New Anomalies」?**

這個功能基於前面提到的「異常定位」技術，並結合了**小樣本學習 (Few-Shot Learning)** 的思想。這不是讓模型從零開始學習一個新的缺陷類型，而是在現有強大模型的基礎上，**快速適應和標記**新的異常模式。

**實現方式：**

1. **特徵提取是基礎：**
    
    - 系統的核心是一個強大的、固定的**特徵提取器**（基於方法二中的PatchCore等模型訓練而來）。這個提取器已經學會了如何將任何影像的任何區域轉換成有意義的特徵向量。它的任務是通用的，不需要為新的異常重新訓練。
        
2. **用五張照片建立「原型」 (Prototype Creation):**
    
    - 當使用者上傳五張包含同一種新異常的照片時，系統會執行以下操作：
        
        - 首先，使用前面提到的**異常定位**方法，在這五張照片上自動找到異常區域的熱力圖和初步的邊界框。
            
        - 接著，系統會從這些被定位出的**異常區域**中，提取它們的**特徵向量**。
            
        - **計算原型向量 (Prototype Vector):** 將這五張照片中所有異常區域的特徵向量取平均值，得到一個能夠代表這種「新異常」的**原型向量**。
            
3. **小樣本分類 (Few-Shot Classification):**
    
    - 現在，這個「原型向量」就被儲存起來，並被賦予一個標籤（例如 "新增異常A"）。
        
    - 未來當有新的產品照片進來時，系統的流程變為：
        
        1. **步驟一（定位）：** 使用PatchCore等模型判斷照片中是否存在異常區域，並生成熱力圖。
            
        2. **步驟二（分類）：** 如果檢測到異常，系統會提取該異常區域的特徵向量。
            
        3. **步驟三（匹配）：** 將這個新的異常特徵向量與所有已知的「原型向量」計算**相似度**（例如餘弦相似度或歐氏距離）。
            
        4. 如果它與 "新增異常A" 的原型向量最接近，系統不僅會畫出邊界框，還會自動標註上 "新增異常A"。
            

**總結：** 這個流程非常高效，因為成本最高的模型訓練（學習通用特徵）是一次性的。新增異常類型只需要計算和儲存一個新的原型向量，這個過程極快，完全可以在幾秒鐘內完成，真正實現了用極少量樣本快速適應新場景。

---

### **技術與模型總結**

|功能|主要挑戰|核心AI技術|推薦模型|
|---|---|---|---|
|**未知缺陷定位 (Bounding Box on Unknown Defect)**|在無監督下精確定位異常區域|自監督特徵學習、度量學習、像素級差異分析|**PatchCore (主流且效果好)**, PaDiM, SPADE, 基於Autoencoder/GAN的重建方法|
|**用5張照片新增異常 (Few-Shot Anomaly Addition)**|不重新訓練模型，快速學習和分類新異常|小樣本學習、原型網路 (Prototypical Networks)|在PatchCore等模型的**特徵空間**中，計算並儲存**原型向量**，然後進行最近鄰匹配|





#### 用AutoEncoder進行Anomaly Localization

### **. Autoencoders (AE) 是 Self-Supervised Learning (SSL) model 嗎？**

**是的，Autoencoder 可以被視為自監督學習 (Self-Supervised Learning, SSL) 的一種早期或基礎形式。**

這個說法可能有些學術上的細微差異，但核心概念是相通的：

- **監督式學習 (Supervised Learning):** 需要**人類標記的標籤** (例如，這張圖是「貓」，那張是「狗」)。
    
- **無監督學習 (Unsupervised Learning):** 完全沒有標籤，模型自己從數據中找結構 (例如，將數據點分成K個群集)。
    
- **自監督學習 (Self-Supervised Learning):** **標籤是從數據本身自動生成的**。模型被賦予一個「藉口任務」(Pretext Task)，透過解決這個任務來學習數據的深層表示(representation)。
    

在Autoencoder的案例中：

- **藉口任務：** 就是「**重建原始輸入**」。
    
- **自動生成的標籤：** **輸出層的目標 (target) 就是輸入層本身**。
    

因此，AE不需要人類來告訴它「正確答案」是什麼，它自己從輸入數據中創造了監督信號。這完全符合自監督學習的核心思想。

---

### **2. Autoencoder 的輸入與輸出是什麼？**

- **輸入 (Input):** 原始數據。在您的應用場景中，就是一張**正常的、無缺陷的產品照片** (例如，一個 `256x256` 像素的影像)。
    
- **輸出 (Output):** 模型**嘗試重建**的數據。理想情況下，它應該是一張與輸入**幾乎一模一樣**的 `256x256` 像素的產品照片。
    

這個過程可以想像成： `Encoder(輸入影像) -> 壓縮的特徵向量 (Latent Representation) -> Decoder(特徵向量) -> 重建的影像 (輸出)`

---

### **3. 如何用 AE 建立可檢測未知異常定位 (Anomaly Localization) 的流程？**

這是一個非常經典的方法，以下是詳細的每一步流程：

#### **步驟一：數據收集與準備**

- **關鍵：** 只收集和使用**大量的、正常的、無缺陷的**產品照片作為訓練集。這個數據集定義了什麼是「正常」。
    
- 對所有照片進行標準化預處理，例如統一尺寸 (e.g., 256x256)、灰階化或正規化像素值。
    

#### **步驟二：Autoencoder 模型訓練**

- **目標：** 訓練一個AE模型，使其能夠**盡可能完美地重建正常的輸入影像**。
    
- **過程：**
    
    1. 將一張正常影像 `X` 輸入模型。
        
    2. 模型輸出重建影像 `X'`。
        
    3. 計算**重建誤差 (Reconstruction Error)**，通常是輸入和輸出之間的均方誤差 (Mean Squared Error, MSE)：`L = ||X - X'||²`。
        
    4. 透過反向傳播演算法，不斷調整模型參數（Encoder和Decoder的權重），以**最小化**這個重建誤差 `L`。
        
- 訓練完成後，模型就成了一個「正常模式專家」，它非常擅長重建它在訓練中見過的正常樣本。
    

#### **步驟三：推論與生成殘差圖 (Residual Map) - 核心步驟**

- **目標：** 找出待測影像中哪些區域是模型「不認識」的。
    
- **過程：**
    
    1. 拿一張**待檢測**的影像 `X_test`（可能正常，也可能包含未知缺陷）輸入到**已經訓練好的AE模型**。
        
    2. 模型輸出重建影像 `X'_test`。
        
    3. 計算**殘差圖 (Residual Map)** 或稱**誤差圖 (Error Map)**：`Residual_Map = |X_test - X'_test|`。這是一個逐像素計算原始影像與重建影像差異絕對值的過程。
        
- **結果：**
    
    - 如果 `X_test` 是正常的，模型可以很好地重建它，所以 `Residual_Map` 整體會非常暗（接近黑色），代表差異很小。
        
    - 如果 `X_test` 包含**異常**（例如一個模型沒見過的刮痕），模型在刮痕區域的重建效果會很差，導致 `Residual_Map` 在對應位置出現**明亮的斑點或區域**。
        

#### **步驟四：閾值化與定位**

- **目標：** 從殘差圖中自動框出異常位置。
    
- **過程：**
    
    1. 對殘差圖進行**二值化 (Thresholding)**：設定一個閾值，將圖中亮度高於該閾值的像素點標記為白色（潛在異常），低於的標記為黑色（正常）。
        
    2. 使用電腦視覺算法，如**尋找連通元件 (Connected Component Analysis)** 或**輪廓檢測 (Contour Detection)**，來找到所有白色的群集區域。
        
    3. 為每一個找到的白色區域計算一個最小的**邊界框 (Bounding Box)**。
        
- 這樣，就完成了對未知異常的自動定位。
    

---

### **4. 主流的 Self-Supervised Learning 模型有哪些？**

除了AE，現代SSL模型更加複雜和強大，主要分為幾個流派：

1. **對比式學習 (Contrastive Learning):**
    
    - **思想：** 將一張圖片的不同增強版本（例如旋轉、裁剪、變色）視為「正樣本對」，而將不同圖片視為「負樣本對」。模型學習一個特徵空間，讓正樣本對的特徵盡量靠近，負樣本對的特徵盡量遠離。
        
    - **代表模型：** **SimCLR**, **MoCo**。
        
2. **知識蒸餾 / 師生網路 (Knowledge Distillation / Teacher-Student):**
    
    - **思想：** 使用兩個結構相同但權重不同的網路（一個Teacher，一個Student）。Teacher的權重更新較慢。將同一張圖片的不同增強版本分別輸入兩個網路，目標是讓Student網路的輸出盡量模仿Teacher網路的輸出。
        
    - **代表模型：** **BYOL**, **DINO**, **DINOv2**。這種方法不需負樣本，訓練更穩定。
        
3. **遮蔽影像模型 (Masked Image Modeling, MIM):**
    
    - **思想：** 借鑒了自然語言處理中BERT模型的思想。隨機遮蓋(Mask)掉一張圖片的部分區域(patches)，然後讓模型去預測或重建被遮蓋的內容。
        
    - **代表模型：** **MAE (Masked Autoencoders)**, **BEiT**。








#### 用DINOv2進行Anomaly Localization

### **用 DINOv3 建立可檢測未知異常定位的流程每一步？**

_注意：目前 (2025年) 最新的公開模型是DINOv2。我們假設DINOv3將是其自然演進，核心思想不變。_

這個流程基於**特徵嵌入 (Feature Embedding)**，與AE的重建方法完全不同，也是目前最先進(State-of-the-Art)的方法。

#### **步驟一：載入預訓練的 DINOv3 模型**

- **關鍵：** 我們**不從頭訓練**DINOv3。我們直接使用在**海量通用數據**（如ImageNet或更大的無標籤數據集）上預訓練好的模型。
    
- 這個預訓練好的模型是一個極其強大的**通用視覺特徵提取器**，它能理解影像的深層語義結構。
    

#### **步驟二：建立正常特徵記憶庫 (Memory Bank) - 核心步驟**

- **目標：** 建立一個代表「所有正常模式」的特徵庫。
    
- **過程：**
    
    1. 收集**一批有代表性的正常、無缺陷**的產品照片（不需要像訓練AE那麼多，幾十到幾百張即可）。
        
    2. 將每張正常照片 `X_normal` 輸入 DINOv3 的骨幹網路 (Backbone)。
        
    3. 從網路的中間層提取**特徵圖 (Feature Map)**。這張圖的每個點都是一個高維的**特徵向量**，代表了原始圖片對應小區域的語義資訊。
        
    4. 將所有正常照片提取出的所有特徵向量全部儲存起來，形成一個龐大的「**正常特徵記憶庫**」。這就是我們對「正常」的定義。
        
    
    - _（這正是工業異常檢測SOTA模型如 **PatchCore** 的核心思想）_
        

#### **步驟三：推論與計算異常分數圖 (Anomaly Score Map)**

- **目標：** 找出待測影像中，哪些區域的特徵「不像」我們記憶庫中的任何一個正常特徵。
    
- **過程：**
    
    1. 拿一張待檢測影像 `X_test` 輸入 DINOv3 提取其特徵圖。
        
    2. 對於特徵圖上的**每一個特徵向量** `f_test`：
        
        - 計算 `f_test` 與「正常特徵記憶庫」中**所有向量的距離**。
            
        - 找到**最近的那個正常特徵**的距離（Nearest Neighbor Distance）。
            
        - 這個「最近距離」就是該區域的**異常分數**。距離越大，代表它越不像任何一個我們已知的正常模式，因此越可能是異常。
            
    3. 將每個區域的異常分數組合起來，就形成了一張**異常分數圖 (Anomaly Score Map)**，也就是一張熱力圖。
        

#### **步驟四：閾值化與定位**

- 這一步與AE的流程完全相同：對異常分數圖進行**二值化**，然後用**輪廓檢測**等方法找到異常群集並生成**邊界框**。
    

---

### **6. AE vs. DINOv3 檢測未知異常定位的優缺點比較**

|比較維度|Autoencoder (AE) - 基於重建|DINOv3 - 基於特徵嵌入|
|---|---|---|
|**核心原理**|學習重建正常影像，透過**像素級的重建誤差**來定位異常。|學習正常影像的**高維語義特徵**分佈，透過**特徵空間的距離**來定位異常。|
|**效能與精度**|**較低**。對於複雜的紋理或結構性缺陷，定位可能不準確。有時會將正常但罕見的模式誤判為異常。|**非常高 (State-of-the-Art)**。DINOv3學習的是抽象的語義特徵，能更好地區分微小的真實缺陷和正常的紋理/光照變化。|
|**對微小變化的敏感度**|**高**。對光照、噪點、相機位置等微小變化非常敏感，容易產生誤報，因為這些變化都會導致像素級的重建誤差。|**低 (魯棒性強)**。DINOv3的特徵具有一定的不變性，對光照、視角等變化不敏感，能更專注於檢測真正的結構性或語義上的異常。|
|**計算成本**|**訓練成本較低**：在特定產品數據集上從頭訓練AE相對較快。<br>**推論成本低**：一次前向傳播即可。|**預訓練成本極高**：訓練DINOv3需要巨大的計算資源（但我們通常直接使用預訓練模型）。<br>**應用成本**：建立記憶庫和計算最近鄰距離的**推論成本相對較高**。|
|**泛化能力**|**差**。為A產品訓練的AE模型，幾乎無法直接用於B產品，需要重新訓練。|**極強**。預訓練的DINOv3是通用的視覺模型，其提取的特徵對各種產品和場景都有效。通常只需為新產品重新建立一個小的正常特徵記憶庫即可。|
|**實現複雜度**|**較簡單**。模型結構和原理相對直觀，容易實現。|**較複雜**。需要理解特徵提取、記憶庫構建、最近鄰搜索等概念，實現起來步驟更多。|
|**數據需求**|需要**大量**的正常樣本來訓練一個魯棒的AE模型。|只需要**少量有代表性**的正常樣本來建立特徵記憶庫即可，因為模型的知識主要來自預訓練。|

您的敘述可以拆解成幾個關鍵點，我們來逐一檢視：

#### **1. Feature Map 的維度**

> "...當選用5張正常image (1024pixel x 1024pixel x 3 channel)輸入DINOv2 Backbone, 網路的中間層提取feature map的data維度是甚麼?"

**這部分基本正確，但需要更精確的計算。**

DINOv2是一個基於**Vision Transformer (ViT)** 的架構。ViT不像CNN那樣逐像素滑動，而是先把影像切割成一個個不重疊的**圖像塊 (Patch)**。DINOv2的標準patch size是 **14x14 像素**。

- **輸入影像尺寸**: `1024 x 1024`
    
- **Patch尺寸**: `14 x 14`
    
- **每行的Patch數量**: `1024 / 14 ≈ 73.14`，取整數為 `73`
    
- **每列的Patch數量**: `1024 / 14 ≈ 73.14`，取整數為 `73`
    
- **特徵圖的空間維度**: 因此，輸出的feature map在空間維度上是 `73 x 73`。
    

所以，DINOv2 Backbone輸出的不是 `1024x1024` 的圖，而是 `73x73` 的圖。

#### **2. Feature Map 上每個點的意義**

> "...這張圖的每個點都是一個高維的feature vector, 所以這每個點代表是1 pixel單位?"

**這是一個關鍵的誤解點。**

- **不對，每個點不代表1個pixel。**
    
- **正確來說，Feature map上的每一個點，都代表原始影像中一個 `14x14` 像素的圖像塊 (Patch) 的特徵摘要。**
    

您可以把它想像成，DINOv2先用一個 `14x14` 的「放大鏡」掃過整張 `1024x1024` 的大圖，每看一個區域，就用一個高維向量來總結這個區域的內容。最後，它給出了一個 `73x73` 的「摘要報告」，這就是feature map。

#### **3. High Dimension (高維) 的意義**

> "...然後high dimension 譬如是1024就代表這個pixel的語義資訊?"

**這部分理解非常準確，只需將 "pixel" 替換為 "patch" 即可。**

- 是的，這個高維度（例如 `dinov2_large` 模型的維度是 **1024**）代表了對應 **patch (14x14 像素區域)** 的深層**語義資訊 (Semantic Information)**。
    
- 這個1024維的向量不是RGB值，而是經過龐大網路計算後得到的抽象特徵。它可能包含了這個patch的紋理、邊緣、形狀，甚至在物體中的相對位置等複雜資訊。這也是DINOv2強大的地方。
    

所以，一張 `1024x1024x3` 的影像輸入後，我們得到的feature map維度是 **`73 x 73 x 1024`**。

#### **4. 建立參考特徵的方式**

> "...最後將這五張image的每個pixel的語義資訊平均形成feature map."

**這是第二個，也是最核心的誤解點。實際作法不是「平均」。**

- **不對，我們不對feature map進行平均。** 如果這樣做，會產生災難性的後果。想像一下，第一張照片的產品稍微偏左，第二張稍微偏右。如果你將 `(i, j)` 位置的特徵向量進行平均，你得到的會是一個模糊的、混合了產品邊緣和背景的無意義特徵。
    
- **正確的做法：建立一個「正常特徵記憶庫 (Normal Feature Memory Bank)」。**
    
    - 我們將5張正常影像分別通過DINOv2，得到5個 `73x73x1024` 的feature map。
        
    - 我們將這5個feature map中**所有**的特徵向量（總共有 `5 * 73 * 73 = 26,645` 個向量，每個向量都是1024維）**全部收集起來**，放入一個巨大的集合或「庫」中。
        
    - 這個庫代表了我們所見過的**所有正常patch**的特徵集合。它捕捉了正常模式的**多樣性**，而不是一個單一的平均值。
        

#### **5. 進行異常比對的方式**

> "...當輸入一張檢測image也輸入DINOv2 backbone, 然後比較這個feature map, 如果檢測image某個pixel的high dimension feature跟feature map對比差異太大, 則視為這pixel異常."

**基於上面第4點的校正，這裡的比對方式也需要修正。**

- **不對，不是點對點 (Point-to-Point) 的比較。**
    
- **正確的做法：點對庫 (Point-to-Bank) 的比較，尋找「最近鄰居 (Nearest Neighbor)」。**
    
    1. 將待測影像輸入DINOv2，得到其 `73x73x1024` 的feature map。
        
    2. 然後，對於待測feature map上的**每一個**特徵向量 `f_test`（總共有 `73*73 = 5329` 個）：
        
        - 我們拿 `f_test` 去跟「正常特徵記憶庫」中**所有 26,645 個**正常的特徵向量進行比較。
            
        - 計算 `f_test` 與庫中每一個向量的距離（例如歐氏距離）。
            
        - 找到庫中與 `f_test` **距離最近**的那個正常特徵向量，這個距離值就是 `f_test` 的**異常分數**。
            
    3. 這個分數的直觀意義是：「**這個待測patch，與我們見過的所有正常patch中最像它的那個，到底有多不像？**」如果連最像的那個都差很遠，那它極有可能就是異常。
        
    4. 我們為待測影像的每個patch都計算出一個異常分數後，就得到了一張 `73x73` 的**異常分數圖 (Anomaly Score Map)**。
        

---

### **校正後的正確流程總結**

讓我們把正確的流程再完整地走一遍：

**階段一：建立參考模型（離線）**

1. **數據準備：** 準備N張（例如5張） `1024x1024` 的正常產品影像。
    
2. **特徵提取：** 將這5張影像逐一輸入預訓練好的DINOv2 Backbone，得到5個維度為 `73x73x1024` 的feature map。
    
3. **建立記憶庫：** 將這5個feature map中所有的特徵向量（共 `5 * 73 * 73 = 26,645` 個1024維向量）匯總起來，建立「正常特徵記憶庫」。
    

**階段二：即時檢測與定位（線上）**

1. **獲取影像：** 產線傳來一張新的 `1024x1024` 待測影像。
    
2. **特徵提取：** 將待測影像輸入同一個DINOv2 Backbone，得到一個 `73x73x1024` 的待測feature map。
    
3. **異常分數計算：**
    
    - 遍歷待測feature map上的每一個特徵向量 `f_test`。
        
    - 計算 `f_test` 與「正常特徵記憶庫」中所有向量的距離，並找出**最小值**（到最近鄰居的距離）。
        
    - 這個最小值就是該patch的異常分數。
        
    - 所有patch的分數構成一張 `73x73` 的**異常分數圖**。
        
4. **定位與輸出：**
    
    - 將 `73x73` 的異常分數圖透過**上採樣 (Upsampling)** 放大回 `1024x1024`，使其與原始影像對齊。
        
    - 對放大後的熱力圖設定一個閾值，高於閾值的區域即為異常。
        
    - 使用輪廓檢測等方法，為這些異常區域生成**邊界框 (Bounding Box)**。
        

您的敘述非常接近正確答案，最大的區別在於用**「記憶庫」**取代**「平均圖」**，用**「點對庫的最近鄰搜索」**取代**「點對點的直接比較」**。這個改進是至關重要的，它保留了正常樣本的豐富性和多樣性，從而實現了極高精度的未知異常檢測。