
- **將大模型 (Teacher) 蒸餾為小模型 (Student)**，在減少模型大小的同時保持準確度。
- 例如 **使用 DeepLabV3+ (Teacher) 來訓練 MobileUNet (Student)**。

```python
from torch.nn import functional as F

teacher_output = teacher_model(input_tensor)
student_output = student_model(input_tensor)

loss = F.kl_div(F.log_softmax(student_output, dim=1),
                F.softmax(teacher_output, dim=1), reduction='batchmean')

```

[Pytorch Knowledge Distillation Tutorial](https://docs.pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html)


現在讓我們嘗試透過加入老師來提高學生網路的測驗準確性。知識提煉是一種實現這一目標的直接技術，基於兩個網路都輸出我們類別的機率分佈。因此，兩個網路共享相同數量的輸出神經元。該方法的工作原理是將額外的損失納入傳統的交叉熵損失中，該損失基於教師網路的 softmax 輸出。假設經過適當訓練的教師網路的輸出活化會攜帶額外的訊息，學生網路可以在訓練期間利用這些資訊。原始研究表明，利用軟目標中較小機率的比率有助於實現深度神經網路的根本目標，即在數據上創建相似結構，其中相似的物件被更緊密地映射在一起。例如，在 CIFAR-10 中，如果卡車有輪子，它可能會被誤認為是汽車或飛機，但它不太可能被誤認為是狗。因此，有理由假設有價值的資訊不僅存在於經過適當訓練的模型的最高預測中，而且存在於整個輸出分佈中。然而，單獨的交叉熵不能充分利用這些訊息，因為非預測類別的活化往往非常小，以至於傳播的梯度不會顯著改變權重來建構這個理想的向量空間。

當我們繼續定義引入師生動態的第一個輔助函數時，我們需要包含一些額外的參數：

- `T`：溫度控制輸出分佈的平滑度。越大`T`，分佈越平滑，因此較小的機率會獲得更大的提升。
    
- `soft_target_loss_weight`：分配給我們即將包含的額外目標的權重。
    
- `ce_loss_weight`：分配給交叉熵的權重。調整這些權重可以推動網路針對任一目標進行最佳化。


Method1
將額外的損失納入傳統的交叉熵損失中，該損失基於教師網路的 softmax 輸出。假設經過適當訓練的教師網路的輸出活化會攜帶額外的訊息，學生網路可以在訓練期間利用這些資訊。
![[Pasted image 20250517151016.png]]


Method2:
需要先解決一個問題。當我們將蒸餾應用於輸出層時，我們提到兩個網路具有相同數量的神經元，等於類別的數量。然而，對於我們的捲積層之後的層來說並非如此。在這裡，在最後的捲積層變平坦之後，老師比學生擁有更多的神經元。我們的損失函數接受兩個相同維度的向量作為輸入，因此我們需要以某種方式匹配它們。我們將透過在老師的捲積層之後添加一個平均池化層來解決這個問題，以降低其維度以匹配學生的維度。

為了繼續，我們將修改我們的模型類，或建立新的模型類。現在，forward 函數不僅傳回網路的 logit，還傳回捲積層之後的扁平隱藏表示。我們為修改後的教師加入了上述池化。
![[Pasted image 20250517151036.png]]