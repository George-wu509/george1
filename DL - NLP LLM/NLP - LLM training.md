

|                                       |                                                                                                        |
| ------------------------------------- | ------------------------------------------------------------------------------------------------------ |
| 掩碼語言模型（Masked Language Modeling, MLM） | MLM是一種在訓練過程中隨機掩蓋輸入文本中的部分詞彙，並讓模型預測這些被掩蓋詞彙的方法。這種方法使模型能夠同時利用上下文資訊，學習詞彙之間的關聯性。==BERT==模型即採用了MLM的訓練方式。      |
| 自回歸預測（Autoregressive Prediction）      | 自回歸預測方法讓模型根據已知的序列，逐步預測下一個元素。在語言模型中，這意味著模型根據前面的詞彙，預測下一個詞。GPT系列模型（如==ChatGPT==）採用了這種訓練方式。                |
| 對比學習（Contrastive Learning）            | 對比學習旨在將匹配的圖像和文本對映射到相似的特徵空間，同時將不匹配的對拉開距離。這種方法在視覺語言模型的預訓練中被廣泛使用，如==CLIP==模型。                             |
| 掩碼圖像建模（Masked Image Modeling, MIM）    | I類似於MLM，MIM在訓練過程中隨機掩蓋圖像的部分區域，讓模型預測被掩蓋的內容。這種方法使模型能夠學習圖像的局部和全局特徵。==ViT（Vision Transformer）==模型採用了這種訓練方式。 |
| 圖像-文本匹配（Image-Text Matching, ITM）     | ITM方法讓模型學習判斷給定的圖像和文本描述是否匹配。這種方法有助於模型理解圖像和文本之間的關聯性，並在多模態任務中表現出色。                                        |
| 多模態融合（Multimodal Fusion）              | 在視覺語言模型中，將圖像和文本的特徵進行融合，使模型能夠同時理解和處理多種模態的信息。這種方法在多模態任務中，如圖像描述生成和視覺問答中，表現出色。                             |
|                                       |                                                                                                        |

機器學習中，根據訓練資料的標註情況，主要分為以下四種學習方式：

1. **有監督學習（Supervised Learning）**：
    
    - **定義**：使用包含輸入和對應標籤的已標記資料進行訓練，目的是學習從輸入到輸出的映射關係。
    - **應用**：分類（如垃圾郵件檢測）、迴歸（如房價預測）等任務。
    - **特點**：需要大量標記資料，訓練過程中模型會根據預測結果與真實標籤之間的誤差進行調整。
2. **無監督學習（Unsupervised Learning）**：
    
    - **定義**：使用未標記的資料進行訓練，目的是發現資料中的隱含結構或模式。
    - **應用**：聚類分析（如客戶分群）、降維（如主成分分析）等任務。
    - **特點**：不需要標記資料，模型自行探索資料的內在結構。
3. **自監督學習（Self-Supervised Learning）**：
    
    - **定義**：從未標記資料中自動生成標籤，將無監督問題轉化為有監督問題進行訓練。
    - **應用**：語言模型的預訓練（如BERT的掩碼語言模型）、圖像填補等任務。
    - **特點**：利用資料本身的特性生成標籤，無需人工標記，適合大規模資料的預訓練。
4. **半監督學習（Semi-Supervised Learning）**：
    
    - **定義**：同時使用少量標記資料和大量未標記資料進行訓練，結合有監督和無監督學習的優點。
    - **應用**：在標記成本高的領域，如醫療影像分析，利用少量標記資料輔以大量未標記資料進行模型訓練。
    - **特點**：有效利用未標記資料，降低對大量標記資料的依賴，提高模型的泛化能力。

**關於大型語言模型（LLM）或視覺大型語言模型（VLLM）的訓練過程：**

這些模型的訓練通常分為兩個階段：預訓練（pre-training）和微調（fine-tuning）。

- **預訓練階段（Pre-training）**：
    
    - **方法**：主要採用自監督學習方法，如掩碼語言模型（MLM）、自回歸預測（Autoregressive Prediction）、對比學習（Contrastive Learning）、掩碼圖像建模（MIM）等。
    - **特點**：利用大量未標記資料，透過設計特定任務（如在文本中隨機掩碼部分詞彙，讓模型預測被掩碼的詞）進行訓練，學習資料的內在結構和特徵。
- **微調階段（Fine-tuning）**：
    
    - **方法**：採用有監督學習方法，使用特定任務的標記資料對預訓練模型進行微調。
    - **特點**：在預訓練的基礎上，透過有標記的資料，讓模型適應特定任務的需求，提高在該任務上的表現。


大型語言模型(LLM)和視覺語言模型(Vision LLM)的常見訓練方式可以分為以下幾類:

1. 自監督學習 (Self-supervised Learning)

這是LLM和Vision LLM最常用的預訓練方法,主要包括:

a) 掩碼語言模型 (Masked Language Model, MLM)

- 使用:BERT等模型
- 方法:隨機遮蔽輸入文本中的一些詞,訓練模型預測這些被遮蔽的詞
- 為何是自監督:模型從數據本身自動生成監督信號,無需人工標註

b) 自迴歸預測 (Autoregressive Prediction)

- 使用:GPT系列模型
- 方法:給定前面的詞,預測下一個詞
- 為何是自監督:同樣利用數據本身產生監督信號

c) 對比學習 (Contrastive Learning)

- 使用:CLIP等視覺語言模型
- 方法:學習將相關的圖像和文本映射到相近的向量空間
- 為何是自監督:利用圖像-文本對作為天然的監督信號

2. 監督學習 (Supervised Learning)

主要用於模型的微調階段:

a) 指令微調 (Instruction Tuning)

- 方法:使用人工編寫的指令-回應對來微調模型
- 為何是監督學習:使用人工標註的數據對

b) 人類反饋的強化學習 (Reinforcement Learning from Human Feedback, RLHF)

- 方法:使用人類反饋來訓練獎勵模型,然後用強化學習來優化語言模型
- 為何是監督學習:依賴人類提供的偏好標註

3. 半監督學習 (Semi-supervised Learning)

在某些情況下使用:a) 偽標籤方法 (Pseudo-labeling)

- 方法:使用預訓練模型為大量無標籤數據生成偽標籤,然後用這些數據進行微調
- 為何是半監督:結合了少量人工標註數據和大量自動標註數據

4. 無監督學習 (Unsupervised Learning)

在LLM和Vision LLM中較少直接使用,但某些預處理步驟可能涉及:a) 詞嵌入 (Word Embedding)

- 方法:學習詞的向量表示,捕捉語義關係
- 為何是無監督:不需要任何標籤,純粹從詞的共現關係學習

分類說明:

1. 自監督學習被廣泛用於LLM和Vision LLM的預訓練,因為:
    
    - 可以利用海量的無標籤數據
    - 自動從數據中生成監督信號,無需昂貴的人工標註
    - 學習到的表徵通常具有良好的通用性
    
2. 監督學習主要用於微調階段,因為:
    
    - 可以針對特定任務進行優化
    - 引入人類知識和偏好,提高模型的可控性和安全性
    
3. 半監督學習在某些情況下很有用,因為:
    
    - 可以結合少量高質量標註數據和大量無標籤數據
    - 在資源有限的情況下提高模型性能
    
4. 純粹的無監督學習在LLM訓練中較少直接使用,因為:
    
    - 自監督學習通常能提供更有效的學習信號
    - 語言和視覺數據本身就包含豐富的結構信息,適合自監督方法
    

總的來說,LLM和Vision LLM的訓練通常是多個階段的組合,從自監督預訓練開始,然後通過監督或半監督方法進行任務特定的微調。這種方法可以充分利用大規模無標籤數據,同時保持對特定任務的適應性和可控性。