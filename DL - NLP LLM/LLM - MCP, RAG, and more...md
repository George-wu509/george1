
MCP - 

RAG - (Retrieval-Augmented Generation)

AI Agent -

MoE -

LoRA - (a lightweight fine-tuning method)

RLHF - (Reinforcement Learning from Human Feedback)
hallucinate - 

Speculative decoding - 是Google和DeepMind发现的大模型推理加速方法

packing - 把多个样本（或样本的不同段）拼接到一个固定长度的序列里训练，从而减少padding浪费、提升GPU利用率。

|                           |     |
| ------------------------- | --- |
| [[#### MCP and RAG]]      |     |
| [[#### AI Agent and MoE]] |     |
|                           |     |


#### MCP and RAG

在大型語言模型（LLM）的快速發展下，模型上下文協定（MCP）與檢索增強生成（RAG）成為了兩項關鍵技術，旨在突破LLM的內在限制，擴展其應用場景。本文將以中文詳細解釋MCP與RAG的定義，並深入探討它們各自以及共同與LLM之間的關係。

### MCP (Model Context Protocol)：賦予LLM與世界互動的能力

**MCP，全稱模型上下文協定（Model Context Protocol）**，可以被理解為一套標準化的「溝通語言」，讓大型語言模型能夠安全、有效地與外部世界進行雙向互動。傳統的LLM主要依賴其龐大的訓練數據來生成內容，但這也意味著它們的知識是靜態的，且無法執行現實世界的任務。MCP的出現正是為了解決這個問題。

**MCP與LLM的關係：**

MCP扮演著LLM與外部工具、數據庫、API及其他應用程式之間的橋樑。其核心關係可以總結為以下幾點：

- **賦予行動能力：** 透過MCP，LLM不再僅僅是文本生成器。它可以根據用戶的指令，調用外部工具來執行特定任務，例如：發送電子郵件、預訂餐廳、查詢即時股價、在客戶關係管理系統（CRM）中更新資訊等。
    
- **獲取即時與動態資訊：** LLM的訓練數據有其截止日期，無法獲取最新的資訊。MCP允許LLM連接到即時數據源，例如新聞API、天氣預報服務等，從而提供更準確、更具時效性的回答。
    
- **標準化介面：** MCP為不同的工具和服務提供了一個統一的溝通標準。開發者無需為每個LLM或每個工具都開發一套獨特的整合方案，大大降低了開發複雜性，促進了生態系的建立。
    
- **安全性與可控性：** MCP的設計也著重於安全性。它提供了一套框架來管理LLM對外部工具的訪問權限，確保其行為在可控的範圍內，避免潛在的風險。
    

簡單來說，如果將LLM比作一個聰明的大腦，MCP就如同賦予這個大腦一雙可以感知世界、並與之互動的手腳。

### RAG (Retrieval-Augmented Generation)：為LLM注入精準知識

**RAG，全稱檢索增強生成（Retrieval-Augmented Generation）**，是一種旨在提升LLM生成內容準確性與相關性的技術框架。其核心思想是，在LLM生成回答之前，先從一個或多個外部知識庫中檢索出與用戶問題最相關的資訊，並將這些資訊作為額外的上下文（Context）提供給LLM。

**RAG與LLM的關係：**

RAG的出現主要是為了解決LLM可能出現的「幻覺」（Hallucination）問題，也就是模型在不確定的情況下，可能會編造不實的資訊。RAG與LLM的關係體現在以下幾個方面：

- **提升事實準確性：** 透過從權威的知識庫（例如：企業內部文件、專業領域的數據庫）中檢索資訊，RAG可以確保LLM生成的內容是有事實依據的，大大減少了錯誤資訊的產生。
    
- **提供最新知識：** 與MCP類似，RAG也能讓LLM接觸到其訓練數據之外的最新資訊。只要知識庫保持更新，LLM就能夠提供與時俱進的回答。
    
- **領域知識的應用：** 對於需要特定領域知識的應用場景（例如：法律、醫療、金融），RAG可以將相關的專業文件作為知識庫，讓通用的LLM也能夠在特定領域提供專業級別的回答，而無需對模型本身進行昂貴的重新訓練。
    
- **可解釋性與可追溯性：** 由於RAG的回答是基於檢索到的特定文件，因此可以追溯到資訊的來源，增加了生成內容的可信度和可解釋性。
    

如果說LLM是一個擁有通用知識的學生，RAG就如同在考試前，為這位學生提供了一本與考題相關的參考書，幫助他更精準、更有依據地回答問題。

### RAG、MCP與LLM的協同關係：打造更強大的AI系統

RAG與MCP並非互斥的技術，它們可以相輔相成，共同提升LLM的能力，構建出更為強大和智能的AI系統。

- **RAG提供「靜態」知識，MCP提供「動態」互動：** 在一個複雜的應用中，RAG可以負責從企業的知識庫中檢索產品規格、操作手冊等相對靜態的資訊，而MCP則可以負責查詢即時的庫存數量、用戶的訂單狀態等動態變化的數據。
    
- **協同工作流程：** 想像一個客戶服務的AI助理。當用戶詢問「我的訂單什麼時候能到貨？」時：
    
    1. AI助理首先可能需要透過**MCP**調用訂單查詢API，獲取用戶的具體訂單資訊和物流狀態（即時、動態數據）。
        
    2. 如果用戶接著問「這款產品的保固政策是什麼？」，AI助理則會透過**RAG**從公司的政策文件中檢索相關的保固條款（靜態、權威知識）。
        
    3. 最後，LLM將整合這兩部分資訊，生成一個既包含即時訂單狀態，又準確解釋了保固政策的完整回答。
        

**總結來說：**

|技術|核心功能|與LLM的關係|應用場景|
|---|---|---|---|
|**MCP**|**賦予行動能力**，與外部世界雙向互動|作為LLM的「手腳」，執行任務、獲取即時數據|智能助理、自動化工作流程、物聯網控制|
|**RAG**|**注入精準知識**，提升回答的準確性與相關性|作為LLM的「參考書」，提供特定領域的、最新的、有依據的資訊|企業知識問答、專業領域的聊天機器人、內容生成|

MCP和RAG的結合，使得大型語言模型不再是一個封閉的系統。RAG為其提供了深厚的知識基礎，而MCP則為其打開了通往現實世界的大門，讓LLM能夠在更廣泛、更複雜的場景中發揮其潛力，成為真正意義上的智能體（Agent）。





#### AI Agent and MoE

這就為您詳細解釋 AI Agent（人工智慧代理）以及 MoE（專家混合模型）的定義，並深入探討它們與大型語言模型（LLM）之間的關係。

### AI Agent (人工智慧代理)：超越對話，主動執行的智能體

**什麼是 AI Agent？**

AI Agent（人工智慧代理）可以被理解為一個能夠自主感知環境、進行決策並執行任務的智能實體。它不僅僅是像傳統聊天機器人那樣被動地回應查詢，而是具備一定程度的自主性，可以為了達成一個預設的目標而主動地進行規劃、推理並採取一系列行動。

一個完整的 AI Agent 通常由以下幾個核心部分組成：

1. **大腦 (Brain)：** 這是 Agent 的核心，通常由一個或多個大型語言模型（LLM）擔任。LLM 負責理解用戶意圖、進行複雜推理、拆解任務、制定計劃以及做出決策。
    
2. **感知 (Perception)：** Agent 需要能夠感知其所處的數位或物理環境。這可能意味著讀取文本、分析圖片、監控數據流等，以收集完成任務所需的資訊。
    
3. **行動 (Action)：** 這是 Agent 影響其環境的方式。行動可以透過調用工具（Tools）來實現，例如：使用搜尋引擎查找資訊、操作 API 來預訂機票、執行程式碼來分析數據等。
    
4. **記憶 (Memory)：** 為了能夠執行長期且複雜的任務，Agent 需要具備記憶能力。這包括短期記憶（記住當前對話的上下文）和長期記憶（儲存過去的經驗、用戶偏好等，以便從中學習和改進）。
    

**AI Agent 與 LLM 的關係：**

如果說 LLM 是一個擁有淵博知識、擅長推理的「大腦」，那麼 AI Agent 就是圍繞這個「大腦」構建的一個完整的「行動體」。它們的關係可以總結如下：

- **LLM 是 AI Agent 的核心引擎：** LLM 為 AI Agent 提供了最關鍵的認知能力。沒有 LLM 的強大語言理解、推理和規劃能力，AI Agent 就無法理解複雜的指令，也無法制定出合理的行動步驟。LLM 的能力上限，在很大程度上決定了 AI Agent 的智能上限。
    
- **AI Agent 是 LLM 能力的延伸與實現：** LLM 本身是一個文本生成模型，它無法直接與外部世界互動。AI Agent 則為 LLM 搭建了一個框架，賦予它「手」和「腳」（即工具調用能力），讓 LLM 的智慧能夠真正落地，去完成現實世界中的任務。LLM 負責思考「做什麼」和「如何做」，而 Agent 框架負責「實際去做」。
    
- **從「能說」到「能做」的飛躍：** LLM 的出現讓機器學會了流利地「說」，而 AI Agent 的目標則是讓機器實現自主地「做」。這是一個從語言智能到行動智能的關鍵飛躍。例如，你可以告訴 LLM 「幫我寫一封預訂餐廳的郵件」，而你可以直接命令一個 AI Agent 「幫我預訂今晚七點在市中心的三人桌」，Agent 會自己去搜尋餐廳、檢查空位、並完成預訂，而無需你指導每一步。
    

簡單來說，**LLM 是 AI Agent 的大腦，而 AI Agent 是 LLM 實現自主任務的完整形態。**

---

### MoE (Mixture of Experts)：更高效、更強大的模型架構

**什麼是 MoE？**

MoE，全稱是專家混合模型（Mixture of Experts），它是一種神經網路架構，特別是應用在 Transformer 模型（LLM 的基礎架構）中。傳統的大型語言模型是一個龐大而密集的（Dense）網路，意味著當模型處理任何輸入時，其所有的參數都會被啟動和計算。這種方式雖然強大，但也極其消耗計算資源。

MoE 架構則採用了一種更聰明、更高效的「分而治之」策略。它的核心思想是：

1. **專家網路 (Expert Networks)：** 將一個龐大的前饋神經網路（Feed-Forward Network, FFN）層，拆分成多個規模較小、功能各異的「專家」網路。每個專家都可能擅長處理特定類型或領域的資訊（例如，一個專家可能擅長處理程式碼，另一個擅長處理詩歌）。
    
2. **門控網路 (Gating Network)：** 引入一個輕量級的「路由器」或「門控」網路。當模型接收到一個輸入（例如一個詞或一個句子）時，門控網路會快速判斷這個輸入應該交給哪些專家來處理最合適。
    
3. **稀疏啟動 (Sparse Activation)：** 門控網路會選擇性地只啟動一小部分（通常是一到兩個）最相關的專家來進行計算，而其他大部分專家則保持不活動狀態。
    

**MoE 與 LLM 的關係：**

MoE 不是一個獨立於 LLM 的東西，而是**一種用於構建 LLM 的先進架構**。它深刻地影響了現代 LLM 的設計和能力。

- **解決規模與成本的矛盾：** LLM 的性能通常與其參數規模成正比，但更大的模型意味著更高的訓練和推理成本。MoE 架構允許模型在總參數數量上達到非常巨大的規模（例如數萬億個參數），但在處理單個輸入時，只動用其中一小部分參數。這就實現了**在不顯著增加計算成本的情況下，大幅擴展模型規模**的效果。
    
- **提升效率和速度：** 由於每次推理都只啟動了少數專家，MoE 模型的訓練和推理速度可以比同等參數規模的密集模型快得多。
    
- **增強模型容量和專業化：** 擁有更多的專家意味著模型有更大的「容量」去學習更多樣化和更複雜的知識。不同的專家可以發展出各自的「專長」，使得模型在處理不同領域的任務時都能夠表現得更好。
    
- **現代前沿 LLM 的標誌：** 許多當前最先進的開源和閉源大型語言模型，如 Google 的 Gemini 系列、Mistral AI 的 Mixtral 8x7B 等，都明確採用了 MoE 架構，這也是它們能夠在性能和效率上取得突破的關鍵原因之一。
    

總結來說，**MoE 是一種讓 LLM 變得更大、更快、更強，同時又更高效的關鍵架構創新。** 它不是與 LLM 並列的概念，而是實現高性能 LLM 的一種核心技術路徑。

### 綜合總結

| 概念           | 核心定義                                   | 與 LLM 的關係                                             | 比喻                                                                  |
| ------------ | -------------------------------------- | ----------------------------------------------------- | ------------------------------------------------------------------- |
| **AI Agent** | 能夠自主感知、決策、並執行任務的智能體。                   | **LLM 是 Agent 的大腦**，Agent 是 LLM 能力的延伸和執行體，實現從「說」到「做」。 | LLM 是 CEO，AI Agent 是整個公司團隊，負責將 CEO 的決策執行落地。                         |
| **MoE**      | 一種高效的模型架構，由多個「專家」網路和一個「門控」網路組成，實現稀疏啟動。 | **MoE 是構建 LLM 的一種先進方式**，讓 LLM 在規模、性能和效率上實現飛躍。         | 一個龐大的密集模型像是全公司員工開全員大會解決所有問題；MoE 模型則像是總機，根據問題類型，只呼叫最相關的幾個部門專家來開小會解決。 |