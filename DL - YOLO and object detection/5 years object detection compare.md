


# 近五年常用物件偵測模型分析：YOLOv8、YOLOv8n、YOLO11、LW-DETR、RF-DETR、D-FINE 與 DETR

**1. 前言**

物件偵測是電腦視覺領域中的一項核心任務，旨在識別並定位影像或影片中的特定物件。這項技術在眾多應用中扮演著至關重要的角色，例如自動駕駛、安全監控、機器人技術以及智慧製造等。隨著深度學習技術的發展，物件偵測模型的效能取得了顯著的進步。本報告將深入探討近五年來常用的幾種物件偵測模型，包括 YOLOv8、YOLOv8n、YOLO11、LW-DETR、RF-DETR、D-FINE 和 DETR。報告將詳細解釋這些模型的架構、關鍵創新之處，並整理它們在 MS COCO 資料集上的效能表現，重點關注延遲 (latency) 和 mAP 50:95 這兩個指標。這些模型的出現和發展，代表了在追求更快速、更精確、更有效率的物件偵測解決方案上的持續努力。YOLO 系列的演進以及基於 Transformer 架構的 DETR 模型的興起，共同塑造了當前物件偵測技術的格局。MS COCO 資料集作為一個廣泛使用的評估基準，其上的效能表現能夠有效地衡量不同模型的優劣 。延遲反映了模型處理影像的速度，對於需要即時反應的應用至關重要；而 mAP 50:95 則是在不同 IoU (Intersection over Union) 閾值下的平均精確度均值，能夠更全面地評估模型的偵測品質。  

這些模型的多樣性，包括基於卷積神經網路 (CNN) 的 YOLO 變體和基於 Transformer 的 DETR 變體，反映了該領域研究的活力，不斷探索不同的架構範式以優化速度、準確性和效率。使用者對於同時包含 YOLO 和 DETR 模型的查詢，暗示了其希望理解當前技術發展狀況，其中既有成熟的 CNN 架構，也有新興的 Transformer 架構。輕量化版本 (YOLOv8n、LW-DETR) 和注重效能的版本 (RF-DETR、D-FINE) 的存在，表明了針對不同應用需求的專業化解決方案的發展趨勢。對於延遲的關注，強調了這些模型在即時應用中的實用重要性，而 mAP 50:95 則反映了在不同 IoU 閾值下的準確度，提供了對偵測品質的全面衡量。

**2. YOLOv8 和 YOLOv8n**

**2.1 詳細架構概述**

YOLOv8 的架構主要由三個部分組成：主幹網路 (Backbone)、頸部網路 (Neck) 和頭部網路 (Head) 。  

**主幹網路 (Backbone):** 主幹網路又稱為特徵提取器，負責從輸入影像中提取有意義的特徵。在初始層中，它捕捉簡單的模式，如邊緣和紋理，並隨著網路處理影像，在多個尺度上建立特徵的階層式表示 。YOLOv8 採用客製化的 CSPDarknet53 作為主幹網路 。CSPDarknet53 利用跨階段局部連接 (cross-stage partial connections) 來改善層之間的資訊流動，並提高偵測準確度 。對於 YOLOv8n 而言，其架構遵循相同的原則，但使用了較小的深度和寬度倍數 (depth and width multiples)，從而減少了參數數量並加快了推論速度 。在 YOLOv8n 中，卷積區塊的輸出通道以及 C2f 區塊中使用的瓶頸區塊數量，會根據 `width_multiple` 和 `depth_multiple` 的數值進行縮減 。CSPDarknet53 的使用表明其優化了特徵提取的效率，延續了先前 YOLO 版本在架構上的成功經驗。YOLOv8n 的縮減則突顯了其針對資源受限環境的設計考量。CSPNet 旨在提高 CNN 的學習能力，同時降低計算成本，這也是 YOLOv8 採用它的原因。YOLOv8n 的存在，直接滿足了邊緣設備或具有嚴格延遲要求的應用對於輕量化模型的需求。  

**頸部網路 (Neck):** 頸部網路作為主幹網路和頭部網路之間的橋樑，執行特徵融合和整合上下文資訊 。它合併來自主幹網路不同階段的特徵圖，以幫助網路偵測不同尺寸的物件，並整合上下文資訊以提高偵測準確度 。YOLOv8 使用了新型的 C2f 模組來取代傳統的特徵金字塔網路 (Feature Pyramid Network, FPN) 。C2f 模組有效地結合了高層語義特徵和低層空間資訊，顯著提升了偵測效能，尤其是在小型物件的偵測方面 。此外，YOLOv8 中也整合了路徑聚合網路 (Path Aggregation Network, PANet)，這有助於跨不同空間解析度的資訊流動，從而改善多尺度物件的偵測 。對於 YOLOv8n，頸部網路的結構也可能與主幹網路成比例地縮減。從 FPN 轉向 C2f 並加入 PANet，顯示其重點在於提升模型偵測各種尺寸物件的能力，並提高準確性，特別是對於通常較難偵測的小型物件。FPN 是物件偵測中常用的處理多尺度物件的機制。C2f 作為其替代方案，暗示其在效率或效能方面具有優勢，尤其是在小型物件上。PANet 進一步增強了這種多尺度特徵融合的過程。  

**頭部網路 (Head):** 頭部網路是網路的最後一部分，負責產生輸出，例如物件偵測的邊界框和置信度分數 。它產生潛在物件的邊界框，分配置信度分數以指示物件存在的可能性，並按類別對偵測到的物件進行排序 。YOLOv8 在頭部網路中使用了多個偵測模組 。YOLOv8 採用了解耦頭 (decoupled head) 架構，這表示它將分類和回歸任務分開執行，這樣可以提高模型效能 。YOLOv8 還採用了無錨框 (anchor-free) 的偵測方法，直接預測物件的中心點，這有助於提高泛化能力並減少邊界框預測的數量 。對於 YOLOv8n，頭部網路也會進行簡化以加快處理速度。轉向解耦和無錨框的頭部網路，代表了相較於早期 YOLO 版本的一個重大架構變更，旨在簡化模型並提高準確性和泛化能力。錨框雖然有用，但可能會引入複雜性，並且對於所有資料集而言可能不是最佳的。無錨框偵測簡化了預測過程。將分類和回歸頭部分開，可以針對每個任務進行獨立的優化。  

**2.2 主要創新之處和與先前 YOLO 版本的差異**

YOLOv8 相較於先前的 YOLO 版本引入了多項關鍵創新和差異 ：  

- **無錨框偵測 (Anchor-Free Detection):** YOLOv8 不再像早期版本那樣依賴預定義的錨框，而是採用無錨框的偵測方法 。這種方法簡化了模型，並能透過直接預測物件的中心點來改善對自訂資料集的泛化能力 。相較於基於錨框的方法，無錨框偵測可以提高泛化能力，並在自訂資料集上實現更快的學習速度 。  
    
- **C2f 模組:** YOLOv8 的主幹網路現在採用了 C2f 模組，取代了先前版本中使用的 C3 模組 。C2f 模組串聯了所有瓶頸模組的輸出，相較於僅使用最後一個瓶頸模組輸出的 C3，這加快了訓練速度並增強了梯度流動 。  
    
- **解耦頭 (Decoupled Head):** 在 YOLOv8 中，偵測頭是解耦的，這表示它分別執行分類和回歸任務 。先前的版本通常以耦合的方式一起執行這些任務。這種解耦有助於提高模型效能 。  
    
- **修改後的損失函數 (Modified Loss Function):** 為了處理解耦頭可能導致的對齊問題 (模型可能定位了一個物件但分類為另一個)，YOLOv8 引入了任務對齊分數 (task alignment score) 。這個分數基於交並比 (Intersection over Union, IoU) 分數和分類分數，有助於模型區分正樣本和負樣本。損失函數使用二元交叉熵 (Binary Cross-entropy, BCE) 進行分類，並結合了完整交並比 (Complete IoU, CIoU) 和分佈式焦點損失 (Distributional Focal Loss, DFL) 進行回歸 。  
    
- **馬賽克資料增強 (Mosaic Data Augmentation) 的變更:** 與 YOLOv4 類似，YOLOv8 採用了馬賽克資料增強，將四個不同的訓練影像混合在一起，以提供更好的上下文資訊 ；然而，YOLOv8 的一個關鍵差異是，這種增強在最後十個訓練週期中會被關閉，以進一步提高效能 。  
    
- **Python 套件和 CLI 實作:** YOLOv8 提供了使用者友善的 Python 套件和基於命令列介面 (Command Line Interface, CLI) 的實作，相較於一些先前的版本，更容易使用和開發 。  
    
- 此外，YOLOv8 系列也包含其他改進，例如基於 EfficientNet 的更新主幹網路、新的特徵融合模組以及增強的資料增強技術 (MixUp, CutMix) 。  
    

總而言之，YOLOv8 專注於架構上的改進，如無錨框偵測和解耦頭，以及精煉的損失函數和馬賽克資料增強策略的調整，以實現比其前身更高的準確度和更快的速度。YOLOv8 代表了顯著的演進，它吸取了先前版本的經驗，並採用了更先進的架構組件和訓練策略，以突破即時物件偵測的速度和準確性的界限。

**2.3 MS COCO 效能分析 (延遲與 mAP 50:95)**

下表列出了 YOLOv8n 和其他 YOLOv8 變體 (s, m, l, x) 在 MS COCO 資料集上的延遲 (Speed CPU ONNX (ms) 和 Speed A100 TensorRT (ms)) 和 mAP 50:95 (mean Average Precision val50-95) ：  

|   |   |   |   |   |
|---|---|---|---|---|
|**模型**|**尺寸 (像素)**|**mAP<sup>val50-95</sup>**|**CPU ONNX 延遲 (毫秒)**|**A100 TensorRT 延遲 (毫秒)**|
|YOLOv8n|640|37.3|80.4|0.99|
|YOLOv8s|640|44.9|128.4|1.20|
|YOLOv8m|640|50.2|234.7|1.83|
|YOLOv8l|640|52.9|375.2|2.39|
|YOLOv8x|640|53.9|479.1|3.53|

這些指標基於在 COCO 資料集上訓練的模型，該資料集包含 80 個預訓練類別。效能指標清楚地說明了 YOLOv8 系列的可擴展性，提供了針對各種需求優化的不同模型，從 YOLOv8n 的超快速推論到較大型模型更高的準確性。

從表中可以看出，模型尺寸/複雜度 (n < s < m < l < x) 與延遲 (通常隨著尺寸增加而增加) 和 mAP 50:95 (通常隨著尺寸增加而增加) 之間存在權衡關係。YOLOv8n 的設計目標是速度和效率，它實現了較低的 mAP，但也具有顯著較低的延遲，使其適用於資源受限或對即時性要求嚴格的應用。

**3. YOLO11**

**3.1 背景與 YOLO 系列的關係**

YOLO11 是 Ultralytics YOLO 系列的最新迭代版本，建立在先前版本的基礎上，旨在以尖端的準確性、速度和效率重新定義可能性 。它繼承了先前版本的優勢並進一步提升 。根據相關資訊，YOLO11 的發布時間約在 2025 年 3 月 ，晚於 YOLOv8。YOLO11 的出現延續了 YOLO 系列在即時物件偵測能力上的持續改進趨勢。  

**3.2 主要特點與架構增強**

YOLO11 具有多項主要特點和架構增強 ：  

- **高速與高效率:** YOLO11 擁有精煉的架構，可實現更快的處理速度，並優化了訓練流程，以高效處理任務而不犧牲準確性。
- **改進的特徵表示:** 該模型採用了增強的主幹網路以提取更豐富的特徵，並重新設計了頸部網路架構，以更好地進行物件偵測和處理複雜的視覺任務。
- **更高的準確性與更低的複雜度:** YOLO11m 在 MS COCO 資料集上實現了更高的平均精確度 (mAP)，同時使用的參數比 YOLOv8m 少 22%，使其更輕量化且更有效率。
- **廣泛的任務支援:** YOLO11 不僅限於物件偵測，還能處理多種電腦視覺任務，包括實例分割、影像分類、姿態估計、定向物件偵測和多物件追蹤。
- **多功能部署:** YOLO11 可以部署在邊緣設備上，支援雲端平台，並與 NVIDIA GPU 相容。

YOLO11 看似著重於在準確性和效率之間取得更好的平衡，提供多任務能力和廣泛的部署選項，使其成為各種應用的多功能選擇。相較於 YOLOv8m，YOLO11m 在準確性更高且參數更少，直接表明其效率有所提升。

**3.3 MS COCO 效能分析 (延遲與 mAP 50:95)**

下表列出了不同 YOLO11 模型 (n, s, m, l, x) 在 MS COCO 資料集上進行物件偵測的延遲 (Speed CPU ONNX 和 Speed T4 TensorRT10) 和 mAP val50-95 ：  

**物件偵測模型:**

|   |   |   |   |   |
|---|---|---|---|---|
|**模型**|**尺寸 (像素)**|**mAP<sup>val50-95</sup>**|**CPU ONNX 延遲 (毫秒)**|**T4 TensorRT10 延遲 (毫秒)**|
|YOLO11n|640|39.5|56.1 ± 0.8|1.5 ± 0.0|
|YOLO11s|640|47.0|90.0 ± 1.2|2.5 ± 0.0|
|YOLO11m|640|51.5|183.2 ± 2.0|4.7 ± 0.1|
|YOLO11l|640|53.4|238.6 ± 1.4|6.2 ± 0.1|
|YOLO11x|640|54.7|462.8 ± 6.7|11.3 ± 0.2|

匯出到試算表

**分割模型:**

|   |   |   |   |   |   |
|---|---|---|---|---|---|
|**模型**|**尺寸 (像素)**|**mAP<sup>box50-95</sup>**|**mAP<sup>mask50-95</sup>**|**CPU ONNX 延遲 (毫秒)**|**T4 TensorRT10 延遲 (毫秒)**|
|YOLO11n-seg|640|38.9|32.0|65.9 ± 1.1|1.8 ± 0.0|
|YOLO11s-seg|640|46.6|37.8|117.6 ± 4.9|2.9 ± 0.0|
|YOLO11m-seg|640|51.5|41.5|281.6 ± 1.2|6.3 ± 0.1|
|YOLO11l-seg|640|53.4|42.9|344.2 ± 3.2|7.8 ± 0.2|
|YOLO11x-seg|640|54.7|43.8|664.5 ± 3.2|15.8 ± 0.7|

匯出到試算表

延遲以毫秒 (ms) 為單位提供，適用於 CPU (ONNX) 和 GPU (T4 TensorRT10) 環境。mAP 值則是在 COCO val2017 資料集上報告的。各種 YOLO11 模型的存在，包括分割模型，使得使用者可以根據其特定的任務需求和計算資源選擇最適合的模型。效能指標證實了不同模型尺寸之間速度和準確性之間的預期權衡。YOLO11n 作為輕量級選項，具有競爭性的速度和不錯的 mAP。較大的 YOLO11 模型 (如 x) 則以增加延遲為代價實現了更高的 mAP。

**4. LW-DETR (Lightweight DETR)**

**4.1 架構說明**

LW-DETR 的架構主要由三個部分組成：ViT 編碼器、投影層和淺層 DETR 解碼器 。  

**編碼器:** LW-DETR 使用純粹的視覺轉換器 (Vision Transformer, ViT) 作為其編碼器 。為了降低計算複雜度，一些轉換器編碼器層採用視窗自注意力 (window self-attention) 而非全域自注意力 。該架構還整合了多層級特徵圖聚合 (multi-level feature map aggregation)，結合編碼器中的中間和最終特徵圖，以創建更豐富的編碼特徵 。  

**投影層:** 卷積投影層連接編碼器和解碼器 。對於微小、小型和中型版本，使用單尺度投影層，而較大型版本則採用多尺度投影層，輸出兩種不同尺度的特徵圖 (1/8 和 1/32) 。投影層使用 C2f 區塊實現，C2f 區塊是跨階段局部密集網路 (cross-stage partial DenseNet) 的擴展 。  

**解碼器:** 解碼器僅由三層轉換器解碼器層堆疊而成，相較於標準 DETR 中典型的六層，顯得較為淺層 。它使用可變形交叉注意力 (deformable cross-attention) 以提高計算效率 。物件查詢 (object queries) 的形成採用混合查詢選擇方案 (mixed-query selection scheme)，結合了可學習的內容查詢 (learnable content queries) 和從投影層預測的前 K 個特徵衍生出的空間查詢 (spatial queries) 。LW-DETR 的架構代表著為實現即時效能而對 DETR 框架進行刻意的調整，其方式是簡化解碼器並優化編碼器的注意力機制。  

**4.2 實現輕量化設計的策略**

LW-DETR 採用了多種策略來實現輕量化設計 ：  

- **淺層解碼器 (Shallow Decoder):** 僅使用三層解碼器層，顯著降低了相較於標準 DETR 六層解碼器的計算成本。
- **交錯視窗和全域注意力 (Interleaved Window and Global Attention):** 在 ViT 編碼器中，將一些全域自注意力層替換為視窗自注意力，降低了與全域注意力相關的二次時間複雜度。
- **視窗主導的特徵圖組織 (Window-Major Feature Map Organization):** 這種針對交錯注意力的高效實現方式減少了對昂貴的記憶體置換操作的需求，進一步提高了推論速度。
- **多層級特徵聚合 (Multi-level Feature Aggregation):** 雖然有助於豐富特徵，但論文強調了整體架構的效率。
- **卷積投影層 (Convolutional Projector):** 使用卷積投影層提供了一種在 ViT 編碼器和 DETR 解碼器之間建立橋樑的有效方式。

這些設計選擇的結合使得 LW-DETR 能夠實現優於 YOLO 的即時物件偵測效能，同時保持輕量化的結構。LW-DETR 的輕量化設計是透過對標準 DETR 架構進行有針對性的修改來實現的，重點在於降低轉換器層的計算成本，使其成為更適合即時應用的選擇。

**4.3 MS COCO 效能分析 (延遲與 mAP 50:95)**

下表列出了不同 LW-DETR 實例 (tiny, small, medium, large, xlarge) 在 MS COCO 資料集上的 mAP 值 ：  

|   |   |
|---|---|
|**模型**|**mAP**|
|LW-DETR-tiny|42.6|
|LW-DETR-small|48.0|
|LW-DETR-medium|52.5|
|LW-DETR-large|56.1|
|LW-DETR-xlarge|58.3|

匯出到試算表

雖然相關資訊並未直接提供具體的延遲數據，但相關論文 和其他研究 強調 LW-DETR 在即時物件偵測方面優於 YOLO，這暗示其相較於標準 DETR 模型具有更低的延遲，並且與 YOLO 模型具有競爭性的延遲。RF-DETR 的效能評估 採用了 LW-DETR 的「總延遲」(Total Latency) 概念，其中包括非極大值抑制 (Non-Maximum Suppression, NMS) 的後處理時間，這表明 LW-DETR 的延遲是以一種全面的方式來考量的。LW-DETR 在準確性和速度之間取得了良好的平衡，在 mAP 方面優於 YOLO 模型，同時保持了即時效能。「總延遲」的概念突顯了對實際端到端效能的關注。  

**5. RF-DETR (Region Focusing DETR)**

**5.1 架構說明**

RF-DETR 是一種基於 Transformer 架構的即時物件偵測模型 。其架構建立在 Deformable DETR 論文提出的基礎之上 。與使用多尺度自注意力的 Deformable DETR 不同，RF-DETR 從單尺度主幹網路提取影像特徵圖 。RF-DETR 架構的關鍵在於結合了 LW-DETR 和預訓練的 DINOv2 主幹網路 。DINOv2 是一個強大的模型，透過自監督學習學會了視覺特徵，這使得 RF-DETR 能夠很好地適應新的視覺領域 。該模型還在多個解析度下進行訓練，這允許在運行時在準確性和延遲之間進行權衡，而無需重新訓練模型 。RF-DETR 提供兩種變體：RF-DETR Base 和 RF-DETR Large，它們具有不同的參數數量 。RF-DETR 透過結合 LW-DETR 的效率和預訓練 DINOv2 主幹網路的強大特徵表示能力，實現了高準確性和即時效能，並著重於領域適應性。  

**5.2 區域關注機制以提升偵測效果**

提供的資訊並未明確詳細說明 RF-DETR 中如同字面意義上的「區域關注機制」(region focusing mechanism)，例如一個名為「區域關注」的特定模組。然而，該架構透過 Transformer 架構固有的注意力機制 以及預訓練的 DINOv2 主幹網路 (後者已學習識別顯著的視覺特徵) 間接地關注相關區域。Transformer 的自注意力機制允許模型捕捉影像中元素之間複雜的關係和依賴性，從而有效地關注相關部分 。在大型資料集 (如 DINOv2 使用的資料集) 上進行預訓練，有助於模型學習通用的視覺特徵，這些特徵可能對於跨不同領域的物件偵測非常重要 。雖然 RF-DETR 沒有一個明確的「區域關注模組」，但它利用了 Transformer 架構固有的注意力機制以及其主幹網路的預訓練知識，有效地關注影像中的相關區域以實現準確的物件偵測。  

**5.3 MS COCO 效能分析 (延遲與 mAP 50:95)**

RF-DETR 是第一個在 COCO 資料集上實現超過 60% mAP 的即時模型 。RF-DETR-large 在 728 的輸入解析度下實現了 60.5 的 mAP 。對於 RF-DETR Base，其 mAP 為 53.3，在 T4 GPU 上的延遲為 6.0 毫秒 ，值得注意的是，該延遲包括了 NMS 時間 (遵循 LW-DETR 的「總延遲」方法) 。相較於其他即時模型 (如 YOLOv8 和 YOLO11)，RF-DETR 在保持具競爭力的延遲的同時，展現出更高的 mAP 。RF-DETR 在 COCO 資料集上的即時物件偵測方面樹立了新的標竿，相較於先前的即時模型，其準確性顯著提高，同時保持了實用的推論速度。  

**6. D-FINE**

**6.1 架構說明**

D-FINE 是一種即時物件偵測器，它重新定義了 DETR (Detection Transformers) 模型中的邊界框回歸任務 。D-FINE 的架構包含兩個關鍵組件 ：  

- **細粒度分佈精煉 (Fine-grained Distribution Refinement, FDR):** FDR 將回歸過程從預測固定座標轉變為迭代地精煉邊界框邊緣的機率分佈。這提供了一種更細粒度的中間表示，顯著提高了定位準確性 。模型採用非均勻加權函數以實現更精細的定位 。  
    
- **全域最佳定位自我蒸餾 (Global Optimal Localization Self-Distillation, GO-LSD):** GO-LSD 是一種雙向優化策略，它透過自我蒸餾將更深層的精煉分佈中的定位知識傳遞到較淺層。這也簡化了更深層的殘差預測任務 。  
    

值得注意的是，D-FINE 在計算密集型模組中加入了輕量級的優化，以在速度和準確性之間取得更好的平衡 。D-FINE 的架構著重於透過一種新穎的機率回歸方法和自我蒸餾機制，提高 DETR 框架內邊界框預測的準確性，且不會顯著增加計算負擔。  

**6.2 核心技術概念與設計理念**

FDR 背後的核心技術概念包括將邊界框邊緣視為機率分佈，並透過解碼器層迭代地精煉這些分佈 。GO-LSD 的設計理念旨在透過從較後層更精煉的預測中提取知識，改善網路所有層的定位資訊學習 。D-FINE 是無錨框的，並且與 DETR 等無錨框架構無縫整合，減少了對預定義錨框的依賴 。其目標是在不增加額外推論和訓練成本的情況下，實現卓越的定位精度 。D-FINE 的設計理念以提高定位精度為中心，這是物件偵測的關鍵方面，它透過擺脫固定的邊界框預測並利用自我監督來提高學習效率。  

**6.3 MS COCO 效能分析 (延遲與 mAP 50:95)**

D-FINE-L 在 COCO 資料集上實現了 54.0% 的 AP，速度為 124 FPS，而 D-FINE-X 則實現了 55.8% 的 AP，速度為 78 FPS (均在 NVIDIA T4 GPU 上測得) 。當 D-FINE 在 Objects365 等更大的資料集上進行預訓練後，效能會進一步提升 (D-FINE-L 達到 57.1% AP，D-FINE-X 達到 59.3% AP) 。值得注意的是，當在 Objects365 上進行預訓練後，D-FINE 超越了所有現有的即時偵測器 。此外，D-FINE 還能在幾乎不增加額外參數和訓練成本的情況下，將各種 DETR 模型的效能提升高達 5.3% AP 。D-FINE 在 MS COCO 資料集上的即時物件偵測方面展現了最先進的效能，尤其是在與更大的資料集進行預訓練後，其有效性和提升其他基於 DETR 模型潛力的能力得以展現。  

**7. DETR (DEtection TRansformer)**

**7.1 架構說明**

DETR 是一種新穎的物件偵測框架，它是第一個成功將 Transformer 作為偵測流程核心組件的模型 。與依賴於手工設計組件 (如錨框生成和非極大值抑制 (NMS)) 的傳統物件偵測方法不同，DETR 將物件偵測視為一個直接的集合預測問題 。  

DETR 架構的主要組件包括 ：  

- **CNN 主幹網路:** DETR 首先使用傳統的 CNN 主幹網路 (如 ResNet) 從輸入影像中提取緊湊的特徵表示 。這個主幹網路接收輸入影像並產生一個較低解析度的激活圖。  
    
- **Transformer 編碼器:** 一個 1x1 的卷積層降低了來自 CNN 的特徵圖的通道維度。然後，這個特徵圖的空間維度被展平成一個序列。這個序列被輸入到 Transformer 編碼器中。編碼器由多個多頭自注意力 (multi-head self-attention) 層和前饋網路 (feed-forward networks, FFNs) 組成。編碼器中的自注意力機制允許模型顯式地建模序列中元素之間的所有成對交互 (即特徵圖中不同的空間位置)，從而實現對影像上下文的全域推理。由於 Transformer 架構是置換不變的，因此將位置編碼 (positional encodings) 添加到編碼器的輸入中，這些編碼提供了關於特徵空間位置的資訊。
- **Transformer 解碼器:** 解碼器接收一組固定的、少量可學習的位置嵌入 (稱為物件查詢) 作為輸入。它還會關注編碼器的輸出。與編碼器類似，解碼器使用多個多頭自注意力層和編碼器-解碼器注意力 (encoder-decoder attention) 層。解碼器中的自注意力允許模型推理不同物件查詢之間的關係。編碼器-解碼器注意力允許每個物件查詢關注編碼後的影像特徵的相關部分。解碼器輸出一組嵌入，每個嵌入代表一個潛在的偵測到的物件。重要的是，DETR 在每個解碼器層並行解碼 N 個物件，這與原始 Transformer 用於機器翻譯等任務的自回歸解碼不同。
- **預測前饋網路 (Prediction Feed-Forward Networks, FFNs):** 來自解碼器的每個輸出嵌入都通過一個共享的前饋網路 (FFN)。這個 FFN 預測相對於輸入影像的邊界框的歸一化中心座標、高度和寬度。它還使用 softmax 函數預測類別標籤。由於 DETR 預測固定大小的 N 個邊界框 (其中 N 大於典型的物件數量)，因此使用一個額外的特殊類別標籤「無物件」(∅) 來表示在特定的查詢槽中沒有偵測到物件。

**7.2 基於 Transformer 的物件偵測方法的基本原理**

DETR 的核心創新在於其使用 Transformer 架構進行物件偵測。這種方法的主要優勢在於 ：  

- **直接集合預測 (Direct Set Prediction):** DETR 並行地直接輸出物件預測的集合，消除了對基於候選框或錨框的間接方法的需求。
- **全域推理 (Global Reasoning):** Transformer 編碼器和解碼器中的自注意力機制允許模型推理影像中所有物件之間的關係以及全域影像上下文。這對於處理遮擋和理解場景特別有益。
- **簡化的流程 (Simplified Pipeline):** 透過移除手工設計的組件 (如 NMS 和錨框生成)，DETR 簡化了偵測流程，使其在概念上更簡單且更容易實現。
- **置換不變性 (Permutation Invariance):** 基於集合的損失函數與二分匹配相結合，確保訓練期間預測的順序無關緊要。

DETR 的基本原理提供了一種更簡化且概念上更直接的物件偵測方法，它利用了 Transformer 架構在全域上下文理解和並行處理方面的優勢。

**7.3 MS COCO 效能分析 (延遲與 mAP 50:95)**

DETR 使用 ResNet-50 主幹網路在 COCO 資料集上實現了 **42 AP** 。使用 ResNet-50-DC5 主幹網路，它實現了 **43.3 AP**。使用 ResNet-101 主幹網路，DETR 實現了 **43.5 AP**，而使用 ResNet-101-DC5 主幹網路，則達到了 **44.9 AP** 。這些結果基於 COCO 2017 val5k 資料集。雖然提供的資訊中沒有以表格形式明確列出原始 DETR 的具體延遲數據，但值得注意的是，儘管 DETR 是一種開創性的方法，但其初始版本通常比優化的基於 CNN 的模型 (如 YOLO) 更慢，並且需要更長的訓練時間，正如 LW-DETR 和 RF-DETR 等旨在實現即時效能的模型的出現所表明的那樣 。隨後的基於 DETR 的模型 (如 Deformable DETR、RT-DETR、LW-DETR、RF-DETR、D-FINE) 則專注於提高準確性和效率 (延遲) 。原始 DETR 證明了 Transformer 在物件偵測方面的潛力，在準確性方面與已建立的基於 CNN 的模型相當，但在速度和訓練時間方面存在不同的權衡，後續研究旨在解決這些問題。  

**8. 比較分析與討論**

**8.1 架構特點比較**

下表總結了所有七個模型的關鍵架構特點：

|   |   |   |   |   |
|---|---|---|---|---|
|**模型**|**主幹網路類型**|**使用 Transformer**|**無錨框**|**主要架構元素**|
|YOLOv8|CSPDarknet53|否|是|C2f 模組, 解耦頭, PANet 整合|
|YOLOv8n|CSPDarknet53|否|是|C2f 模組, 解耦頭, PANet 整合 (縮小版本)|
|YOLO11|未明確說明|否|是|增強的主幹網路, 重新設計的頸部網路|
|LW-DETR|ViT|是|是|淺層解碼器, 交錯視窗與全域注意力, 卷積投影層|
|RF-DETR|DINOv2|是|是|基於 LW-DETR, 單尺度主幹網路|
|D-FINE|DETR 相容|是|是|細粒度分佈精煉 (FDR), 全域最佳定位自我蒸餾 (GO-LSD)|
|DETR|CNN (ResNet)|是|是|Transformer 編碼器與解碼器, 物件查詢|

匯出到試算表

**8.2 創新之處與主要差異**

|   |   |
|---|---|
|**模型**|**主要創新之處與差異**|
|YOLOv8|無錨框偵測, C2f 模組, 解耦頭, 修改後的損失函數, 馬賽克資料增強的變更|
|YOLOv8n|YOLOv8 的縮小版本，更注重速度和效率|
|YOLO11|更高的準確性與更低的複雜度 (相較於 YOLOv8m), 廣泛的任務支援, 多功能部署|
|LW-DETR|專為即時效能設計的輕量級 DETR 變體, 淺層解碼器, 高效的注意力機制|
|RF-DETR|結合 LW-DETR 和預訓練的 DINOv2 主幹網路，著重於領域適應性和高準確性|
|D-FINE|重新定義 DETR 中的邊界框回歸任務，採用細粒度分佈精煉 (FDR) 和全域最佳定位自我蒸餾 (GO-LSD)，在不增加額外成本的情況下提高定位精度|
|DETR|首個成功將 Transformer 作為物件偵測核心組件的模型，採用直接集合預測、全域推理，並消除了對手工設計組件的需求|

匯出到試算表

**8.3 MS COCO 效能比較 (延遲與 mAP 50:95)**

|   |   |   |
|---|---|---|
|**模型**|**延遲 (T4 GPU TensorRT10) (毫秒)**|**mAP val50-95**|
|YOLOv8n|0.99|37.3|
|YOLOv8s|1.20|44.9|
|YOLOv8m|1.83|50.2|
|YOLOv8l|2.39|52.9|
|YOLOv8x|3.53|53.9|
|YOLO11n|1.5 ± 0.0|39.5|
|YOLO11s|2.5 ± 0.0|47.0|
|YOLO11m|4.7 ± 0.1|51.5|
|YOLO11l|6.2 ± 0.1|53.4|
|YOLO11x|11.3 ± 0.2|54.7|
|LW-DETR-tiny|資訊未提供|42.6|
|LW-DETR-small|資訊未提供|48.0|
|LW-DETR-medium|資訊未提供|52.5|
|LW-DETR-large|資訊未提供|56.1|
|LW-DETR-xlarge|資訊未提供|58.3|
|RF-DETR Base|6.0 (包含 NMS)|53.3|
|RF-DETR Large|資訊未提供|60.5|
|D-FINE-L|~8.1 (124 FPS)|54.0|
|D-FINE-X|~12.8 (78 FPS)|55.8|
|DETR (R50)|資訊未提供|42.0|
|DETR (R101)|資訊未提供|43.5|

匯出到試算表

**8.4 效能權衡討論**

從效能表中可以看出，YOLO 系列通常優先考慮速度，而一些基於 DETR 的模型則旨在實現更高的準確性。模型複雜度 (例如，參數數量)、延遲和 mAP 之間存在明顯的權衡關係。例如，YOLOv8n 和 LW-DETR 由於其較小的尺寸和更快的速度，更適合邊緣設備等資源受限的環境。另一方面，RF-DETR 和 D-FINE 則在保持即時效能的同時，實現了更高的準確性，使其適用於需要高精度的應用。YOLO11 則在準確性和效率之間取得了良好的平衡，並提供了廣泛的任務支援。原始的 DETR 模型雖然在概念上具有突破性，但在初始版本中，其速度和訓練時間相較於優化的 CNN 模型可能不具優勢，但它為後續基於 Transformer 的物件偵測模型的研究奠定了基礎。

**9. 結論**
本報告詳細分析了近五年常用的七種物件偵測模型。

YOLOv8 及其變體 YOLOv8n 在速度和準確性方面都取得了顯著的進步，並透過無錨框偵測和解耦頭等創新架構，持續引領即時物件偵測的發展。

YOLO11 作為最新的 YOLO 系列成員，進一步提升了準確性和效率，並擴展了任務支援範圍。

LW-DETR 和 RF-DETR 則代表了基於 Transformer 架構的輕量化和高效能解決方案，LW-DETR 透過簡化架構實現即時效能，而 RF-DETR 則結合預訓練模型實現了卓越的準確性和領域適應性。

D-FINE 透過重新定義邊界框回歸任務，在不增加額外成本的情況下顯著提高了定位精度。

最初的 DETR 模型雖然在速度上有所不足，但其基於 Transformer 的架構為後續的研究開闢了新的方向。總體而言，這些模型展現了電腦視覺領域在物件偵測技術上的持續進步，不斷追求更高的速度、更強的準確性和更廣泛的應用能力。未來的研究方向可能包括進一步優化模型以適應邊緣設備、提高對小型物件的偵測準確性以及發展更強大的開放詞彙物件偵測技術。