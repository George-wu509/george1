

|                       |     |
| --------------------- | --- |
| [[###1. 目標檢測模型訓練流程]]  |     |
| [[###2. 資料集準備]]       |     |
|                       |     |
| [[###YOLO馬賽克增強跟混合增強]] |     |
|                       |     |


### 1. 目標檢測模型訓練流程

- **資料集準備：**
    - 收集大量的車牌圖像資料，並進行標註。標註內容包括車牌的邊界框位置。
    - 將資料集分為訓練集、驗證集和測試集。
    - 車牌影像的品質與多樣性是影響模型檢測精準度的重要關鍵。
- **模型選擇與權重載入：**
    - 選擇適合您需求的目標檢測模型，如您提及的 RCNN、YOLO、FCOS、SSD、RetinaNet、EfficientDet、MobileNet-SSD、DETR、RT-DETR 或 RF-DETR。
    - 從官方來源或預訓練模型庫下載預訓練的權重檔案。
    - 將預訓練權重載入到模型中，作為初始權重。
- **資料增強：**
    - 使用各種資料增強技術，如旋轉、縮放、翻轉、裁剪、顏色調整等，增加資料集的多樣性，提高模型的泛化能力。
- **模型微調（Fine-tuning）：**
    - 使用您的車牌資料集對預訓練模型進行微調。
    - 調整模型的最後幾層（通常是與目標檢測相關的層），使其適應車牌檢測任務。
    - 設定訓練參數，如學習率、批次大小、迭代次數等。
    - 使用優化器（如 Adam 或 SGD）訓練模型。
    - 在驗證集上監控模型的性能，調整訓練參數以獲得最佳效果。
- **模型評估：**
    - 使用測試集評估模型的性能，常用指標包括 mAP（平均精度均值）、Precision（精確度）、Recall（召回率）等。
- **模型部署：**
    - 將訓練好的模型部署到實際應用中，進行車牌檢測。

**2. 是否需要重新訓練？**

- **需要：**
    - 絕大多數的目標檢測模型，包括您提及的 RCNN、YOLO、FCOS、SSD、RetinaNet、EfficientDet、MobileNet-SSD、DETR、RT-DETR 和 RF-DETR，都需要使用特定的車牌資料集進行重新訓練或微調。
    - 預訓練模型通常是在通用目標檢測資料集（如 COCO 或 ImageNet）上訓練的，它們可能無法直接很好地適應車牌檢測任務。
    - 車牌具有特定的外觀和特徵，與通用目標有很大差異。所以需要特別收集車牌圖片並且標記。
- **類似 DinoV2 的基礎模型：**
    - DinoV2 屬於自監督學習的影像模型，擁有強大的影像特徵提取能力，但是DinoV2 本身不是專門設計用於目標檢測的模型。將 DinoV2 的特徵提取能力與目標檢測頭結合，可以有效提高檢測的精確度。
    - 使用類似DinoV2這類的基礎模型，仍然需要車牌的標記資料來訓練目標檢測的接續模組，以達到最佳的車牌檢測效果，所以還是需要準備標註的車牌datasets。
    - 這類模型強調的是強大的特徵提取能力，這有助於簡化目標檢測的訓練過程，減少對大量標註資料的依賴，但是目標檢測的後續模型，還是需要標記的資料做訓練。

**重點提示：**

- 資料集的品質和數量對模型性能至關重要。
- 資料增強可以有效提高模型的泛化能力。
- 模型微調是將預訓練模型適應特定任務的關鍵步驟。


### 2. 資料集準備

- **圖像尺寸與格式：**
    - 大多數深度學習目標檢測模型（包括您提到的那些）都對輸入圖像的尺寸有一定的要求。
    - 通常，建議將圖像調整為統一的尺寸，以便進行批次處理和加速訓練。常見的尺寸包括 224x224、512x512 或 608x608。
    - 然而，某些模型（例如 YOLO 系列）具有一定的靈活性，允許輸入不同尺寸的圖像。在這種情況下，模型可能會自動調整圖像尺寸或使用多尺度訓練。
    - 圖像格式通常為常見的格式，例如 JPEG、PNG 或 BMP。
- **訓練資料集標註格式：**
    - 目標檢測任務的標註通常包含以下資訊：
        - 圖像檔案路徑：指向圖像檔案的絕對或相對路徑。
        - 目標類別：表示目標所屬的類別（例如，車牌）。
        - 邊界框座標：定義目標邊界框的位置。常見的格式包括：
            - Pascal VOC 格式：`[x_min, y_min, x_max, y_max]`，其中 (x_min, y_min) 是左上角座標，(x_max, y_max) 是右下角座標。
            - COCO 格式：`[x_min, y_min, width, height]`，其中 (x_min, y_min) 是左上角座標，width 和 height 是邊界框的寬度和高度。
            - YOLO 格式：中心座標(x_center, y_center)， box 的寬度，高度，以及class index. 皆會正規化到0~1之間。
    - 標註檔案的格式可能因工具或資料集而異。常見的格式包括：
        - XML 檔案（用於 Pascal VOC）
        - JSON 檔案（用於 COCO）
        - TXT file (用於 Yolo)
- **範例說明:**
    - 假設我們的車牌資料集中有一個名為 "image001.jpg" 的圖像，其中包含一個車牌。
    - 使用 Pascal VOC 格式，標註檔案可能如下所示：
        - `<annotation>`
            - `<filename>image001.jpg</filename>`
            - `<object>`
                - `<name>license_plate</name>`
                - `<bndbox>`
                    - `<xmin>100</xmin>`
                    - `<ymin>200</ymin>`
                    - `<xmax>300</xmax>`
                    - `<ymax>250</ymax>`
                - `</bndbox>`
            - `</object>`
        - `</annotation>`

**2. 模型輸出格式：**

- 目標檢測模型的輸出通常包含以下資訊：
    - 邊界框座標：預測目標的邊界框位置。
    - 目標類別：預測目標的類別。
    - 置信度分數：表示模型對預測結果的置信程度。
- 不同的目標檢測模型在輸出格式上可能略有差異：
    - **RCNN 系列（RCNN、Fast RCNN、Faster RCNN）：**
        - 每個預測目標輸出一個邊界框和一個類別機率向量。
    - **YOLO 系列：**
        - 將圖像劃分為網格，並為每個網格單元預測多個邊界框、類別機率和置信度分數。
    - **SSD、RetinaNet、EfficientDet、MobileNet-SSD:**
        - 這類模型大多是one stage 的模型，所以輸出為一系列的Bounding box 的預測，每一個box都會有座標，類別機率，以及置信度。
    - **FCOS:**
        - 屬於Anchor free 的模型，所以輸出為對於圖像中每一個像素點預測到目標的距離，以及每一個像素點為目標的機率，以及類別。
    - **DETR、RT-DETR、RF-DETR：**
        - 使用 Transformer 架構，直接預測一組固定數量的目標邊界框和類別。

**補充說明：**

- 資料集的品質和數量對模型性能至關重要。
- 資料增強可以有效提高模型的泛化能力。
- 訓練模型時，通常會需要調整很多超參數，才能使模型達到最佳狀態。

### YOLO馬賽克增強跟混合增強

YOLO 系列模型在訓練過程中採用的幾種策略，包括“馬賽克增強”（Mosaic Augmentation）、“混合增強”（MixUp Augmentation）以及“大 epoch 訓練時長”，並指出這些策略的特點：它們不會損害模型在推理（inference）階段的性能，反而能顯著提升模型的整體性能（例如 mAP）。以下是這句話的逐步解釋：

1. **“不會傷害到模型的推理性能”**：
    - 這裡的“推理性能”指的是模型在實際部署時的表現，例如推理速度和資源消耗。這些增強策略主要應用於訓練階段，對模型的網路結構或參數量沒有直接影響，因此推理時的計算複雜度和延遲（latency）不會增加。
    - 換句話說，這些策略是“訓練時增強”（training-time augmentation），而不是改變模型架構或推理流程，因此不會拖慢推理速度。
2. **“大幅度提升模型的性能”**：
    - 這裡的“性能”通常指的是模型的檢測精度，例如在 COCO 數據集上的 mAP（mean Average Precision）。這些策略通過增加訓練數據的多樣性、提升模型的泛化能力以及充分利用長時間訓練，顯著提高了模型的準確性和魯棒性。
3. **具體策略**：
    - “馬賽克增強”、“混合增強”和“大 epoch 訓練時長”是 YOLO 系列（特別是 YOLOv4、YOLOv5 和 YOLOv8）常用的訓練技巧，下面將詳細介紹前兩者。

---

### 詳細介紹“馬賽克增強”（Mosaic Augmentation）

#### 定義

馬賽克增強是一種數據增強技術，由 YOLOv4 首次引入，並在後續版本（如 YOLOv5 和 YOLOv8）中廣泛使用。它通過將四張訓練圖像拼接成一張新圖像來模擬更複雜的場景，從而增加數據的多樣性和模型的學習能力。

#### 工作原理

1. **圖像拼接**：
    - 從數據集中隨機選取四張圖像。
    - 將這四張圖像按一定比例縮放並拼接成一張大圖，通常以 2x2 的網格形式排列。
    - 拼接過程中，每張圖像的邊界框（bounding box）和類別標籤會根據新的坐標系重新計算。
2. **隨機性**：
    - 圖像的縮放比例、拼接位置和裁剪方式是隨機的，增加了數據的隨機性。
    - 例如，四張圖像可能被放置在不同的角落，並可能有部分重疊或裁剪。
3. **輸出**：
    - 生成一張新的訓練圖像，包含來自四張圖像的物件和背景，邊界框標籤也隨之更新。

#### 視覺示例

假設有四張圖像：

- 圖 A：一隻狗
- 圖 B：一輛車
- 圖 C：一個人
- 圖 D：一棵樹 馬賽克增強可能將它們拼接成如下形式：

text

CollapseWrapCopy

`| 狗 (A) | 車 (B) | | 人 (C) | 樹 (D) |`

最終生成的圖像包含狗、車、人和樹四個物件，邊界框也隨之調整。

#### 優勢

1. **增加數據多樣性**：
    - 通過拼接多張圖像，模擬了更複雜的場景（例如多物件、重疊物件），這在真實世界中很常見。
    - 減少了對單一圖像場景的過擬合，提升了模型的泛化能力。
2. **提升小物件檢測**：
    - 由於圖像縮放和拼接，小物件在最終圖像中可能以不同尺度出現，幫助模型學習檢測小物件的能力。
    - 研究顯示，這對 COCO 數據集中的小物件 mAP 有顯著提升。
3. **高效利用上下文信息**：
    - 拼接後的圖像包含多樣的背景和物件，迫使模型學習區分前景和背景，提升了魯棒性。
4. **訓練時增強**：
    - 這種增強僅在訓練時應用，推理時仍使用原始圖像，因此不影響推理速度。

#### YOLOv8 中的應用

- 在 YOLOv8 的訓練過程中，馬賽克增強是默認啟用的，但有一個特別策略：在最後 10 個 epoch 中關閉馬賽克增強（參見 Ultralytics 文檔）。這是因為接近訓練結束時，使用原始圖像有助於模型更好地適應真實測試數據，提升最終精度。

#### 證據

根據 YOLOv5 和 YOLOv8 的實驗數據，啟用馬賽克增強後，mAP 通常提升 1-2%，特別是在小物件檢測場景中效果顯著。

---

### 詳細介紹“混合增強”（MixUp Augmentation）

#### 定義

混合增強（MixUp）是一種數據增強技術，最初由 Zhang 等人於 2017 年提出（論文 _mixup: Beyond Empirical Risk Minimization_），後被 YOLO 系列採用。它通過將兩張圖像按一定比例線性組合生成新圖像，並混合它們的標籤，來增加數據的多樣性。

#### 工作原理

1. **圖像混合**：
    - 從數據集中隨機選取兩張圖像，例如圖 A 和圖 B。
    - 使用一個隨機權重 λ\lambdaλ（通常從 Beta 分佈中抽樣，範圍在 [0, 1] 之間），按以下公式混合： 新圖像=λ⋅圖 A+(1−λ)⋅圖 B\text{新圖像} = \lambda \cdot \text{圖 A} + (1 - \lambda) \cdot \text{圖 B}新圖像=λ⋅圖 A+(1−λ)⋅圖 B
    - 例如，若 λ=0.7\lambda = 0.7λ=0.7，新圖像由 70% 的圖 A 和 30% 的圖 B 組成。
2. **標籤混合**：
    - 對應的邊界框和類別標籤也按相同比例混合： 新標籤=λ⋅標籤 A+(1−λ)⋅標籤 B\text{新標籤} = \lambda \cdot \text{標籤 A} + (1 - \lambda) \cdot \text{標籤 B}新標籤=λ⋅標籤 A+(1−λ)⋅標籤 B
    - 如果圖 A 有一隻狗，圖 B 有一輛車，混合後的新圖像標籤可能是“70% 狗 + 30% 車”。
3. **輸出**：
    - 生成一張新的訓練圖像，其像素值和標籤都是兩張圖像的加權組合。

#### 視覺示例

- 圖 A：一隻狗（標籤：狗，邊界框 [x1, y1, x2, y2]）
- 圖 B：一輛車（標籤：車，邊界框 [x3, y3, x4, y4]）
- 若 λ=0.6\lambda = 0.6λ=0.6：
    - 新圖像 = 0.6 * 圖 A + 0.4 * 圖 B
    - 新標籤 = 0.6 * 狗標籤 + 0.4 * 車標籤 結果是一張模糊的圖像，同時包含狗和車的特徵，標籤也反映了這種混合。

#### 優勢

1. **平滑決策邊界**：
    - MixUp 迫使模型學習線性插值數據的表示，而不是僅適應單一類別的樣本，從而平滑了分類器的決策邊界，提升了泛化能力。
2. **增強魯棒性**：
    - 通過引入“模糊”數據，模型學會處理不確定性場景，例如重疊物件或低質量圖像。
3. **減少過擬合**：
    - 混合後的數據增加了訓練樣本的多樣性，降低了模型對特定圖像的依賴。
4. **訓練時增強**：
    - 與馬賽克增強類似，MixUp 只影響訓練過程，不改變推理階段的計算負擔。

#### YOLOv8 中的應用

- MixUp 在 YOLOv8 中作為可選增強策略，通常與馬賽克增強搭配使用。
- 其 λ\lambdaλ 值從 Beta 分佈中抽樣（例如 Beta(α, α)，α 通常設為 1.0），確保混合比例的隨機性。

#### 證據

研究表明，MixUp 在分類任務中可提升 1-2% 的準確率，在 YOLO 的檢測任務中，對 mAP 的提升略小（約 0.5-1%），但對模型的穩定性和泛化能力有顯著幫助。

---

### “大 epoch 訓練時長”的作用

#### 解釋

- “大 epoch 訓練時長”指的是在訓練 YOLO 模型時使用較多的迭代次數（epoch），例如 YOLOv8 默認建議 100-300 個 epoch，甚至在大型數據集上可能更多。
- 這與增強策略相輔相成，因為更多的訓練輪次允許模型充分利用增強數據的多樣性。

#### 優勢

1. **充分利用數據增強**：
    - 馬賽克和 MixUp 生成了大量虛擬數據，長時間訓練確保模型能充分學習這些數據的模式。
2. **提升收斂性**：
    - 更大的 epoch 數有助於模型找到損失函數的更優解，提升最終精度。
3. **不影響推理**：
    - 訓練時長只影響訓練階段，對推理速度無影響。

#### YOLOv8 的特殊策略

- 如前所述，YOLOv8 在最後 10 個 epoch 關閉馬賽克增強，這是一個意外但有效的細節。這種策略讓模型在訓練末期專注於原始數據，提升了與測試集的匹配度。

---

### 總結這句話的含義

- **馬賽克增強**通過拼接多張圖像，模擬複雜場景，提升小物件檢測和泛化能力。
- **混合增強**通過線性混合圖像和標籤，平滑決策邊界，增強魯棒性。
- **大 epoch 訓練時長**充分利用增強數據，確保模型收斂到更高的性能。
- 這些策略的共同點是它們只作用於訓練階段，不增加推理時的計算負擔，因此“不會傷害推理性能”；同時，它們顯著提升了模型的檢測精度（mAP）和適應性，從而“大幅度提升模型性能”。

這種訓練策略是 YOLO 系列能夠在速度和精度之間取得平衡的關鍵原因之一，尤其在 YOLOv8 中得到了進一步優化。