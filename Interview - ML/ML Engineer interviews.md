
What concepts are commonly asked in ğ— ğ—Ÿ ğ—˜ğ—»ğ—´ğ—¶ğ—»ğ—²ğ—²ğ—¿ ğ—¶ğ—»ğ˜ğ—²ğ—¿ğ˜ƒğ—¶ğ—²ğ˜„ğ˜€?  
  
These are fair game in interviews at ğ˜€ğ˜ğ—®ğ—¿ğ˜ğ˜‚ğ—½ğ˜€, ğ—°ğ—¼ğ—»ğ˜€ğ˜‚ğ—¹ğ˜ğ—¶ğ—»ğ—´ ğ—³ğ—¶ğ—¿ğ—ºğ˜€ & ğ—™ğ—”ğ—”ğ—¡ğ—šğ˜€.  
  
Here's a comprehensive list ğŸ‘‡ğŸ‘‡ğŸ‘‡  
  
ğ— ğ—Ÿ ğ—™ğ˜‚ğ—»ğ—±ğ—®ğ—ºğ—²ğ—»ğ˜ğ—®ğ—¹ğ˜€  
â†³ Variance-Bias Trade-off  
â†³ Regression Algorithms (e.g. Linear, Polynomial)  
â†³ Classification Algorithms (e.g. GBM)  
â†³ Clustering Algorithms  
â†³ Deep Learning (e.g. DNN, LSTM, Transformers)  
â†³ Reinforcement Learning  
â†³ Model Evaluation Metrics (Precision, Recall, AUC-ROC)  
â†³ Handling Missing Data  
â†³ Feature Scaling (Normalization, Standardization)  
â†³ Feature Selection Techniques  
â†³ Dimensionality Reduction (PCA, t-SNE)  
â†³ Encoding Categorical Data  
â†³ Hyperparameter Tuning  
â†³ Cross-Validation Techniques  
â†³ Regularization Methods (L1, L2)  
  
ğ—–ğ—¼ğ—±ğ—¶ğ—»ğ—´  
â†³ Python for ML  
â†³ Writing Efficient Code  
â†³ Data Structures and Algorithms  
â†³ Implementing ML Algorithms from Scratch  
â†³ Working with ML Libraries (scikit-learn, PyTorch)  
  
ğ—¦ğ˜†ğ˜€ğ˜ğ—²ğ—º ğ——ğ—²ğ˜€ğ—¶ğ—´ğ—»  
â†³ Data Ingestion and Preprocessing  
â†³ ETL/ELT Processes  
â†³ Handling Big Data (Hadoop, Spark)  
â†³ Kafka for streaming  
â†³ Caching  
â†³ SQL vs noSQL  
â†³ Load Balancing  
â†³ Edge Deploymebt  
  
ğ— ğ—Ÿ ğ—¦ğ˜†ğ˜€ğ˜ğ—²ğ—º ğ——ğ—²ğ˜€ğ—¶ğ—´ğ—»  
â†³ Design Recommender System  
â†³ Fraud Detection System  
â†³ Real-Time Bidding  
â†³ Chatbot Architecture  
â†³ Sentiment Analysis Pipeline  
â†³ Image Classification System  
â†³ Voice Recognition System  
  
ğ— ğ—Ÿ ğ——ğ—²ğ—½ğ—¹ğ—¼ğ˜†ğ—ºğ—²ğ—»ğ˜  
â†³ Model Serving (Batch, Real-Time)  
â†³ Monitoring and Maintaining Models in Production  
â†³ Model Retraining Strategies  
â†³ Workflow Orchestration (Airflow, Kubeflow)  
â†³ Experiment Tracking (MLflow)  
â†³ Model Registry and Versioning  
  
Now go ace your next interviewğŸ‘‡  
  
ğŸ“• ğ—œğ—»ğ˜ğ—²ğ—¿ğ˜ƒğ—¶ğ—²ğ˜„ ğ—£ğ—¿ğ—²ğ—½ ğ—–ğ—¼ğ˜‚ğ—¿ğ˜€ğ—²ğ˜€: https://lnkd.in/gzgB-dHT  
ğŸ“˜ ğ—ğ—¼ğ—¶ğ—» ğ——ğ—¦ ğ—œğ—»ğ˜ğ—²ğ—¿ğ˜ƒğ—¶ğ—²ğ˜„ ğ—•ğ—¼ğ—¼ğ˜ğ—°ğ—®ğ—ºğ—½: https://lnkd.in/eiA5Ntdp  
ğŸ“™ ğ—ğ—¼ğ—¶ğ—» ğ— ğ—Ÿğ—˜ ğ—œğ—»ğ˜ğ—²ğ—¿ğ˜ƒğ—¶ğ—²ğ˜„ ğ—•ğ—¼ğ—¼ğ˜ğ—°ğ—®ğ—ºğ—½: https://lnkd.in/e6HbN6dy  
ğŸ“— ğ—”ğ—• ğ—§ğ—²ğ˜€ğ˜ğ—¶ğ—»ğ—´ ğ—–ğ—¼ğ˜‚ğ—¿ğ˜€ğ—²: https://lnkd.in/g82dMJ77


### **1. Variance-Bias Trade-offï¼ˆåå·®-æ–¹å·®æ¬Šè¡¡ï¼‰**

åå·®-æ–¹å·®æ¬Šè¡¡ï¼ˆBias-Variance Trade-offï¼‰æ˜¯æ©Ÿå™¨å­¸ç¿’ä¸­é—œæ–¼**æ¨¡å‹è¤‡é›œåº¦èˆ‡æ³›åŒ–èƒ½åŠ›**ä¹‹é–“çš„å¹³è¡¡å•é¡Œï¼š

- **åå·®ï¼ˆBiasï¼‰**ï¼šæŒ‡æ¨¡å‹å°æ•¸æ“šçš„é©é…èƒ½åŠ›ã€‚å¦‚æœåå·®é«˜ï¼Œèªªæ˜æ¨¡å‹éæ–¼ç°¡å–®ï¼Œå¯èƒ½ç„¡æ³•æ•æ‰æ•¸æ“šçš„çœŸå¯¦æ¨¡å¼ï¼Œå°è‡´**æ¬ æ“¬åˆï¼ˆunderfittingï¼‰**ã€‚
- **æ–¹å·®ï¼ˆVarianceï¼‰**ï¼šæŒ‡æ¨¡å‹å°æ•¸æ“šçš„æ•æ„Ÿåº¦ã€‚å¦‚æœæ–¹å·®é«˜ï¼Œèªªæ˜æ¨¡å‹å°è¨“ç·´æ•¸æ“šå­¸å¾—éå¥½ï¼Œå¯èƒ½è¨˜ä½äº†æ•¸æ“šçš„ç´°ç¯€å’Œå™ªè²ï¼Œå°è‡´**éæ“¬åˆï¼ˆoverfittingï¼‰**ã€‚

**è§£æ±ºæ–¹æ³•ï¼š**

- å¢åŠ æ•¸æ“šé‡å¯ä»¥é™ä½æ–¹å·®
- é©ç•¶çš„æ­£å‰‡åŒ–ï¼ˆL1, L2ï¼‰å¯ä»¥é™ä½éæ“¬åˆ
- ä½¿ç”¨äº¤å‰é©—è­‰ï¼ˆCross-Validationï¼‰ä¾†é¸æ“‡æœ€ä½³æ¨¡å‹

---

### **2. Regression Algorithmsï¼ˆå›æ­¸æ¼”ç®—æ³•ï¼‰**

å›æ­¸ï¼ˆRegressionï¼‰æ˜¯ç”¨æ–¼é æ¸¬**é€£çºŒæ•¸å€¼**çš„æ¨¡å‹ï¼š

- **ç·šæ€§å›æ­¸ï¼ˆLinear Regressionï¼‰**ï¼šå‡è¨­è¼¸å…¥ç‰¹å¾µèˆ‡è¼¸å‡ºä¹‹é–“çš„é—œä¿‚ç‚ºç·šæ€§ï¼Œä¾‹å¦‚ï¼š y=w1x1+w2x2+â‹¯+wnxn+by = w_1 x_1 + w_2 x_2 + \dots + w_n x_n + by=w1â€‹x1â€‹+w2â€‹x2â€‹+â‹¯+wnâ€‹xnâ€‹+b
- **å¤šé …å¼å›æ­¸ï¼ˆPolynomial Regressionï¼‰**ï¼šåœ¨ç·šæ€§å›æ­¸çš„åŸºç¤ä¸ŠåŠ å…¥å¤šé …å¼ç‰¹å¾µï¼Œä¾‹å¦‚ï¼š y=w1x+w2x2+w3x3+by = w_1 x + w_2 x^2 + w_3 x^3 + by=w1â€‹x+w2â€‹x2+w3â€‹x3+b ç”¨æ–¼æ“¬åˆéç·šæ€§æ•¸æ“šã€‚

---

### **3. Classification Algorithmsï¼ˆåˆ†é¡æ¼”ç®—æ³•ï¼Œä¾‹å¦‚ GBMï¼‰**

åˆ†é¡ï¼ˆClassificationï¼‰æ˜¯ç”¨ä¾†é æ¸¬**é›¢æ•£æ¨™ç±¤**ï¼ˆä¾‹å¦‚ã€Œè²“æˆ–ç‹—ã€ã€ã€Œç–¾ç—…æˆ–å¥åº·ã€ï¼‰çš„ç®—æ³•ï¼š

- **æ±ºç­–æ¨¹ï¼ˆDecision Treeï¼‰**ï¼šæ ¹æ“šæ•¸æ“šç‰¹å¾µé€å±¤é€²è¡Œåˆ†é¡ã€‚
- **æ¢¯åº¦æå‡æ©Ÿï¼ˆGradient Boosting Machine, GBMï¼‰**ï¼šä¸€ç¨®é›†æˆå­¸ç¿’ï¼ˆEnsemble Learningï¼‰æ–¹æ³•ï¼Œå®ƒé€éå¤šæ£µæ±ºç­–æ¨¹ä¾†æå‡åˆ†é¡æ€§èƒ½ï¼ˆå¦‚ XGBoost, LightGBMï¼‰ã€‚

---

### **4. Clustering Algorithmsï¼ˆèšé¡æ¼”ç®—æ³•ï¼‰**

èšé¡ï¼ˆClusteringï¼‰æ˜¯**ç„¡ç›£ç£å­¸ç¿’**æ–¹æ³•ï¼Œå°‡æ•¸æ“šé»åŠƒåˆ†ç‚ºä¸åŒçš„çµ„ï¼š

- **K-Means**ï¼šåŸºæ–¼è³ªå¿ƒï¼ˆcentroidï¼‰å°æ•¸æ“šé€²è¡Œåˆ†çµ„ã€‚
- **DBSCAN**ï¼šåŸºæ–¼å¯†åº¦ï¼Œé©åˆéçƒå½¢åˆ†ä½ˆçš„æ•¸æ“šã€‚
- **å±¤æ¬¡èšé¡ï¼ˆHierarchical Clusteringï¼‰**ï¼šç”Ÿæˆä¸€æ£µæ¨¹ç‹€çš„æ•¸æ“šåˆ†é¡çµæ§‹ã€‚

---

### **5. Deep Learningï¼ˆæ·±åº¦å­¸ç¿’ï¼‰**

- **æ·±åº¦ç¥ç¶“ç¶²çµ¡ï¼ˆDeep Neural Network, DNNï¼‰**ï¼šå¤šå±¤æ„ŸçŸ¥æ©Ÿï¼ˆMLPï¼‰ï¼Œé©ç”¨æ–¼çµæ§‹åŒ–æ•¸æ“šèˆ‡ç°¡å–®çš„å½±åƒè™•ç†ã€‚
- **é•·çŸ­æ™‚è¨˜æ†¶ç¶²çµ¡ï¼ˆLSTMï¼‰**ï¼šè™•ç†æ™‚é–“åºåˆ—æ•¸æ“šï¼ˆå¦‚èªéŸ³è­˜åˆ¥ã€è‚¡å¸‚é æ¸¬ï¼‰ã€‚
- **è®Šæ›å™¨ï¼ˆTransformersï¼‰**ï¼šä¾‹å¦‚ BERTã€GPTï¼Œç”¨æ–¼è‡ªç„¶èªè¨€è™•ç†ï¼ˆNLPï¼‰ã€‚

---

### **6. Reinforcement Learningï¼ˆå¼·åŒ–å­¸ç¿’ï¼‰**

å¼·åŒ–å­¸ç¿’ï¼ˆRLï¼‰æ˜¯åŸºæ–¼çå‹µæ©Ÿåˆ¶çš„å­¸ç¿’æ–¹æ³•ï¼š

- **Agent**ï¼ˆæ™ºèƒ½é«”ï¼‰åœ¨ç’°å¢ƒï¼ˆEnvironmentï¼‰ä¸­æ¡å–è¡Œå‹•ï¼ˆActionï¼‰ï¼Œç²å–çå‹µï¼ˆRewardï¼‰ã€‚
- å¸¸è¦‹ç®—æ³•åŒ…æ‹¬ï¼š
    - Q-Learningï¼ˆåŸºæ–¼ Q å€¼æ›´æ–°ï¼‰
    - Deep Q Networkï¼ˆDQNï¼‰
    - Actor-Criticï¼ˆç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼‰

---

### **7. Model Evaluation Metricsï¼ˆæ¨¡å‹è©•ä¼°æŒ‡æ¨™ï¼‰**

- **æº–ç¢ºç‡ï¼ˆAccuracyï¼‰**ï¼šæ­£ç¢ºé æ¸¬æ•¸é‡ä½”ç¸½æ•¸çš„æ¯”ä¾‹ã€‚
- **ç²¾ç¢ºç‡ï¼ˆPrecisionï¼‰**ï¼š Precision=TPTP+FP\text{Precision} = \frac{TP}{TP + FP}Precision=TP+FPTPâ€‹ è¡¨ç¤ºæ¨¡å‹é æ¸¬ç‚ºæ­£é¡çš„æ¨£æœ¬ä¸­æœ‰å¤šå°‘æ˜¯çœŸæ­£çš„æ­£é¡ã€‚
- **å¬å›ç‡ï¼ˆRecallï¼‰**ï¼š Recall=TPTP+FN\text{Recall} = \frac{TP}{TP + FN}Recall=TP+FNTPâ€‹ è¡¨ç¤ºå¯¦éš›çš„æ­£é¡æ¨£æœ¬ä¸­æœ‰å¤šå°‘è¢«æ¨¡å‹æ­£ç¢ºé æ¸¬ã€‚
- **AUC-ROC**ï¼šè¡¡é‡åˆ†é¡æ¨¡å‹çš„æ€§èƒ½ï¼ŒAUC è¶Šæ¥è¿‘ 1ï¼Œæ¨¡å‹è¶Šå„ªç§€ã€‚

---

### **8. Handling Missing Dataï¼ˆè™•ç†ç¼ºå¤±æ•¸æ“šï¼‰**

- **åˆªé™¤ç¼ºå¤±æ•¸æ“š**ï¼šå¦‚æœç¼ºå¤±å€¼æ¯”ä¾‹ä½ï¼Œå¯ä»¥åˆªé™¤ã€‚
- **å¡«å……ç¼ºå¤±å€¼ï¼ˆImputationï¼‰**ï¼š
    - ä½¿ç”¨å‡å€¼ã€ä¸­ä½æ•¸ã€çœ¾æ•¸å¡«è£œ
    - ä½¿ç”¨å›æ­¸æˆ– KNN é€²è¡Œæ’è£œ

---

### **9. Feature Scalingï¼ˆç‰¹å¾µç¸®æ”¾ï¼‰**

- **æ­¸ä¸€åŒ–ï¼ˆNormalizationï¼‰**ï¼š xâ€²=xâˆ’minâ¡(x)maxâ¡(x)âˆ’minâ¡(x)x' = \frac{x - \min(x)}{\max(x) - \min(x)}xâ€²=max(x)âˆ’min(x)xâˆ’min(x)â€‹ æŠŠæ•¸æ“šæ˜ å°„åˆ° [0,1]ã€‚
- **æ¨™æº–åŒ–ï¼ˆStandardizationï¼‰**ï¼š xâ€²=xâˆ’Î¼Ïƒx' = \frac{x - \mu}{\sigma}xâ€²=Ïƒxâˆ’Î¼â€‹ è®“æ•¸æ“šçš„å‡å€¼ç‚º 0ï¼Œæ¨™æº–å·®ç‚º 1ã€‚

---

### **10. Feature Selection Techniquesï¼ˆç‰¹å¾µé¸æ“‡æ–¹æ³•ï¼‰**

- **Filter æ–¹æ³•**ï¼ˆåŸºæ–¼çµ±è¨ˆï¼‰ï¼šå¦‚çš®çˆ¾æ£®ç›¸é—œä¿‚æ•¸ã€ä¿¡æ¯å¢ç›Šã€‚
- **Wrapper æ–¹æ³•**ï¼ˆåŸºæ–¼æ¨¡å‹ï¼‰ï¼šå¦‚éºå‚³ç®—æ³•ï¼ˆGAï¼‰ã€‚
- **åµŒå…¥å¼æ–¹æ³•ï¼ˆEmbeddedï¼‰**ï¼šå¦‚ Lasso å›æ­¸ï¼ˆL1 æ­£å‰‡åŒ–ï¼‰ã€‚

---

### **11. Dimensionality Reductionï¼ˆé™ç¶­ï¼‰**

- **ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰**ï¼šåŸºæ–¼å”æ–¹å·®çŸ©é™£æ‰¾åˆ°æœ€ä½³æŠ•å½±æ–¹å‘ï¼Œä¿ç•™æœ€å¤§è®Šç•°æ•¸ã€‚
- **t-SNE**ï¼šéç·šæ€§é™ç¶­æ–¹æ³•ï¼Œé©åˆå¯è¦–åŒ–ã€‚

---

### **12. Encoding Categorical Dataï¼ˆç·¨ç¢¼é¡åˆ¥ç‰¹å¾µï¼‰**

- **ç¨ç†±ç·¨ç¢¼ï¼ˆOne-Hot Encodingï¼‰**ï¼šé©ç”¨æ–¼ç„¡åºé¡åˆ¥è®Šé‡ã€‚
- **æ¨™ç±¤ç·¨ç¢¼ï¼ˆLabel Encodingï¼‰**ï¼šé©ç”¨æ–¼æœ‰åºè®Šé‡ï¼ˆå¦‚å°ã€ä¸­ã€å¤§ï¼‰ã€‚

---

### **13. Hyperparameter Tuningï¼ˆè¶…åƒæ•¸èª¿æ•´ï¼‰**

- **ç¶²æ ¼æœç´¢ï¼ˆGrid Searchï¼‰**ï¼šå˜—è©¦æ‰€æœ‰å¯èƒ½çš„åƒæ•¸çµ„åˆã€‚
- **éš¨æ©Ÿæœç´¢ï¼ˆRandom Searchï¼‰**ï¼šéš¨æ©ŸæŠ½æ¨£éƒ¨åˆ†åƒæ•¸çµ„åˆã€‚
- **è²è‘‰æ–¯å„ªåŒ–ï¼ˆBayesian Optimizationï¼‰**ï¼šæ ¹æ“šéå»çµæœä¾†é¸æ“‡æ–°çš„åƒæ•¸çµ„åˆã€‚

---

### **14. Cross-Validation Techniquesï¼ˆäº¤å‰é©—è­‰æŠ€è¡“ï¼‰**

- **K æŠ˜äº¤å‰é©—è­‰ï¼ˆK-Fold Cross Validationï¼‰**ï¼šå°‡æ•¸æ“šåˆ†æˆ K å€‹å­é›†ï¼Œæ¯æ¬¡ç”¨ K-1 å€‹å­é›†è¨“ç·´ï¼Œå‰©é¤˜ 1 å€‹å­é›†æ¸¬è©¦ã€‚
- **ç•™ä¸€é©—è­‰ï¼ˆLOO, Leave-One-Outï¼‰**ï¼šæ¯æ¬¡ç”¨ n-1 å€‹æ•¸æ“šè¨“ç·´ï¼Œ1 å€‹æ•¸æ“šæ¸¬è©¦ã€‚

---

### **15. Regularization Methodsï¼ˆæ­£å‰‡åŒ–ï¼‰**

- **L1 æ­£å‰‡åŒ–ï¼ˆLassoï¼‰**ï¼šè®“éƒ¨åˆ†æ¬Šé‡è®Š 0ï¼Œå¯¦ç¾ç‰¹å¾µé¸æ“‡ã€‚
- **L2 æ­£å‰‡åŒ–ï¼ˆRidgeï¼‰**ï¼šæŠ‘åˆ¶éå¤§çš„æ¬Šé‡ï¼Œæå‡æ¨¡å‹ç©©å®šæ€§ã€‚