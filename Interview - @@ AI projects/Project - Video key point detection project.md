
My colab project:
https://colab.research.google.com/drive/18JK_0xvzLe-XszfTzxHr_Ztgjl6ueB3L

Reference:  
![[Pasted image 20250807001320.png]]

Post: Replacing Human Referees with AI
https://www.linkedin.com/posts/skalskip92_computervision-opensource-multimodal-activity-7355920959477837825-gjVm?utm_source=share&utm_medium=member_desktop&rcm=ACoAABMNj2MBAJSP3cWd4xpiz4wB7qdx43hvW18

Basketball AI: Automatic Detection of 3-Second Violation (Colab)
https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/basketball-ai-automatic-detection-of-3-second-violations.ipynb

Supervision github
https://github.com/roboflow/supervision

```
As a basketball and computer vision fan, I've always been fascinated by the idea of replacing referees with AI. Automating complex rules like the 3-second violation is challenging, and requires a lot of moving parts. But last week, [Alexander Bodner](https://www.linkedin.com/in/alexanderbodner/) and I finally got an MVP algorithm working!  
  
The best part is that all three models we used to build this project are fully open-source and available under the Apache 2.0 license:  
  
- RF-DETR â€“ for ball, number, player, referee, and basket detection.  
- SAM2 â€“ for player segmentation and tracking.  
- ViTPose++ â€“ for player pose estimation.  
  
â®‘ ðŸ”— notebook with the code: [https://lnkd.in/d2WPAgiv](https://lnkd.in/d2WPAgiv)  
  
You'll find links to my basketball player detection, ball keypoint detection models, as well as an end-to-end blog post explaining every step of the algorithm, in the comments below
```


#### detect players skeletons

```python
def plot_skeletons(key_points: sv.KeyPoints, tracker_ids: list[int], frame: np.array):
    for tracker_id in tracker_ids:
        tracker_key_points = key_points[int(tracker_id) - 1]
        annotator = sv.EdgeAnnotator(
            color=COLOR.by_idx(tracker_id),
            thickness=3
        )
        frame = annotator.annotate(scene=frame, key_points=tracker_key_points)
    return frame
```
é€™å€‹ cell å®šç¾©äº†ä¸€å€‹åç‚º `plot_skeletons` çš„è¼”åŠ©å‡½å¼ï¼Œå…¶ä¸»è¦åŠŸèƒ½æ˜¯åœ¨å½±åƒä¸Šç¹ªè£½æª¢æ¸¬åˆ°çš„éª¨éª¼ã€‚

- **`def plot_skeletons(key_points: sv.KeyPoints, tracker_ids: list[int], frame: np.array):`**
    
    - é€™æ˜¯ä¸€å€‹å‡½å¼å®šç¾©ã€‚å®ƒæŽ¥å—ä¸‰å€‹åƒæ•¸ï¼š
        
        - `key_points`: é¡žåž‹ç‚º `sv.KeyPoints`ï¼Œé€™æ˜¯ `supervision` å‡½å¼åº«ä¸­ç”¨ä¾†å„²å­˜é—œéµé»žè³‡è¨Šçš„ç‰©ä»¶ã€‚å®ƒåŒ…å«äº†æ¯å€‹äººç‰©çš„é—œç¯€åº§æ¨™ã€‚
            
        - `tracker_ids`: é¡žåž‹ç‚º `list[int]`ï¼Œä¸€å€‹æ•´æ•¸åˆ—è¡¨ï¼Œä»£è¡¨äº†æ¯å€‹è¢«è¿½è¹¤äººç‰©çš„å”¯ä¸€ IDã€‚
            
        - `frame`: é¡žåž‹ç‚º `np.array`ï¼Œä»£è¡¨å–®å€‹å½±åƒå¹€ï¼ˆä½¿ç”¨ NumPy é™£åˆ—æ ¼å¼ï¼‰ã€‚
            
- **`for tracker_id in tracker_ids:`**
    
    - é€™æ˜¯ä¸€å€‹è¿´åœˆï¼Œéæ­· `tracker_ids` åˆ—è¡¨ä¸­çš„æ¯ä¸€å€‹è¿½è¹¤ IDã€‚é€™ç¢ºä¿äº†æˆ‘å€‘ç‚ºæ¯ä¸€ä½è¢«åµæ¸¬åˆ°çš„äººç‰©ç¹ªè£½éª¨éª¼ã€‚
        
- **`tracker_key_points = key_points[int(tracker_id) - 1]`**
    
    - å¾ž `key_points` ç‰©ä»¶ä¸­å–å‡ºèˆ‡ç•¶å‰ `tracker_id` ç›¸å°æ‡‰çš„é—œéµé»žè³‡è¨Šã€‚
        
    - é€™è£¡æœ‰ä¸€å€‹å°ç´°ç¯€ï¼šå› ç‚ºè¿½è¹¤ ID é€šå¸¸å¾ž 1 é–‹å§‹ï¼Œè€Œ Python åˆ—è¡¨æˆ–é™£åˆ—çš„ç´¢å¼•æ˜¯å¾ž 0 é–‹å§‹ï¼Œæ‰€ä»¥æˆ‘å€‘éœ€è¦å°‡ `tracker_id` æ¸› 1 ä¾†ç²å–æ­£ç¢ºçš„ç´¢å¼•ã€‚
        
- **`annotator = sv.EdgeAnnotator(...)`**
    
    - å‰µå»ºä¸€å€‹ `supervision.EdgeAnnotator` ç‰©ä»¶ã€‚é€™å€‹ç‰©ä»¶å°ˆé–€ç”¨æ–¼åœ¨é—œéµé»žä¹‹é–“ç¹ªè£½é‚Šï¼ˆä¹Ÿå°±æ˜¯éª¨éª¼ï¼‰ã€‚
        
    - `color=COLOR.by_idx(tracker_id)`: æ ¹æ“šè¿½è¹¤ ID è‡ªå‹•é¸æ“‡ä¸€å€‹é¡è‰²ï¼Œé€™æ¨£æ¯å€‹ä¸åŒçš„äººç‰©éª¨éª¼æœƒæœ‰ä¸åŒçš„é¡è‰²ï¼Œæ–¹ä¾¿å€åˆ†ã€‚
        
    - `thickness=3`: è¨­å®šç¹ªè£½éª¨éª¼ç·šæ¢çš„ç²—ç´°ç‚º 3 åƒç´ ã€‚
        
- **`frame = annotator.annotate(scene=frame, key_points=tracker_key_points)`**
    
    - ä½¿ç”¨å‰µå»ºå¥½çš„ `annotator` ç‰©ä»¶ä¾†åœ¨ç•¶å‰ `frame` ä¸Šç¹ªè£½éª¨éª¼ã€‚
        
    - å®ƒå°‡ `tracker_key_points` ä¸­çš„é—œéµé»žé€£æŽ¥èµ·ä¾†ï¼Œå½¢æˆéª¨éª¼ï¼Œä¸¦å°‡ç¹ªè£½çµæžœæ‡‰ç”¨åˆ° `frame` ä¸Šã€‚
        
- **`return frame`**
    
    - å‡½å¼è¿”å›žå·²ç¶“ç¹ªè£½äº†æ‰€æœ‰éª¨éª¼çš„å½±åƒå¹€ã€‚
        

ç¸½çµä¾†èªªï¼Œé€™å€‹ cell æä¾›äº†ä¸€å€‹å¯é‡è¤‡ä½¿ç”¨çš„å‡½å¼ï¼Œå°‡çµ¦å®šçš„é—œéµé»žæ•¸æ“šè¦–è¦ºåŒ–ç‚ºéª¨éª¼ï¼Œä¸¦å°‡å…¶ç–ŠåŠ åˆ°å½±åƒä¸Šã€‚


```python
import torch
from transformers import AutoProcessor, VitPoseForPoseEstimation

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

POSE_ESTIMATION_MODEL_ID = "usyd-community/vitpose-plus-large"

pose_estimation_processor = AutoProcessor.from_pretrained(POSE_ESTIMATION_MODEL_ID)
pose_estimation_model = VitPoseForPoseEstimation.from_pretrained(
    POSE_ESTIMATION_MODEL_ID, device_map=DEVICE)
```
### **Cell 2: æ¨¡åž‹è¼‰å…¥èˆ‡åˆå§‹åŒ–**

é€™å€‹ cell ä¸»è¦è² è²¬è¼‰å…¥é è¨“ç·´çš„å§¿å‹¢ä¼°è¨ˆï¼ˆpose estimationï¼‰æ¨¡åž‹ã€‚

- **`import torch`**
    
    - å¼•å…¥ PyTorch å‡½å¼åº«ï¼Œé€™æ˜¯è¨±å¤šæ·±åº¦å­¸ç¿’æ¨¡åž‹çš„æ ¸å¿ƒã€‚
        
- **`from transformers import AutoProcessor, VitPoseForPoseEstimation`**
    
    - å¾ž Hugging Face çš„ `transformers` å‡½å¼åº«ä¸­å¼•å…¥å…©å€‹é‡è¦çš„é¡žåˆ¥ï¼š
        
        - `AutoProcessor`: ä¸€å€‹é€šç”¨çš„è™•ç†å™¨ï¼Œå¯ä»¥æ ¹æ“šæ¨¡åž‹ ID è‡ªå‹•è¼‰å…¥å°æ‡‰çš„è³‡æ–™é è™•ç†å™¨ã€‚
            
        - `VitPoseForPoseEstimation`: ç”¨æ–¼å§¿å‹¢ä¼°è¨ˆçš„ Vision Transformer æ¨¡åž‹ã€‚
            
- **`DEVICE = "cuda" if torch.cuda.is_available() else "cpu"`**
    
    - ä¸€å€‹æ¨™æº–çš„ç¨‹å¼ç¢¼ç‰‡æ®µï¼Œç”¨æ–¼æª¢æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„ NVIDIA GPUï¼ˆå³ `cuda`ï¼‰ã€‚å¦‚æžœæœ‰ï¼Œæ¨¡åž‹å°‡åœ¨ GPU ä¸Šé‹è¡Œï¼Œä»¥ç²å¾—æ›´å¿«çš„é‹ç®—é€Ÿåº¦ï¼›å¦å‰‡ï¼Œå®ƒå°‡åœ¨ CPU ä¸Šé‹è¡Œã€‚
        
- **`POSE_ESTIMATION_MODEL_ID = "usyd-community/vitpose-plus-large"`**
    
    - å®šç¾©ä¸€å€‹å­—ä¸²è®Šæ•¸ï¼Œå„²å­˜äº†æˆ‘å€‘å°‡è¦ä½¿ç”¨çš„å§¿å‹¢ä¼°è¨ˆæ¨¡åž‹çš„ IDã€‚é€™å€‹ ID æŒ‡å‘ Hugging Face Hub ä¸Šçš„ç‰¹å®šæ¨¡åž‹ï¼Œé€™è£¡é¸æ“‡çš„æ˜¯ `vitpose-plus-large`ã€‚
        
- **`pose_estimation_processor = AutoProcessor.from_pretrained(POSE_ESTIMATION_MODEL_ID)`**
    
    - å¾ž Hugging Face Hub ä¸‹è¼‰ä¸¦è¼‰å…¥èˆ‡æ¨¡åž‹ ID ç›¸æ‡‰çš„è³‡æ–™é è™•ç†å™¨ã€‚é€™å€‹è™•ç†å™¨è² è²¬å°‡åŽŸå§‹å½±åƒï¼ˆä¾‹å¦‚ NumPy é™£åˆ—ï¼‰è½‰æ›æˆæ¨¡åž‹å¯ä»¥ç†è§£çš„æ ¼å¼ï¼ˆä¾‹å¦‚ï¼Œèª¿æ•´å¤§å°ã€æ­£è¦åŒ–ç­‰ï¼‰ã€‚
        
- **`pose_estimation_model = VitPoseForPoseEstimation.from_pretrained(...)`**
    
    - å¾ž Hugging Face Hub ä¸‹è¼‰ä¸¦è¼‰å…¥æŒ‡å®šçš„å§¿å‹¢ä¼°è¨ˆæ¨¡åž‹ã€‚
        
    - `device_map=DEVICE`: å‘Šè¨´ `transformers` å‡½å¼åº«å°‡æ¨¡åž‹è¼‰å…¥åˆ°æˆ‘å€‘ä¹‹å‰ç¢ºå®šçš„ `DEVICE`ï¼ˆGPU æˆ– CPUï¼‰ä¸Šã€‚
        

ç¸½çµä¾†èªªï¼Œé€™å€‹ cell è¼‰å…¥äº†éª¨éª¼æª¢æ¸¬ï¼ˆå§¿å‹¢ä¼°è¨ˆï¼‰æ¨¡åž‹ï¼Œç‚ºå¾ŒçºŒçš„è™•ç†åšæº–å‚™ã€‚


```python
import cv2
import torch
from tqdm import tqdm

PLAYER_ID = 2
CONFIDENCE_THRESHOLD = 0.3
IOU_THRESHOLD = 0.7
INTERVAL = 30

annotated_frames = []

video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)
frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)
frame = next(frame_generator)

result = PLAYER_DETECTION_MODEL.infer(frame, confidence=CONFIDENCE_THRESHOLD, iou_threshold=IOU_THRESHOLD)[0]
detections = sv.Detections.from_inference(result)
detections = detections[detections.class_id == PLAYER_ID]

XYXY = detections.xyxy
CLASS_ID = detections.class_id
TRACKE_ID = list(range(1, len(CLASS_ID) + 1))

detections = sv.Detections(
    xyxy=XYXY,
    tracker_id=np.array(TRACKE_ID)
)

with torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):
    predictor.load_first_frame(frame)

    for xyxy, tracker_id in zip(XYXY, TRACKE_ID):
        xyxy = np.array([xyxy])

        _, object_ids, mask_logits = predictor.add_new_prompt(
            frame_idx=0,
            obj_id=tracker_id,
            bbox=xyxy
        )

for index, frame in tqdm(enumerate(frame_generator)):

    with torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):
        tracker_ids, mask_logits = predictor.track(frame)
        tracker_ids = np.array(tracker_ids)
        masks = (mask_logits > 0.0).cpu().numpy()
        masks = np.squeeze(masks).astype(bool)

        masks = np.array([
            filter_segments_by_distance(mask, distance_threshold=300)
            for mask
            in masks
        ])

        detections = sv.Detections(
            xyxy=sv.mask_to_xyxy(masks=masks),
            mask=masks,
            tracker_id=tracker_ids
        )

    if index % INTERVAL == 0:

        boxes = sv.xyxy_to_xywh(detections.xyxy)
        inputs = pose_estimation_processor(frame, boxes=[boxes], return_tensors="pt").to(DEVICE)

        # This is MOE architecture, we should specify dataset indexes for each image in range 0..5
        inputs["dataset_index"] = torch.tensor([0], device=DEVICE)

        with torch.no_grad():
            outputs = pose_estimation_model(**inputs)

        results = pose_estimation_processor.post_process_pose_estimation(outputs, boxes=[boxes])
        key_points = sv.KeyPoints.from_transformers(results[0])

        annotated_frame = frame.copy()
        annotated_frame = plot_skeletons(key_points=key_points, tracker_ids=detections.tracker_id, frame=annotated_frame)
        annotated_frames.append(annotated_frame)
```
### **Cell 3: ä¸»è¦è™•ç†è¿´åœˆ**

é€™æ˜¯æ•´å€‹å°ˆæ¡ˆçš„æ ¸å¿ƒéƒ¨åˆ†ï¼Œè² è²¬å½±ç‰‡çš„è®€å–ã€äººç‰©è¿½è¹¤ã€éª¨éª¼ä¼°è¨ˆå’Œçµæžœå„²å­˜ã€‚

**ç¨‹å¼ç¢¼åˆ†ç‚ºå¹¾å€‹ä¸»è¦å€å¡Šï¼š**

**1. åˆå§‹åŒ–èˆ‡ç¬¬ä¸€å¹€è™•ç†**

- **`PLAYER_ID = 2`**: å®šç¾©äººç‰©é¡žåˆ¥çš„ IDã€‚é€™é€šå¸¸æ˜¯æ ¹æ“šæ¨¡åž‹è¨“ç·´æ™‚çš„é¡žåˆ¥å°æ‡‰è¡¨æ±ºå®šçš„ï¼Œé€™è£¡ `2` å¯èƒ½ä»£è¡¨ 'person'ã€‚
    
- **`CONFIDENCE_THRESHOLD = 0.3`, `IOU_THRESHOLD = 0.7`**: è¨­å®šç‰©ä»¶åµæ¸¬çš„ç½®ä¿¡åº¦é–¾å€¼å’Œäº¤é›†æ¯”ï¼ˆIoUï¼‰é–¾å€¼ï¼Œç”¨æ–¼éŽæ¿¾ä½Žè³ªé‡çš„åµæ¸¬çµæžœã€‚
    
- **`INTERVAL = 30`**: è¨­å®šä¸€å€‹é–“éš”ï¼Œè¡¨ç¤ºæ¯éš” 30 å¹€æ‰é€²è¡Œä¸€æ¬¡éª¨éª¼ä¼°è¨ˆã€‚é€™æ˜¯ä¸€å€‹å„ªåŒ–æŽªæ–½ï¼Œå› ç‚ºéª¨éª¼ä¼°è¨ˆæ¯”äººç‰©è¿½è¹¤æ›´è€—è²»è¨ˆç®—è³‡æºã€‚
    
- **`annotated_frames = []`**: å‰µå»ºä¸€å€‹ç©ºåˆ—è¡¨ï¼Œç”¨æ–¼å„²å­˜å¸¶æœ‰éª¨éª¼è¨»é‡‹çš„å½±åƒå¹€ã€‚
    
- **`video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)`**: å¾žå½±ç‰‡æª”æ¡ˆè·¯å¾‘ç²å–å½±ç‰‡çš„å…ƒæ•¸æ“šï¼ˆå¦‚å¹€çŽ‡ã€è§£æžåº¦ç­‰ï¼‰ã€‚
    
- **`frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)`**: å‰µå»ºä¸€å€‹å¹€ç”Ÿæˆå™¨ï¼Œå®ƒå…è¨±æˆ‘å€‘é€å¹€è®€å–å½±ç‰‡ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡æ€§å°‡æ•´å€‹å½±ç‰‡è¼‰å…¥è¨˜æ†¶é«”ã€‚
    
- **`frame = next(frame_generator)`**: ç²å–å½±ç‰‡çš„ç¬¬ä¸€å¹€ã€‚
    
- **`result = PLAYER_DETECTION_MODEL.infer(...)`**: ä½¿ç”¨ä¸€å€‹é å…ˆå®šç¾©å¥½çš„ `PLAYER_DETECTION_MODEL`ï¼ˆé€™å€‹æ¨¡åž‹åœ¨ä½¿ç”¨è€…æä¾›çš„ç¨‹å¼ç¢¼ä¸­æ²’æœ‰æ˜Žç¢ºå®šç¾©ï¼Œä½†æŽ¨æ¸¬æ˜¯ä¸€å€‹ç”¨æ–¼äººç‰©åµæ¸¬çš„æ¨¡åž‹ï¼Œä¾‹å¦‚ YOLOv8ï¼‰å°ç¬¬ä¸€å¹€é€²è¡Œäººç‰©åµæ¸¬ã€‚
    
- **`detections = sv.Detections.from_inference(result)`**: å°‡åµæ¸¬çµæžœè½‰æ›ç‚º `supervision` å‡½å¼åº«çš„ `Detections` ç‰©ä»¶ï¼Œé€™æ˜¯ä¸€ç¨®æ¨™æº–åŒ–çš„æ•¸æ“šæ ¼å¼ã€‚
    
- **`detections = detections[detections.class_id == PLAYER_ID]`**: éŽæ¿¾åµæ¸¬çµæžœï¼Œåªä¿ç•™é¡žåˆ¥ ID ç‚º `PLAYER_ID`ï¼ˆå³äººç‰©ï¼‰çš„åµæ¸¬çµæžœã€‚
    
- **`XYXY = detections.xyxy`, `CLASS_ID = detections.class_id`**: æå–é‚Šç•Œæ¡†åº§æ¨™å’Œé¡žåˆ¥ IDã€‚
    
- **`TRACKE_ID = list(range(1, len(CLASS_ID) + 1))`**: ç‚ºç¬¬ä¸€å¹€ä¸­çš„æ¯å€‹äººç‰©æ‰‹å‹•åˆ†é…ä¸€å€‹å”¯ä¸€çš„è¿½è¹¤ IDï¼Œå¾ž 1 é–‹å§‹ã€‚
    
- **`detections = sv.Detections(...)`**: é‡æ–°å‰µå»º `Detections` ç‰©ä»¶ï¼Œä¸¦åŠ å…¥æˆ‘å€‘æ‰‹å‹•åˆ†é…çš„ `tracker_id`ã€‚
    

**2. å½±ç‰‡è¿½è¹¤å™¨åˆå§‹åŒ–**

- **`with torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):`**: é€™æ˜¯ä¸€å€‹ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç”¨æ–¼å„ªåŒ–æ¨¡åž‹æŽ¨è«–ã€‚
    
    - `torch.inference_mode()`: ç¦ç”¨æ¢¯åº¦è¨ˆç®—ï¼Œé€™å¯ä»¥æ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨ä¸¦åŠ é€Ÿé‹ç®—ï¼Œå› ç‚ºæˆ‘å€‘åªé€²è¡ŒæŽ¨è«–è€Œä¸æ˜¯è¨“ç·´ã€‚
        
    - `torch.autocast("cuda", dtype=torch.bfloat16)`: ä½¿ç”¨è‡ªå‹•æ··åˆç²¾åº¦ï¼ˆAMPï¼‰ä¾†é€²è¡Œé‹ç®—ï¼Œé€™å¯ä»¥æé«˜åœ¨æ”¯æ´çš„ GPU ä¸Šçš„é‹ç®—é€Ÿåº¦ï¼ŒåŒæ™‚ä¿æŒæ¨¡åž‹çš„æº–ç¢ºæ€§ã€‚
        
- **`predictor.load_first_frame(frame)`**: é€™æ˜¯ä¸€å€‹å½±ç‰‡è¿½è¹¤å™¨ï¼ˆæŽ¨æ¸¬æ˜¯ `segment-anything` æˆ–é¡žä¼¼æ¨¡åž‹çš„è¿½è¹¤å™¨ï¼‰çš„æ–¹æ³•ï¼Œç”¨æ–¼è¼‰å…¥å½±ç‰‡çš„ç¬¬ä¸€å¹€ä½œç‚ºè¿½è¹¤çš„èµ·é»žã€‚
    
- **`for xyxy, tracker_id in zip(XYXY, TRACKE_ID):`**: éæ­·ç¬¬ä¸€å¹€ä¸­æ‰€æœ‰åµæ¸¬åˆ°çš„äººç‰©ã€‚
    
- **`predictor.add_new_prompt(...)`**: å‘Šè¨´è¿½è¹¤å™¨ç¬¬ä¸€å¹€ä¸­æ¯å€‹ç›®æ¨™çš„ä½ç½®ï¼ˆé€šéŽé‚Šç•Œæ¡† `bbox`ï¼‰ï¼Œé€™æ¨£è¿½è¹¤å™¨å°±çŸ¥é“å¾žå“ªè£¡é–‹å§‹è¿½è¹¤ã€‚
    

**3. ä¸»å½±ç‰‡è™•ç†è¿´åœˆ**

- **`for index, frame in tqdm(enumerate(frame_generator)):`**: é€™æ˜¯ä¸»è¿´åœˆï¼Œéæ­·å½±ç‰‡ä¸­çš„æ¯ä¸€å¹€ã€‚`tqdm` å‡½å¼åº«ç”¨æ–¼é¡¯ç¤ºé€²åº¦æ¢ï¼Œè®“ä½¿ç”¨è€…çŸ¥é“è™•ç†é€²åº¦ã€‚
    
- **`tracker_ids, mask_logits = predictor.track(frame)`**: åœ¨æ¯ä¸€å¹€ä¸Šï¼Œä½¿ç”¨è¿½è¹¤å™¨ä¾†è¿½è¹¤ä¹‹å‰è¢«è­˜åˆ¥çš„äººç‰©ã€‚å®ƒæœƒè¿”å›žæ¯å€‹è¢«è¿½è¹¤äººç‰©çš„ ID å’Œä¸€å€‹è¡¨ç¤ºå…¶åˆ†å‰²æŽ©ç¢¼çš„ logitsã€‚
    
- **`masks = (mask_logits > 0.0).cpu().numpy()`**: å°‡ logits è½‰æ›ç‚ºäºŒé€²åˆ¶æŽ©ç¢¼ï¼ˆbinary masksï¼‰ã€‚`> 0.0` æ˜¯ä¸€å€‹é–¾å€¼ï¼Œç”¨æ–¼å°‡ logits è½‰æ›ç‚ºå¸ƒæž—å€¼ï¼Œç„¶å¾Œ `.cpu().numpy()` å°‡å…¶è½‰æ›ç‚ºå¯ä¾› CPU è™•ç†çš„ NumPy é™£åˆ—ã€‚
    
- **`masks = np.array([filter_segments_by_distance(...) for mask in masks])`**: é€™éƒ¨åˆ†ç¨‹å¼ç¢¼ç”¨æ–¼å„ªåŒ–ã€‚å®ƒå¯èƒ½æ˜¯ä¸€å€‹è‡ªå®šç¾©å‡½å¼ `filter_segments_by_distance`ï¼Œç”¨æ–¼ç§»é™¤ä¸€äº›è·é›¢è¼ƒé æˆ–ç„¡é—œç·Šè¦çš„åˆ†å‰²å€åŸŸï¼Œä»¥æé«˜æ•ˆçŽ‡æˆ–æº–ç¢ºæ€§ã€‚
    
- **`detections = sv.Detections(...)`**: æ ¹æ“šè¿½è¹¤å™¨çš„çµæžœé‡æ–°å‰µå»º `Detections` ç‰©ä»¶ï¼Œé€™æ¬¡åŒ…å«äº†åˆ†å‰²æŽ©ç¢¼å’Œè¿½è¹¤ IDã€‚
    

**4. éª¨éª¼ä¼°è¨ˆå’Œç¹ªè£½å€å¡Š**

- **`if index % INTERVAL == 0:`**: æª¢æŸ¥ç•¶å‰å¹€çš„ç´¢å¼•æ˜¯å¦æ˜¯ `INTERVAL` çš„å€æ•¸ã€‚é€™å°±æ˜¯ä¹‹å‰æåˆ°çš„å„ªåŒ–ï¼Œåªåœ¨ç‰¹å®šé–“éš”çš„å¹€ä¸Šé€²è¡Œéª¨éª¼ä¼°è¨ˆã€‚
    
- **`boxes = sv.xyxy_to_xywh(detections.xyxy)`**: å°‡é‚Šç•Œæ¡†æ ¼å¼å¾ž `xyxy`ï¼ˆå·¦ä¸Šè§’å’Œå³ä¸‹è§’ï¼‰è½‰æ›ç‚º `xywh`ï¼ˆä¸­å¿ƒé»žå’Œå¯¬é«˜ï¼‰ï¼Œé€™æ˜¯å§¿å‹¢ä¼°è¨ˆæ¨¡åž‹æ‰€éœ€è¦çš„æ ¼å¼ã€‚
    
- **`inputs = pose_estimation_processor(...)`**: ä½¿ç”¨ cell 2 ä¸­è¼‰å…¥çš„ `pose_estimation_processor` ä¾†é è™•ç†ç•¶å‰å¹€å’Œé‚Šç•Œæ¡†ï¼Œå°‡å…¶è½‰æ›ç‚ºæ¨¡åž‹æ‰€éœ€çš„å¼µé‡æ ¼å¼ã€‚
    
- **`inputs["dataset_index"] = torch.tensor([0], device=DEVICE)`**: é€™æ˜¯ä¸€å€‹ç‰¹å®šæ–¼ `VitPose` æ¨¡åž‹çš„åƒæ•¸ï¼Œç”¨æ–¼æŒ‡å®šè³‡æ–™é›†ç´¢å¼•ã€‚
    
- **`with torch.no_grad(): outputs = pose_estimation_model(**inputs)`**: é€²è¡Œå§¿å‹¢ä¼°è¨ˆæŽ¨è«–ã€‚`torch.no_grad()` å†æ¬¡ç¢ºä¿ä¸è¨ˆç®—æ¢¯åº¦ï¼Œä»¥ç¯€çœè¨˜æ†¶é«”å’Œæ™‚é–“ã€‚
    
- **`results = pose_estimation_processor.post_process_pose_estimation(...)`**: å°æ¨¡åž‹çš„è¼¸å‡ºé€²è¡Œå¾Œè™•ç†ï¼Œå°‡åŽŸå§‹è¼¸å‡ºè½‰æ›ç‚ºäººé¡žå¯è®€çš„é—œéµé»žæ ¼å¼ã€‚
    
- **`key_points = sv.KeyPoints.from_transformers(results[0])`**: å°‡å¾Œè™•ç†çµæžœè½‰æ›ç‚º `supervision` çš„ `KeyPoints` ç‰©ä»¶ã€‚
    
- **`annotated_frame = frame.copy()`**: å‰µå»ºç•¶å‰å¹€çš„å‰¯æœ¬ï¼Œä»¥é¿å…ç›´æŽ¥ä¿®æ”¹åŽŸå§‹å¹€ã€‚
    
- **`annotated_frame = plot_skeletons(...)`**: å‘¼å« cell 1 ä¸­å®šç¾©çš„å‡½å¼ï¼Œå°‡é—œéµé»žæ•¸æ“šç¹ªè£½åˆ°è¤‡è£½çš„å¹€ä¸Šã€‚
    
- **`annotated_frames.append(annotated_frame)`**: å°‡è™•ç†å¥½çš„è¨»é‡‹å¹€æ·»åŠ åˆ°åˆ—è¡¨ä¸­ï¼Œä»¥ä¾¿å¾ŒçºŒé¡¯ç¤ºæˆ–å„²å­˜ã€‚
    

ç¸½çµä¾†èªªï¼Œcell 3 æ˜¯ä¸€å€‹å®Œæ•´çš„å½±ç‰‡è™•ç†æµæ°´ç·šï¼Œå®ƒé¦–å…ˆåµæ¸¬ä¸¦åˆå§‹åŒ–è¿½è¹¤ç›®æ¨™ï¼Œç„¶å¾Œåœ¨æ¯ä¸€å¹€ä¸Šè¿½è¹¤é€™äº›ç›®æ¨™ï¼Œä¸¦åœ¨ç‰¹å®šçš„å¹€ä¸Šé€²è¡Œæ›´è€—æ™‚çš„éª¨éª¼ä¼°è¨ˆï¼Œæœ€å¾Œå°‡çµæžœè¦–è¦ºåŒ–ä¸¦å„²å­˜èµ·ä¾†ã€‚


```python
images = annotated_frames[:5]
titles = [f"frame {index * INTERVAL}" for index in range(0, len(images))]

sv.plot_images_grid(
    images=images,
    grid_size=(5, 1),
    size=(5, 15),
    titles=titles
)
```
### **Cell 4: çµæžœè¦–è¦ºåŒ–**

é€™å€‹ cell è² è²¬å°‡è™•ç†å¥½çš„çµæžœé¡¯ç¤ºçµ¦ä½¿ç”¨è€…ã€‚

- **`images = annotated_frames[:5]`**: å¾žå„²å­˜çš„è¨»é‡‹å¹€åˆ—è¡¨ä¸­ï¼Œå–å‡ºå‰ 5 å¹€ã€‚
    
- **`titles = [f"frame {index * INTERVAL}" for index in range(0, len(images))]`**: å‰µå»ºä¸€å€‹æ¨™é¡Œåˆ—è¡¨ï¼Œç”¨æ–¼æ¨™è¨»æ¯å¼µå½±åƒå°æ‡‰çš„åŽŸå§‹å¹€è™Ÿã€‚ç”±æ–¼æˆ‘å€‘æ˜¯æ¯éš” `INTERVAL` å¹€æ‰è™•ç†ä¸€æ¬¡ï¼Œæ‰€ä»¥æ¨™é¡Œæœƒæ˜¯ `frame 0`, `frame 30`, `frame 60` ç­‰ã€‚
    
- **`sv.plot_images_grid(...)`**: å‘¼å« `supervision` å‡½å¼åº«ä¸­çš„å‡½å¼ï¼Œä»¥ç¶²æ ¼å½¢å¼é¡¯ç¤ºå¤šå¼µå½±åƒã€‚
    
    - `images=images`: å‚³å…¥è¦é¡¯ç¤ºçš„å½±åƒåˆ—è¡¨ã€‚
        
    - `grid_size=(5, 1)`: æŒ‡å®šç¶²æ ¼çš„å¤§å°ç‚º 5 è¡Œ 1 åˆ—ï¼Œé€™å°‡åž‚ç›´æŽ’åˆ—å½±åƒã€‚
        
    - `size=(5, 15)`: è¨­å®šç¸½é¡¯ç¤ºå€åŸŸçš„å¤§å°ã€‚
        
    - `titles=titles`: å‚³å…¥ä¹‹å‰å‰µå»ºçš„æ¨™é¡Œåˆ—è¡¨ï¼Œç‚ºæ¯å¼µå½±åƒæ·»åŠ æ¨™é¡Œã€‚
        

ç¸½çµä¾†èªªï¼Œé€™å€‹ cell æ˜¯ä¸€å€‹ç°¡å–®ä½†æœ‰æ•ˆçš„è¦–è¦ºåŒ–å·¥å…·ï¼Œç”¨æ–¼å¿«é€Ÿæª¢æŸ¥éª¨éª¼ä¼°è¨ˆçš„çµæžœï¼Œé€šå¸¸æœƒç”¨ä¾†é©—è­‰æ•´å€‹è™•ç†æµç¨‹æ˜¯å¦æ­£ç¢ºã€‚


```python
def plot_ankles(key_points: sv.KeyPoints, tracker_ids: list[int], frame: np.array):
    key_points = key_points[:, [15, 16]]
    h, w = frame.shape[:2]
    text_scale = sv.calculate_optimal_text_scale(resolution_wh=(w, h))
    for tracker_id in tracker_ids:
        tracker_key_points = key_points[int(tracker_id) - 1]
        annotator = sv.VertexLabelAnnotator(
            color=COLOR.by_idx(tracker_id),
            text_color=sv.Color.BLACK,
            text_scale=text_scale,
            border_radius=2
        )
        frame = annotator.annotate(scene=frame, key_points=tracker_key_points, labels=["L", "R"])
    return frame
```



```python
import cv2
import torch
from tqdm import tqdm

PLAYER_ID = 2
CONFIDENCE_THRESHOLD = 0.3
IOU_THRESHOLD = 0.7
INTERVAL = 30

annotated_frames = []

video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)
frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)
frame = next(frame_generator)

result = PLAYER_DETECTION_MODEL.infer(frame, confidence=CONFIDENCE_THRESHOLD, iou_threshold=IOU_THRESHOLD)[0]
detections = sv.Detections.from_inference(result)
detections = detections[detections.class_id == PLAYER_ID]

XYXY = detections.xyxy
CLASS_ID = detections.class_id
TRACKE_ID = list(range(1, len(CLASS_ID) + 1))

detections = sv.Detections(
    xyxy=XYXY,
    tracker_id=np.array(TRACKE_ID)
)

with torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):
    predictor.load_first_frame(frame)

    for xyxy, tracker_id in zip(XYXY, TRACKE_ID):
        xyxy = np.array([xyxy])

        _, object_ids, mask_logits = predictor.add_new_prompt(
            frame_idx=0,
            obj_id=tracker_id,
            bbox=xyxy
        )

for index, frame in tqdm(enumerate(frame_generator)):

    with torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):
        tracker_ids, mask_logits = predictor.track(frame)
        tracker_ids = np.array(tracker_ids)
        masks = (mask_logits > 0.0).cpu().numpy()
        masks = np.squeeze(masks).astype(bool)

        masks = np.array([
            filter_segments_by_distance(mask, distance_threshold=300)
            for mask
            in masks
        ])

        detections = sv.Detections(
            xyxy=sv.mask_to_xyxy(masks=masks),
            mask=masks,
            tracker_id=tracker_ids
        )

    if index % INTERVAL == 0:

        boxes = sv.xyxy_to_xywh(detections.xyxy)
        inputs = pose_estimation_processor(frame, boxes=[boxes], return_tensors="pt").to(DEVICE)

        # This is MOE architecture, we should specify dataset indexes for each image in range 0..5
        inputs["dataset_index"] = torch.tensor([0], device=DEVICE)

        with torch.no_grad():
            outputs = pose_estimation_model(**inputs)

        results = pose_estimation_processor.post_process_pose_estimation(outputs, boxes=[boxes])
        key_points = sv.KeyPoints.from_transformers(results[0])

        annotated_frame = frame.copy()
        annotated_frame = plot_ankles(key_points=key_points, tracker_ids=detections.tracker_id, frame=annotated_frame)
        annotated_frames.append(annotated_frame)
```



```python
images = annotated_frames[:5]
titles = [f"frame {index * INTERVAL}" for index in range(0, len(images))]

sv.plot_images_grid(
    images=images,
    grid_size=(5, 1),
    size=(5, 15),
    titles=titles
)

```



```python

```


