
Project Colab:
https://colab.research.google.com/drive/1akylaaDxSBU9grLk-iw2tVs56Yti7guv

|                                   |     |
| --------------------------------- | --- |
|                                   |     |
|                                   |     |
| [[#### Automatic mask generator]] |     |












#### Automatic mask generator

# 增強自動化分割：結合 DINOv3 語意特徵與 SAM2 遮罩能力的協同方法

## 架構基礎：對比 SAM2 的處理流程與 DINOv3 的感知能力

實現精確、全自動影像分割的挑戰，不僅在於繪製精確邊界的能力，更在於最初識別何為獨立物件的初步步驟。使用者觀察到 `SAM2AutomaticMaskGenerator` 的不準確性，源於其物件提案策略的根本限制。本節將解構此生成器的運作機制，揭示其語意無知性 (semantic agnosticism)，並介紹 DINOv3 作為一個強大的視覺骨幹網路，它能提供必要的語意敏銳度，以建立更智慧、更精確的分割流程。

### SAM2AutomaticMaskGenerator 的機制：一種基於網格的方法

Segment Anything Model 2 (SAM2) 是一個最先進的基礎模型，用於影像和影片中可提示的視覺分割 。其核心優勢在於能根據使用者提供的提示（如點、邊界框或文字）生成高品質的物件遮罩 。在所提供的程式碼中使用的  

`SAM2AutomaticMaskGenerator` 類別，是一個專為在沒有明確使用者提示的情況下自動分割整個影像而設計的特定實作 。  

此生成器的運作原理是一種系統性的、暴力破解的取樣方法 。它在輸入影像上覆蓋一個密集的點網格。這個網格的密度由  

`points_per_side` 參數控制；例如，值為 64 會產生一個 64×64=4096 個提示點的網格。對於網格中的每一個點，SAM2 模型會預測多個可能包含該點的潛在遮罩。這個過程會生成大量詳盡的候選遮罩，涵蓋影像的各個部分 。  

在生成階段之後，會進行嚴格的篩選和去重複流程。候選遮罩會根據幾個品質指標進行評估。`pred_iou_thresh` 參數會過濾掉模型自身預測的交集比 (IoU) 較低的遮罩，這表示信心不足。`stability_score_thresh` 參數則會移除那些在二值化閾值輕微擾動時會發生顯著變化的遮罩，這暗示了模糊性或不穩定性。最後，為了消除覆蓋同一物件的冗餘遮罩，會根據 `box_nms_thresh` 參數應用非極大值抑制 (NMS)，該參數比較遮罩邊界框的 IoU 。其他參數，如  

`crop_n_layers` 和 `min_mask_region_area`，則允許在處理較小物件時提升性能，並進行後處理以清理最終的遮罩 。  

這種方法的根本限制，也是觀察到不準確性的根本原因，在於其提案機制是**語意無知的**。基於網格的取樣是均勻的，且對影像的實際內容一無所知。它不具備對何為一個連貫「物件」的內在理解。這導致了幾種可預測的失敗模式：

1. **物件破碎化：** 對於大型或細長的物件，多個網格點可能落在其邊界內。由這些單獨點生成的遮罩可能只捕捉到物件的一部分（例如，椅子的腿而不是整張椅子），導致表示破碎。
    
2. **不正確的合併或分割：** 放置在兩個相鄰但不同物件邊界附近的單一點，可能會生成一個錯誤地將它們合併的遮罩。相反地，單一物件內有紋理表面上的一個點，可能會產生一個小的、局部的遮罩，從而錯誤地分割了該物件。
    
3. **超參數敏感性：** 最終輸出的品質高度依賴於眾多超參數（`points_per_side`、`stability_score_thresh`、`pred_iou_thresh` 等）之間複雜的相互作用 。為給定的影像或領域找到最佳組合，可能是一個充滿挑戰且不直觀的試誤過程。  
    

本質上，使用者的問題並非 SAM2 核心分割能力的失敗，而是其預設_提案生成_機制的失敗。`SAM2AutomaticMaskGenerator` 是一個用於詳盡遮罩發現的強大工具，但其暴力破解的特性缺乏語意智慧，無法持續識別完整、有意義的物件。因此，解決方案不是取代 SAM2，而是為其配備一種更優越、具備語意感知能力的初始物件提案生成方法。

### DINOv3：透過自我監督學習實現語意敏銳度

由 Meta AI 開發的 DINOv3 是電腦視覺領域的一個重要里程碑，它證明了自我監督學習 (SSL) 可以產生在多種任務上超越專門、有監督模型的視覺基礎模型 。與依賴人工標註標籤的模型不同，DINOv3 透過在一個前所未有規模的資料集上進行訓練——一個包含 17 億張影像的精選資料集，模型架構擴展至 70 億個參數——直接從原始像素中學習豐富的視覺表示 。  

DINOv3 的核心創新在於其產生強大、高解析度**密集特徵**的能力 。該模型（通常是 Vision Transformer, ViT）處理影像時，首先將其分割成一個不重疊的區塊網格（例如 16x16 像素）。對於每個區塊，DINOv3 骨幹網路會輸出一個高維特徵向量（例如，ViT-B 為 768 維，ViT-L 為 1024 維）。這些就是「密集特徵」，因為它們為影像的每個局部區域提供了豐富的描述。  

這些透過複雜的自我蒸餾過程學習到的特徵，其深遠的特性在於它們的相似性直接對應於影像內的語意相似性 。如果兩個區塊屬於同一個物件——例如，貓毛的兩個不同部分——它們對應的 DINOv3 特徵向量在高維特徵空間中會非常接近。這可以用餘弦相似度來衡量。相反地，來自不同物件的區塊（例如，貓毛與它所坐的草地）將具有不相似的特徵向量。這種湧現的特性使得 DINOv3 能夠在從未被明確訓練過分割遮罩或物件標籤的情況下，隱含地理解物件邊界和語意關係 。  

DINOv3 實用性的一個關鍵方面是其作為**凍結骨幹網路**的設計 。在密集的自我監督預訓練之後，模型的權重保持固定。這個凍結的骨幹網路可以「開箱即用」，為眾多下游任務提取高品質特徵，包括物件偵測、語意分割和深度估計 。通常只需在這些凍結的特徵之上訓練一個輕量級的適配器或一個簡單的線性分類器，即可達到最先進的性能，從而大大減少了對大量特定任務微調和標註資料的需求 。  

這種「凍結骨幹網路」的理念完美地適用於解決 `SAM2AutomaticMaskGenerator` 的局限性。與其依賴一個語意盲目的點網格，不如利用 DINOv3 豐富、密集的特徵，首先識別影像中語意連貫的區域。這些對應於潛在物件的區域，可以用來生成高品質的提案（例如，邊界框），然後傳遞給 SAM2 進行精確分割。這將任務從暴力破解搜索轉變為一個智慧的兩階段過程：首先，用 DINOv3 感知物件，然後，用 SAM2 分割它們。

## 建議流程：從無監督發現到精準遮罩

為了克服基於網格方法的局限性，我們提出了一個新的、更複雜的流程。此架構利用 DINOv3 和 SAM2 的獨特優勢，創建了一個協同工作流程，將語意理解的任務與精確遮罩生成的任務分開。這產生了一個強大的、完全無監督的系統，能夠進行零樣本物件發現和分割。

### 新範式：語意提案生成

核心策略是用一個多步驟流程取代 `SAM2AutomaticMaskGenerator`，該流程使用 DINOv3 生成具有語意意義的物件提案，然後將這些提案輸入 SAM2 的基於提示的預測器進行最終分割。此流程包含四個不同階段：

1. **特徵提取：** 輸入影像首先通過一個凍結的 DINOv3 Vision Transformer (ViT) 骨幹網路。此步驟計算整個影像的密集區塊特徵，將視覺資訊轉換為一個編碼了語意關係的高維空間 。  
    
2. **無監督物件發現：** 提取出的區塊特徵接著被送入一個無監督聚類演算法。此過程根據特徵的相似性對區塊特徵進行分群。由於 DINOv3 中的特徵相似性對應於語意相似性，因此產生的群集代表了影像中語意連貫的區域，有效地在沒有任何先驗標籤或監督的情況下「發現」場景中的物件 。  
    
3. **邊界框提案：** 對於上一步中識別出的每個區塊群集，計算一個緊密的邊界框，以包含該群集中的所有區塊。這些邊界框並非任意的；它們直接源自語意分組的區域，因此可作為高品質、物件感知的提示。
    
4. **基於提示的分割：** 最後，將這些生成的邊界框作為提示傳遞給 `SAM2ImagePredictor`。SAM2 框架的這個組件專門設計用於接收像點或框這樣的提示，並生成精確、高保真度的分割遮罩 。  
    

這個流程設計非常有效，因為它利用了每個模型的互補優勢。DINOv3 憑藉其從 17 億張影像中提煉出的廣泛知識，提供了對場景的全局、語意理解，確保發送給 SAM2 的提案已經與真實的物件邊界高度對齊 。而 SAM2 則應用其強大高效的遮罩解碼器，將這些邊界框提案精煉成像素級完美的分割，這是它所擅長的任務 。這種方法在概念上類似於已建立的流程，如 Grounded-SAM，它將一個由文字提示的物件偵測器與 SAM 結合 。然而，所提出的方法是獨特的，並且可以說更符合「自動」生成的目標，因為它使用完全  

_無監督_的方法來生成提案，不需要文字或任何其他手動輸入。

這從兩個基礎模型中創建了一個強大的、零樣本的物件偵測器和分割器。雖然 DINOv3 的研究強調了其與輕量級、_經過訓練_的適配器一起用於正式的物件偵測任務 ，但這個流程表明，其特徵的語意豐富度如此之高，以至於即使是一個簡單、無需訓練的聚類演算法，也能作為下游分割模型的合格物件提議器。這種模組化設計，將語意理解與最終的分割任務解耦，是現代電腦視覺工程中一個強大且可推廣的模式。  

### 技術深入探討：透過特徵聚類生成物件提案

無監督提案生成階段的實作涉及幾個關鍵技術步驟，這些步驟將 DINOv3 模型的原始輸出轉換為一組可供 SAM2 使用的可操作邊界框提示。

#### 特徵提取與重塑

當一張影像通過 DINOv3 ViT 模型時，輸出是一系列特徵向量或「標記 (tokens)」。這些包括用於全局影像表示的 `` 標記、幾個有助於捕捉全局上下文的「註冊 (register)」標記，以及對此任務最重要的，一系列的區塊標記——每個對應於輸入影像的一個 16x16 區塊 。第一步是從模型的輸出中分離出這些區塊標記。如果輸入影像的大小為  

H×W，它會被分割成 (H/16)×(W/16) 個區塊。然後，區塊標記被重塑為一個大小為 (H/16)×(W/16)×D 的二維特徵圖，其中 D 是特徵維度（例如，ViT-B 為 768）。這個特徵圖提供了一個低解析度但語意豐富的影像表示。  

#### 使用 PCA 進行降維

原始的 DINOv3 特徵向量是高維的，這可能因「維度災難」和計算開銷而對聚類演算法構成挑戰。為了解決這個問題，我們對特徵向量應用了主成分分析 (PCA)。PCA 是一種標準的降維技術，它識別出主成分——資料中變異最大的方向——並將資料投影到由這些成分構成的較低維度子空間上。這個過程有效地壓縮了特徵向量，同時保留了最重要的語意資訊。對於 DINOv3 特徵，將維度降至一個較小的數值（例如 6 到 16）通常足以捕捉物件聚類所需的主要語意差異，同時減少雜訊並提高後續聚類步驟的效率。

#### K-Means 聚類

利用降維後的特徵向量，我們採用 K-Means 聚類演算法對影像區塊進行分群。K-Means 是一種迭代演算法，它將 N 個區塊特徵劃分為 k 個不同且不重疊的群集。它旨在透過將每個特徵向量分配給最近的群集中心，來最小化每個群集內的變異。在這種情況下，k 個群集中的每一個都將對應於影像的一個語意上不同的區域。超參數 k 直接控制演算法將嘗試發現的物件或不同區域的數量。

#### 從群集到邊界框

提案生成過程的最後一步是將群集分配轉換為邊界框。對於 k 個群集中的每一個，我們識別出所有被分配到該群集的影像區塊。由於每個區塊在 (H/16)×(W/16) 網格中都有已知的位置，因此很容易確定群集的空間範圍。我們找到給定群集內區塊的最小和最大行和列索引。然後將這些索引乘以區塊大小（16 像素），以計算原始影像像素空間中邊界框的左上角 (xmin​,ymin​) 和右下角 (xmax​,ymax​) 座標。這個過程產生一個包含 k 個邊界框的列表，每個邊界框對應一個發現的語意區域，準備好以 `[x_min, y_min, x_max, y_max]` 的格式傳遞給 SAM2 。  

## 實作：修正並擴充的 Colab 儲存格

本節提供了一個完整、獨立且可執行的解決方案，用於在 Google Colab 環境中實作 DINOv3 引導的分割流程。程式碼結構清晰、註解詳盡且可直接使用，以更強大、更準確的方法取代使用者原有的儲存格。

### 環境設定與模型實例化

在執行主流程之前，必須安裝並載入必要的函式庫和模型。此設定確保 DINOv3 和 SAM2 的所有相依性都已正確配置。

#### 先決條件

應在 Colab 儲存格中執行以下指令來安裝所需的 Python 套件。`sam2` 函式庫直接從其官方 GitHub 儲存庫安裝，而 `transformers` 則提供從 Hugging Face Hub 存取 DINOv3 模型的功能。`timm` 是某些模型架構的相依項，而 `scikit-learn` 則用於 PCA 和 K-Means 演算法。

#### 模型載入

環境準備就緒後，下一步是實例化 DINOv3 和 SAM2 模型。對於 DINOv3，此範例使用 `dinov3_vitb16` 模型，它在性能和計算需求之間提供了很好的平衡 。它使用  

`transformers` 函式庫從 Hugging Face Hub 載入，這簡化了下載和快取模型權重的過程 。  

對於 SAM2，我們使用 `SAM2ImagePredictor`，因為它是為基於提示的分割而設計的。我們使用從複製的儲存庫中的 `build_sam2` 輔助函式，從設定檔和預訓練的檢查點來建構模型，檢查點會自動下載 。  

### 帶註解的程式碼解決方案

以下程式碼儲存格包含了所提議流程的完整、端到端的實作。它整合了所有步驟，從影像載入和特徵提取到提案生成、遮罩預測和最終視覺化。每個邏輯區塊都附有詳細的註解，解釋其目的。

### 微調與控制：關鍵參數指南

所提議的流程引入了一套新的、直觀的、高層次的控制項，取代了 `SAM2AutomaticMaskGenerator` 中更細微且較不直觀的參數。理解這些新參數是將流程應用於不同影像和需求的關鍵。

#### DINOv3 模型選擇

DINOv3 骨幹網路的選擇是第一個主要決策。DINOv3 家族包括幾種不同大小的模型，主要是 Vision Transformers (ViT)，如 ViT-S (Small)、ViT-B (Base)、ViT-L (Large) 和更大的變體 。權衡很直接：  

- **較大的模型 (例如 `dinov3_vitl16`)：** 這些模型在更多資料上用更多參數進行訓練，從而產生更強大、更細膩的特徵表示。它們通常更擅長區分細微的語意差異，並可能產生更準確的物件提案。然而，它們需要更多的 GPU 記憶體並具有更高的推論延遲。
    
- **較小的模型 (例如 `dinov3_vits16`)：** 這些模型速度更快、記憶體效率更高，使其適用於資源限制更嚴格的應用。雖然它們的特徵仍然非常強大，但可能無法捕捉到與較大模型相同程度的細節。
    

對於在 Colab 環境中的一般用途，`dinov3_vitb16` 或 `dinov3_vitl16` 是絕佳的起點，它們在性能和效率之間提供了穩健的平衡。

#### 聚類參數 (`n_clusters`)

在這個新流程中，最關鍵的超參數是 K-Means 演算法的 `n_clusters`（通常表示為 k）。這個整數值直接指定了流程應在影像中嘗試發現的獨立語意區域（即物件和背景區域）的數量。

- **設定 `n_clusters`：** 如果您大致知道場景中有多少個物件，可以將 k 設定為該數字（加上一或兩個以考慮不同的背景區域）。對於有兩隻斑馬和一個背景的範例影像，設定 k=3 或 k=4 會是合適的。
    
- **`n_clusters` 的影響：**
    
    - 如果 k 太低，演算法可能會將多個不同的物件歸為一個群集，導致一個包含所有物件的大邊界框。
        
    - 如果 k 太高，單一物件可能會根據紋理或光線的細微變化被分割成多個群集，導致破碎的提案。 實驗這個單一、直觀的參數是調整整個流程分割輸出的主要方法。
        

#### PCA 維度 (`n_pca_components`)

PCA 後保留的主成分數量是另一個調整參數，儘管其影響通常不如 `n_clusters` 直接。此參數控制在聚類前對 DINOv3 特徵應用的降維程度。介於 6 和 16 之間的值通常是有效的。保留太少的主成分可能會合併不同的語意資訊，而保留太多則可能保留可能混淆聚類演算法的雜訊。所提供程式碼中的預設值 8 是一個穩健的起點。

## 分析與進階觀點

從基於網格的取樣方法轉向 DINOv3 引導的語意提案流程，代表了自動分割方法的一個根本性改進。本節提供了兩種方法的比較分析，討論了固有的權衡，並探討了更進階的應用和未來方向。

### 比較分析

直接比較揭示了將語意理解整合到分割流程中的深遠影響。

#### 定性討論

當應用於典型影像時，原始的 `SAM2AutomaticMaskGenerator` 通常會產生大量遮罩，其中許多是較大物件的微小、破碎片段或冗餘的重疊。最終結果可能看起來雜亂，且缺乏連貫的物件級結構。相比之下，DINOv3 引導的流程產生了更乾淨、更易於解釋的輸出。因為該過程是從發現語意上完整的區域開始的，所以最終的遮罩直接對應於場景中的主要物件。它有效地避免了基於網格方法固有的破碎和過度分割問題，產生了更完整且更符合人類對場景感知的遮罩。

#### 定量討論與權衡

雖然完整的基準測試超出了本報告的範圍，但所提議流程的預期定量性能顯著更高。當與地面真實分割遮罩進行評估時，DINOv3 引導的方法幾乎肯定會達到更高的平均交集比 (mIoU) 分數。這是因為其提案是基於語意的，從而產生能更好地捕捉每個物件完整範圍的遮罩。

這種準確性的提升帶來了明顯的權衡：計算成本。新的流程引入了兩個額外的計算步驟：一次通過 DINOv3 模型的正向傳播和 PCA/K-Means 聚類過程。DINOv3 的正向傳播，特別是使用較大的 ViT 模型時，是一個不可忽視的操作，會增加延遲並提高 GPU 記憶體需求。原始的 `SAM2AutomaticMaskGenerator` 僅涉及單一模型，因此速度更快。兩種方法之間的選擇取決於特定應用的優先順序：對於需要最高語意準確性的任務，DINOv3 流程更優越；對於速度至上且可接受一定程度破碎的應用，原始生成器可能就足夠了。

下表總結了兩種方法之間的主要差異。

|特性|原始方法：`SAM2AutomaticMaskGenerator`|建議方法：DINOv3 提案 + `SAM2ImagePredictor`|
|---|---|---|
|**提案機制**|密集的、均勻的點提示網格（語意無知）。|對語意特徵進行無監督聚類以尋找物件區域。|
|**語意感知**|低。依賴於取樣點的局部影像特徵。|高。利用 DINOv3 的全局上下文和區塊級語意關係。|
|**使用的 SAM2 組件**|`SAM2AutomaticMaskGenerator`|`SAM2ImagePredictor`|
|**主要輸出**|影像中找到的所有潛在遮罩的詳盡列表。|直接對應於語意上不同物件提案的遮罩。|
|**計算成本**|較低（單一模型傳播）。|較高（增加了 DINOv3 正向傳播和聚類的開銷）。|
|**關鍵控制參數**|`points_per_side`、`stability_score_thresh`、`pred_iou_thresh`。|DINOv3 模型大小、群集數量 (`k`)、PCA 維度。|

匯出到試算表

### 替代方法與未來工作

所提議的流程是一個強大的通用工具，但其模組化設計允許進一步的專業化和擴展。在所呈現的無監督聚類方法和更專業的偵測器頭之間做選擇，不僅是技術上的，也是策略上的。聚類方法非常通用，能夠在「野外」發現和分割新穎或不常見的物件，這與「分割一切」的理念完美契合。相比之下，偵測器頭方法是一種專門工具，針對一組已知的物件類別在速度和準確性上進行了優化。理解這種區別，可以讓從業者為其特定應用選擇最佳的提案引擎。

#### 使用預訓練的偵測器頭

官方的 DINOv3 儲存庫提供了預訓練的偵測器頭，例如，一個在 COCO 資料集上訓練的偵測器頭 。對於專注於常見物件類別（例如，人、車、動物）的應用，用這個專門的偵測器頭取代無監督聚類模組可能會帶來顯著的優勢。這種方法（DINOv3 骨幹網路 + 偵測器頭 → 邊界框 → SAM2 預測器）對於領域內的物件可能會比聚類更快，且可能更準確，因為偵測器頭是為定位任務明確訓練的。  

#### 基於特徵的遮罩精煉

DINOv3 特徵不僅可用於初始提案生成。一種進階技術是利用這些特徵對 SAM2 生成的遮罩進行後處理和精煉。例如，SAM2 可能偶爾在視覺複雜的區域產生一個小的、虛假的遮罩。可以通過檢查其邊界內底層 DINOv3 特徵的一致性來驗證此遮罩。如果特徵高度不一致，則可以將該遮罩視為可能的雜訊而丟棄，從而獲得更乾淨的最終輸出。

#### 擴展至影片分割

SAM2 和 DINOv3 都具備處理影片資料的強大能力。SAM2 是專為影片設計的，包含一個記憶機制以跨影格追蹤物件 。DINOv3 特徵也已被證明對影片分割追蹤有效 。所提議的流程可以自然地擴展到影片。一個穩健的方法是在一個關鍵影格上運行基於 DINOv3 的提案生成，以識別初始感興趣的物件。然後，可以將 SAM2 產生的遮罩反饋給 SAM2 的影片預測器，該預測器將利用其時間傳播和追蹤能力，在影片序列的其餘部分分割這些物件。這結合了在靜態影格上進行無監督發現的優勢與高效、最先進的追蹤能力。  

資料來源和相關內容

[

](https://arxiv.org/abs/2503.00042)

