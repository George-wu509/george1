
![[Pasted image 20250807001636.png]]

Reference: 
https://www.linkedin.com/in/skalskip92/recent-activity/all/

EdgeTAM

EdgeTAM: Real-Time On-Device Video Segmentation ðŸ”¥ ðŸ”¥ ðŸ”¥  
  
Developed by [AI at Meta](https://www.linkedin.com/company/aiatmeta/), EdgeTAM lets you run high-quality video object segmentation and tracking at up to 16 FPS right on an iPhone 15 Pro Max.  
  
Previous models like SAM 2 delivered great segmentation and tracking, but were too slow for mobile (only 1 FPS) due to a heavy memory attention module. EdgeTAM uses a 2D Spatial Perceiver, a lightweight solution that compresses and structures memory features, cutting compute and keeping accuracy high.  
  
EdgeTAM matches or outperforms state-of-the-art models on tough video and image segmentation benchmarks, while running 22 times faster on edge hardware.  
  
EdgeTAM supports the same prompts as SAM 2. You can click in a video, draw a box, or mix prompt types.  
  
â®‘ ðŸ”— top CVPR 2025 papers: [https://lnkd.in/dbNRXHW2](https://lnkd.in/dbNRXHW2)  
  
Links to the paper, code and demo are in the comments below.