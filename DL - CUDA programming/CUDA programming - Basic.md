

|                               |     |
| ----------------------------- | --- |
| [[### CUDA Programming è©³ç´°ä»‹ç´¹]] |     |
| [[###CUDA Programming æ˜¯ä»€éº¼ï¼Ÿ]]  |     |
|                               |     |
|                               |     |
|                               |     |


### CUDA Programming è©³ç´°ä»‹ç´¹

CUDA (Compute Unified Device Architecture) æ˜¯ç”± NVIDIA é–‹ç™¼çš„ä¸€ç¨®ä¸¦è¡Œè¨ˆç®—å¹³å°å’Œ APIï¼Œå…è¨±é–‹ç™¼è€…åˆ©ç”¨ GPU ä¾†åŠ é€Ÿè¨ˆç®—å¯†é›†å‹æ‡‰ç”¨ã€‚CUDA ä¸»è¦åŸºæ–¼ **SIMT (Single Instruction, Multiple Thread)** æ¶æ§‹ï¼Œæ¯å€‹ GPU æ ¸å¿ƒå¯ä»¥åŒæ™‚åŸ·è¡Œç›¸åŒçš„æŒ‡ä»¤ï¼Œä½†å¯ä»¥é‡å°ä¸åŒçš„æ•¸æ“šé€²è¡Œæ“ä½œã€‚

CUDA çš„ä¸»è¦çµ„ä»¶åŒ…æ‹¬ï¼š

- **Threads (ç·šç¨‹)ï¼š** å–®å€‹ GPU æ ¸å¿ƒåŸ·è¡Œçš„æœ€å°å–®ä½ã€‚
- **Blocks (å€å¡Š)ï¼š** ä¸€çµ„ GPU ç·šç¨‹ã€‚
- **Grids (ç¶²æ ¼)ï¼š** ä¸€çµ„ Blocksï¼Œå®šç¾©äº† CUDA æ ¸å¿ƒçš„é‹è¡Œç¯„åœã€‚
- **Shared Memory (å…±äº«è¨˜æ†¶é«”)ï¼š** å¯åœ¨åŒä¸€å€‹ Block å…§å…±äº«æ•¸æ“šçš„è¨˜æ†¶é«”å€åŸŸï¼Œæ¯” Global Memory æ›´å¿«ã€‚
- **Global Memory (å…¨åŸŸè¨˜æ†¶é«”)ï¼š** æ‰€æœ‰ç·šç¨‹éƒ½èƒ½è¨ªå•çš„è¨˜æ†¶é«”ï¼Œä½†è¨ªå•é€Ÿåº¦è¼ƒæ…¢ã€‚

---

## å¦‚ä½•ä½¿ç”¨ CUDA Programming åŠ é€Ÿ AI Segmentation Model

å‡è¨­æˆ‘å€‘æœ‰ä¸€å€‹ AI Segmentation Modelï¼Œåœ¨æ¨ç†éç¨‹ä¸­ï¼Œæˆ‘å€‘å¸Œæœ›é€é CUDA æé«˜æ¨¡å‹é‹è¡Œé€Ÿåº¦ã€‚ä¸»è¦æœ‰ä»¥ä¸‹å¹¾ç¨®åŠ é€Ÿæ–¹å¼ï¼š

1. **ä½¿ç”¨ PyTorch CUDA Tensor åŠ é€Ÿæ¨ç†**
2. **ä½¿ç”¨ CUDA Kernel ç·¨å¯« GPU åŠ é€Ÿå‡½æ•¸**
3. **åˆ©ç”¨ cuDNN å’Œ TensorRT é€²ä¸€æ­¥å„ªåŒ–**

### **æ–¹æ³• 1ï¼šä½¿ç”¨ PyTorch CUDA Tensor åŠ é€Ÿæ¨ç†**

å¦‚æœä½ çš„æ¨¡å‹æ˜¯åŸºæ–¼ PyTorchï¼Œæœ€ç°¡å–®çš„æ–¹å¼å°±æ˜¯å°‡æ¨¡å‹å’Œæ•¸æ“šè½‰æ›åˆ° CUDAï¼š
```python
import torch
import torchvision.models as models

# åŠ è¼‰ AI segmentation model (ä»¥ DeepLabV3 ç‚ºä¾‹)
model = models.segmentation.deeplabv3_resnet50(pretrained=True)

# å°‡æ¨¡å‹ç§»å‹•åˆ° GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
model.eval()

# æ¸¬è©¦è¼¸å…¥åœ–åƒ
input_tensor = torch.randn(1, 3, 512, 512).to(device)  # éš¨æ©Ÿç”Ÿæˆä¸€å¼µåœ–ç‰‡
output = model(input_tensor)

# é¡¯ç¤º segmentation mask
segmentation_mask = output['out'].argmax(dim=1).squeeze().cpu().numpy()
print(segmentation_mask.shape)  # (512, 512)

```

é€™ç¨®æ–¹å¼å¯ä»¥è®“ PyTorch è‡ªå‹•å°‡é‹ç®—è½‰æ›ç‚º CUDA é‹è¡Œï¼Œé©åˆ **æ¨ç†åŠ é€Ÿ**ã€‚

---

### **æ–¹æ³• 2ï¼šä½¿ç”¨ CUDA Kernel ç·¨å¯« GPU åŠ é€Ÿå‡½æ•¸**

æœ‰æ™‚å€™ï¼Œæˆ‘å€‘éœ€è¦åœ¨ CUDA ä¸Šè‡ªå®šç¾©é‹ç®—ï¼Œä¾‹å¦‚åŠ é€Ÿå¾Œè™•ç†æˆ–æŸäº›è‡ªè¨‚é‹ç®—ã€‚é€™æ™‚å€™å¯ä»¥ä½¿ç”¨ `torch.cuda` ä¾†ç·¨å¯« CUDA æ ¸å¿ƒå‡½æ•¸ã€‚ä¾‹å¦‚ï¼Œæˆ‘å€‘å¯ä»¥ç·¨å¯«ä¸€å€‹ç°¡å–®çš„ **2D å·ç©æ“ä½œ**ï¼š
```python
import torch
import torch.nn.functional as F
from torch.utils.cpp_extension import load

# ä½¿ç”¨ CUDA Kernel é€²è¡Œå·ç©é‹ç®—
cuda_code = """
extern "C" __global__ void conv2d_cuda(float* input, float* kernel, float* output, 
                                       int H, int W, int KH, int KW) {
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;

    if (x >= H || y >= W) return;

    float sum = 0.0;
    for (int i = 0; i < KH; ++i) {
        for (int j = 0; j < KW; ++j) {
            int xi = x + i - KH / 2;
            int yj = y + j - KW / 2;
            if (xi >= 0 && xi < H && yj >= 0 && yj < W) {
                sum += input[xi * W + yj] * kernel[i * KW + j];
            }
        }
    }
    output[x * W + y] = sum;
}
"""

# ç·¨è­¯ CUDA å…§æ ¸å‡½æ•¸
cuda_module = load(name="conv2d_cuda", sources=[], extra_cuda_sources=[cuda_code])

# æ¸¬è©¦ CUDA å…§æ ¸å‡½æ•¸
H, W = 512, 512
KH, KW = 3, 3
input_tensor = torch.randn(H, W, device="cuda")
kernel = torch.ones(KH, KW, device="cuda") / (KH * KW)  # å¹³å‡æ± åŒ–
output_tensor = torch.zeros(H, W, device="cuda")

# ä½ˆå±€ CUDA åŸ·è¡Œåƒæ•¸
threads_per_block = (16, 16)
blocks_per_grid = ((H + threads_per_block[0] - 1) // threads_per_block[0],
                   (W + threads_per_block[1] - 1) // threads_per_block[1])

# èª¿ç”¨ CUDA æ ¸å¿ƒå‡½æ•¸
cuda_module.conv2d_cuda(input_tensor.data_ptr(), kernel.data_ptr(), output_tensor.data_ptr(),
                        H, W, KH, KW, block=threads_per_block, grid=blocks_per_grid)

# å–å›è¼¸å‡º
output_tensor = output_tensor.cpu().numpy()
print(output_tensor.shape)  # (512, 512)

```

é€™ç¨®æ–¹å¼é©ç”¨æ–¼ **è‡ªå®šç¾© CUDA é‹ç®—**ï¼Œå¯ä»¥åŠ é€Ÿç‰¹å®šçš„æ•¸å­¸é‹ç®—ï¼Œä¾‹å¦‚å·ç©ã€æ± åŒ–æˆ–å¾Œè™•ç†æ­¥é©Ÿã€‚

---

### **æ–¹æ³• 3ï¼šä½¿ç”¨ cuDNN å’Œ TensorRT é€²ä¸€æ­¥å„ªåŒ–**

å¦‚æœä½ å¸Œæœ›é€²ä¸€æ­¥å„ªåŒ– AI segmentation modelï¼ŒNVIDIA æä¾› **cuDNN (CUDA Deep Neural Network Library)** å’Œ **TensorRT** ä¾†åŠ é€Ÿæ¨ç†ã€‚

ä½¿ç”¨ TensorRT å¯ä»¥è®“ PyTorch æ¨¡å‹åŠ é€Ÿï¼Œä¾‹å¦‚ï¼š
```python
import torch
import torch_tensorrt

# è½‰æ› PyTorch æ¨¡å‹ç‚º TensorRT åŠ é€Ÿç‰ˆæœ¬
trt_model = torch_tensorrt.compile(model, inputs=[torch_tensorrt.Input((1, 3, 512, 512), dtype=torch.float32)], enabled_precisions={torch.float16})

# é€²è¡Œæ¨ç†
output_trt = trt_model(input_tensor)
print(output_trt['out'].shape)  # (1, 21, 512, 512) for DeepLabV3 segmentation

```

é€™ç¨®æ–¹æ³•é©ç”¨æ–¼ **ç”Ÿç”¢ç’°å¢ƒçš„ AI æ¨ç†åŠ é€Ÿ**ï¼Œå¯ä»¥ç²å¾—æ¥µè‡´æ•ˆèƒ½ã€‚

---

## **å…¶ä»–å…©å€‹ä¾‹å­**

### **ä¾‹å­ 1ï¼šä½¿ç”¨ CUDA åŠ é€ŸçŸ©é™£ä¹˜æ³•**
```python
import torch

# å‰µå»ºå…©å€‹éš¨æ©ŸçŸ©é™£
A = torch.randn(1000, 1000).cuda()
B = torch.randn(1000, 1000).cuda()

# åœ¨ GPU ä¸Šè¨ˆç®—çŸ©é™£ä¹˜æ³•
C = torch.matmul(A, B)

print(C.shape)  # (1000, 1000)

```

é€™ç¨®æ–¹å¼é©ç”¨æ–¼ **å¤§å‹æ•¸æ“šè™•ç†**ï¼Œå¦‚ç¥ç¶“ç¶²è·¯å‰å‘å‚³æ’­è¨ˆç®—ã€‚

---

### **ä¾‹å­ 2ï¼šä½¿ç”¨ CUDA åŠ é€Ÿå½±åƒéŠ³åŒ–**
```python
import torch
import torch.nn.functional as F

# å®šç¾©å½±åƒéŠ³åŒ–çš„å·ç©æ ¸
sharpen_kernel = torch.tensor([[-1, -1, -1],
                               [-1,  9, -1],
                               [-1, -1, -1]], dtype=torch.float32, device="cuda").unsqueeze(0).unsqueeze(0)

# è¼‰å…¥å½±åƒä¸¦è½‰æ›ç‚º Tensor
image = torch.randn(1, 1, 512, 512, device="cuda")  # å‡è¨­ç‚ºç°éšå½±åƒ

# åŸ·è¡Œå·ç©
sharpened_image = F.conv2d(image, sharpen_kernel, padding=1)

print(sharpened_image.shape)  # (1, 1, 512, 512)

```

é€™ç¨®æ–¹å¼é©ç”¨æ–¼ **å½±åƒè™•ç†åŠ é€Ÿ**ï¼Œå¦‚é‚Šç·£æª¢æ¸¬æˆ–å½±åƒå¢å¼·ã€‚

---

## **çµè«–**

ä½¿ç”¨ CUDA å¯ä»¥é¡¯è‘—æå‡ AI segmentation model çš„æ¨ç†é€Ÿåº¦ï¼Œæ–¹æ³•åŒ…æ‹¬ï¼š

1. **PyTorch CUDA Tensor åŠ é€Ÿ**
2. **è‡ªå®šç¾© CUDA Kernel**
3. **TensorRT é€²ä¸€æ­¥å„ªåŒ–**

ä¸¦ä¸” CUDA ä¹Ÿé©ç”¨æ–¼çŸ©é™£è¨ˆç®—ã€å½±åƒè™•ç†ç­‰å„ç¨®æ‡‰ç”¨å ´æ™¯ã€‚ğŸš€




### CUDA Programming æ˜¯ä»€éº¼ï¼Ÿ

CUDA Programming **ä¸åƒ…åƒ…** æŒ‡ CUDA Kernel ç·¨å¯« GPU åŠ é€Ÿå‡½æ•¸ï¼Œé‚„åŒ…æ‹¬ä½¿ç”¨ CUDA ç›¸é—œ APIï¼ˆå¦‚ `CUDA C/C++`ã€`cuDNN`ã€`TensorRT`ï¼‰ä¾†å„ªåŒ– GPU é‹ç®—ã€‚  
PyTorch CUDA Tensor åŠ é€Ÿæ¨ç† **ç¢ºå¯¦ä¹Ÿæ˜¯ CUDA Programming çš„ä¸€éƒ¨åˆ†**ï¼Œä½†å®ƒçš„åº•å±¤ä»ç„¶ä¾è³´æ–¼ CUDA Kernelã€‚

---

## **PyTorch CUDA Tensor åŠ é€Ÿæ¨ç†æ˜¯å¦å…§éƒ¨åŸç†ä¹Ÿæ˜¯ç”¨ CUDA Kernelï¼Ÿ**

### âœ… **æ˜¯çš„ï¼ŒPyTorch CUDA Tensor å…§éƒ¨ç¢ºå¯¦ä½¿ç”¨ CUDA Kernel**

ç•¶ä½ ä½¿ç”¨ PyTorch çš„ `model.to("cuda")` æˆ– `tensor.to("cuda")` æ™‚ï¼ŒPyTorch æœƒï¼š

1. **å°‡ Tensor å­˜æ”¾åˆ° GPU è¨˜æ†¶é«”**ï¼ˆGlobal Memoryï¼‰
2. **èª¿ç”¨å·²å„ªåŒ–çš„ CUDA æ ¸å¿ƒå‡½æ•¸**ï¼ˆé€™äº›å‡½æ•¸ä¾†è‡ª NVIDIA cuBLASã€cuDNN æˆ– PyTorch æœ¬èº«çš„ CUDA Kernelï¼‰
3. **åœ¨ GPU ä¸ŠåŸ·è¡ŒçŸ©é™£é‹ç®—ã€å·ç©ç­‰æ“ä½œ**
4. **å°‡çµæœè¿”å› CPU æˆ–ç¹¼çºŒåœ¨ GPU è™•ç†**

ä½ å¯ä»¥æª¢æŸ¥ PyTorch å…§éƒ¨æ˜¯å¦ä½¿ç”¨ CUDAï¼š

python

è¤‡è£½ç·¨è¼¯

`import torch print(torch.cuda.is_available())  # True è¡¨ç¤º PyTorch å·²æ”¯æ´ CUDA print(torch.backends.cudnn.enabled)  # True è¡¨ç¤º cuDNN å·²å•Ÿç”¨`

èˆ‰ä¾‹ä¾†èªªï¼Œç•¶ä½ é€™æ¨£å¯«ï¼š

python

è¤‡è£½ç·¨è¼¯

`A = torch.randn(1000, 1000).cuda() B = torch.randn(1000, 1000).cuda() C = torch.matmul(A, B)  # é€™è£¡ PyTorch æœƒè‡ªå‹•èª¿ç”¨ CUDA æ ¸å¿ƒ`

PyTorch **åº•å±¤æœƒèª¿ç”¨ cuBLAS (CUDA Basic Linear Algebra Subprograms) ä¾†åŸ·è¡ŒçŸ©é™£ä¹˜æ³•**ï¼Œé€™æ˜¯ NVIDIA æä¾›çš„é«˜æ•ˆèƒ½ CUDA æ ¸å¿ƒå‡½æ•¸ã€‚

ä½ å¯ä»¥é–‹å•Ÿ **CUDA Profiling** ä¾†æŸ¥çœ‹ PyTorch å¯¦éš›ä½¿ç”¨çš„ CUDA æ ¸å¿ƒï¼š

python

è¤‡è£½ç·¨è¼¯

`import torch with torch.autograd.profiler.profile(use_cuda=True) as prof:     C = torch.matmul(A, B) print(prof)  # é€™è£¡æœƒåˆ—å‡º CUDA Kernel èª¿ç”¨è¨˜éŒ„`

ç¸½çµä¾†èªªï¼š

- **PyTorch CUDA Tensor** æ˜¯ **å°è£å¥½çš„ CUDA æ ¸å¿ƒ**ï¼Œä½ ä¸éœ€è¦è‡ªå·±å¯« CUDA Kernelï¼Œå°±èƒ½è®“é‹ç®—åœ¨ GPU ä¸ŠåŸ·è¡Œã€‚
- **PyTorch å…§éƒ¨æœƒæ ¹æ“šé‹ç®—é¡å‹èª¿ç”¨æœ€ä½³åŒ–çš„ CUDA Kernel**ï¼ˆä¾‹å¦‚ `cuDNN` ç”¨æ–¼å·ç©ï¼Œ`cuBLAS` ç”¨æ–¼çŸ©é™£é‹ç®—ï¼‰ã€‚

---

## **é™¤äº† CUDA Kernelï¼ŒCUDA é‚„æœ‰å“ªäº›åŠ é€Ÿæ–¹å¼ï¼Ÿ**

CUDA æä¾›è¨±å¤š **ä¸åŒå±¤ç´š** çš„åŠ é€ŸæŠ€è¡“ï¼Œä¸åªæ˜¯è‡ªå·±å¯« CUDA Kernelï¼Œé‚„å¯ä»¥ç”¨ CUDA Libraryã€Graphã€TensorRTã€Multi-GPU ç­‰æŠ€è¡“ä¾†é€²ä¸€æ­¥å„ªåŒ–ï¼š

### **1. ä½¿ç”¨ cuBLAS/cuDNN ç­‰ CUDA Library åŠ é€Ÿ**

NVIDIA æä¾›äº†è¨±å¤š CUDA å„ªåŒ–çš„å‡½å¼åº«ï¼š

- **cuBLAS**ï¼šç”¨æ–¼çŸ©é™£è¨ˆç®—ï¼ˆå¦‚ `torch.matmul`ï¼‰ã€‚
- **cuDNN**ï¼šç”¨æ–¼æ·±åº¦å­¸ç¿’çš„å·ç©ã€æ± åŒ–ç­‰é‹ç®—ï¼ˆå¦‚ `torch.nn.Conv2d`ï¼‰ã€‚
- **cuSPARSE**ï¼šç”¨æ–¼ç¨€ç–çŸ©é™£è¨ˆç®—ã€‚
- **cuFFT**ï¼šç”¨æ–¼ FFT è®Šæ›ã€‚

ä½ å¯ä»¥ç›´æ¥èª¿ç”¨é€™äº› CUDA Libraryï¼Œæˆ–è€…é€é PyTorch ä¾†ä½¿ç”¨ï¼š

python

è¤‡è£½ç·¨è¼¯

`import torch  # é€™è£¡ PyTorch å…§éƒ¨æœƒèª¿ç”¨ cuDNN ä¾†åŠ é€Ÿ Conv2D conv = torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3).cuda() input_tensor = torch.randn(1, 3, 224, 224).cuda() output = conv(input_tensor)`

---

### **2. CUDA Graph (æé«˜è¨ˆç®—æ•ˆç‡)**

- é©ç”¨æ–¼ **é‡è¤‡æ€§è¨ˆç®—**ï¼Œå¦‚ batch inferenceï¼Œèƒ½å¤  **æ¸›å°‘ Kernel Launch é–‹éŠ·**ã€‚
- **åŸç†**ï¼šå°‡é‹ç®—è®Šæˆ **è¨ˆç®—åœ– (Graph Execution)**ï¼Œè®“ GPU ä¸ç”¨æ¯æ¬¡éƒ½é‡æ–°è§£æ Kernel ä¾è³´é—œä¿‚ã€‚

python

è¤‡è£½ç·¨è¼¯

`import torch  # æº–å‚™æ¨¡å‹å’Œè¼¸å…¥ model = torch.nn.Conv2d(3, 64, 3).cuda() input_tensor = torch.randn(1, 3, 224, 224, device="cuda")  # è¨˜éŒ„è¨ˆç®—åœ– stream = torch.cuda.Stream() with torch.cuda.stream(stream):     g = torch.cuda.CUDAGraph()     with torch.cuda.graph(g):         output = model(input_tensor)`

é€™å¯ä»¥ **é¡¯è‘—æ¸›å°‘æ¨ç†å»¶é²**ã€‚

---

### **3. ä½¿ç”¨ TensorRT é€²ä¸€æ­¥åŠ é€Ÿæ¨ç†**

TensorRT æ˜¯ NVIDIA æä¾›çš„ **æ·±åº¦å­¸ç¿’æ¨ç†å„ªåŒ–æ¡†æ¶**ï¼Œå¯ä»¥å°‡ PyTorch æ¨¡å‹è½‰æ›æˆé«˜æ•ˆèƒ½ CUDA Kernelï¼š

python

è¤‡è£½ç·¨è¼¯

`import torch import torch_tensorrt  model = torch.nn.Conv2d(3, 64, 3).cuda() input_tensor = torch.randn(1, 3, 224, 224, device="cuda")  trt_model = torch_tensorrt.compile(model, inputs=[torch_tensorrt.Input(input_tensor.shape, dtype=torch.float32)]) output = trt_model(input_tensor)`

TensorRT æœƒ **è‡ªå‹•èåˆé‹ç®— (Operator Fusion)**ï¼Œä¾‹å¦‚æŠŠ Conv2D å’Œ BatchNorm åˆä½µæˆä¸€å€‹ Kernelï¼ŒåŠ é€Ÿè¨ˆç®—ã€‚

---

### **4. Multi-GPU åˆ†ä½ˆå¼é‹ç®—**

ç•¶ GPU è¨˜æ†¶é«”ä¸å¤ ç”¨æ™‚ï¼Œå¯ä»¥ç”¨ **Data Parallel (DP)** æˆ– **Distributed Data Parallel (DDP)**ï¼š

python

è¤‡è£½ç·¨è¼¯

`from torch.nn.parallel import DistributedDataParallel as DDP model = DDP(model, device_ids=[0,1])`

é€™ç¨®æ–¹æ³•èƒ½å¤  **åŒæ™‚ä½¿ç”¨å¤šå¼µ GPU åŠ é€Ÿè¨ˆç®—**ã€‚

---

### **5. æ··åˆç²¾åº¦ (FP16) è¨ˆç®—**

å¯ä»¥ä½¿ç”¨ **AMP (Automatic Mixed Precision)** ä¾†æ¸›å°‘è¨˜æ†¶é«”ä½”ç”¨ä¸¦åŠ é€Ÿè¨ˆç®—ï¼š

python

è¤‡è£½ç·¨è¼¯

`scaler = torch.cuda.amp.GradScaler()  with torch.cuda.amp.autocast():     output = model(input_tensor)  scaler.scale(loss).backward() scaler.step(optimizer) scaler.update()`

é€™å¯ä»¥ **æå‡æ¨ç†é€Ÿåº¦ 2 å€**ï¼Œç‰¹åˆ¥é©ç”¨æ–¼ Transformer æˆ– CNN æ¨¡å‹ã€‚

---

### **6. ä½¿ç”¨ Shared Memory (å…±äº«è¨˜æ†¶é«”) æé«˜è¨ˆç®—æ•ˆç‡**

ç•¶ä½ å¯« CUDA Kernel æ™‚ï¼Œå¯ä»¥ä½¿ç”¨ **å…±äº«è¨˜æ†¶é«” (Shared Memory)** ä¾†åŠ é€Ÿè®€å–ï¼š

cpp

è¤‡è£½ç·¨è¼¯

`__global__ void shared_memory_example(float *input, float *output) {     __shared__ float smem[1024];  // å…±äº«è¨˜æ†¶é«”      int tid = threadIdx.x;     smem[tid] = input[tid];  // æŠŠæ•¸æ“šæ”¾å…¥å…±äº«è¨˜æ†¶é«”      __syncthreads();  // åŒæ­¥æ‰€æœ‰ç·šç¨‹      output[tid] = smem[tid] * 2;  // è¨ˆç®—ä¸¦è¼¸å‡º }`

é€™æ¨£ **è¨˜æ†¶é«”å­˜å–æœƒæ¯” Global Memory æ›´å¿«**ï¼Œæå‡è¨ˆç®—æ•ˆèƒ½ã€‚

---

## **ç¸½çµ**

CUDA Programming ä¸åƒ…åƒ…æ˜¯è‡ªå·±å¯« CUDA Kernelï¼Œé‚„åŒ…æ‹¬ï¼š

|**æŠ€è¡“**|**æ˜¯å¦å…§éƒ¨ç”¨ CUDA Kernel**|**å„ªé»**|
|---|---|---|
|**PyTorch CUDA Tensor**|âœ…|ç›´æ¥ä½¿ç”¨ GPU é‹ç®—|
|**CUDA Kernel (è‡ªå®šç¾©é‹ç®—)**|âœ…|å®Œå…¨æ§åˆ¶ CUDA é‹ç®—|
|**cuBLAS/cuDNN (NVIDIA å‡½å¼åº«)**|âœ…|å…§å»ºå„ªåŒ–çš„é«˜æ•ˆèƒ½ Kernel|
|**CUDA Graph**|âœ…|æ¸›å°‘ Kernel å•Ÿå‹•é–‹éŠ·|
|**TensorRT**|âœ…|è‡ªå‹•å„ªåŒ–æ¨ç†|
|**Multi-GPU (DDP)**|âœ…|æé«˜è¨ˆç®—è¦æ¨¡|
|**AMP æ··åˆç²¾åº¦ (FP16)**|âœ…|é™ä½è¨˜æ†¶é«”ä½¿ç”¨ï¼ŒåŠ é€Ÿæ¨ç†|
|**CUDA Shared Memory**|âœ…|æ›´å¿«çš„è¨˜æ†¶é«”å­˜å–|

é€™äº›æŠ€è¡“ **éƒ½å±¬æ–¼ CUDA Programming**ï¼Œä½ å¯ä»¥æ ¹æ“šè‡ªå·±çš„éœ€æ±‚é¸æ“‡åˆé©çš„æ–¹æ³•ä¾†æå‡ segmentation model çš„æ•ˆèƒ½ ğŸš€ã€‚



Reference:
CUDAå¿«é€Ÿå…¥é—¨-01-åŸºç¡€è¯­æ³•æ¦‚å¿µ - é¦™è‰ç¾äººçš„æ–‡ç«  - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/16641687170