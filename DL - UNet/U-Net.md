
ref:  [UNet理解，pytorch实现，源码解读](https://zhuanlan.zhihu.com/p/571760241)
ref: [Unet论文超级详解（附图文：超细节超容易理解）](https://zhuanlan.zhihu.com/p/716339396)
ref: [U-Net原理分析与代码解读](https://zhuanlan.zhihu.com/p/150579454)
![[unet.png]]

如上图，Unet 网络结构是对称的，形似英文字母 U 所以被称为 Unet。整张图都是由蓝/白色框与各种颜色的箭头组成，其中，**蓝/白色框表示 feature map；蓝色箭头表示 3x3 卷积，用于特征提取；灰色箭头表示 skip-connection，用于[特征融合](https://zhida.zhihu.com/search?content_id=121594236&content_type=Article&match_order=1&q=%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88&zhida_source=entity)；红色箭头表示池化 pooling，用于降低维度；绿色箭头表示上采样 upsample，用于恢复维度；青色箭头表示 1x1 卷积，用于输出结果**。其中灰色箭头`copy and crop`中的`copy`就是`concatenate`而`crop`是为了让两者的长宽一致


ref: [nn.ConvTranspose2d原理，深度网络如何进行上采样？](https://blog.51cto.com/u_15274944/5244229)

**U-Net** 是一種經典的圖像分割模型，主要用於醫學影像分割和其他需要像素級別精確的任務。以下是 U-Net 的損失函數與優化方法的詳細解釋：

---

### 1. **損失函數 (Loss Function)**

U-Net 的核心任務是**語義分割**，即為圖像中的每個像素分配一個類別標籤。因此，其損失函數通常是基於像素級別的比較。

以下是一些在 U-Net 中常用或與其相關的損失函數：

1. **交叉熵損失 (Cross-Entropy Loss) / 對數損失 (Log Loss)**
    
    - **基本原理**：交叉熵損失是分類問題中最常用的損失函數之一。在語義分割中，每個像素都被視為一個獨立的分類問題。它衡量的是模型預測的類別概率分佈與真實類別分佈之間的差異。
    - **二元交叉熵 (Binary Cross-Entropy, BCE)**：
        - **適用場景**：當分割任務是二分類時（例如，前景 vs 背景，或者只分割一種特定物件）。
        - **U-Net 輸出**：網路的最後一層對每個像素通常會使用 Sigmoid 激活函數，輸出一張概率圖，其中每個像素值表示其屬於前景的概率 p。
        - **計算方式**：對於每個像素，如果真實標籤 y 為 1 (前景)，損失為 −log(p)；如果 y 為 0 (背景)，損失為 −log(1−p)。總損失是所有像素損失的平均值或總和。
        - 公式簡化形式（針對單個像素）：LBCE​=−[ylog(p)+(1−y)log(1−p)]
    - **分類交叉熵 (Categorical Cross-Entropy)**：
        - **適用場景**：當分割任務是多分類時（例如，分割背景、細胞核、細胞質等多個類別）。
        - **U-Net 輸出**：網路的最後一層對每個像素通常會使用 Softmax 激活函數，為每個類別輸出一張概率圖，每個像素在所有類別上的概率總和為1。
        - **計算方式**：對於每個像素，損失是其真實類別所對應的預測概率的負對數。
        - 公式簡化形式（針對單個像素，假設真實類別為 k）：LCCE​=−log(pk​)，其中 pk​ 是模型預測該像素屬於真實類別 k 的概率。
    - **優點**：
        - 原理清晰，易於理解和實現。
        - 能夠很好地懲罰預測錯誤的類別。
        - 當與 Softmax/Sigmoid 結合時，梯度計算相對穩定。
    - **缺點**：
        - **類別不平衡問題**：在許多分割任務中（尤其是醫學影像），背景像素的數量可能遠遠超過前景像素。標準的交叉熵損失會給予所有像素相同的權重，導致模型偏向於預測數量較多的類別（通常是背景），而忽略數量較少的類別。
2. **加權交叉熵損失 (Weighted Cross-Entropy Loss)**
    
    - **基本原理**：為了解決類別不平衡問題，可以為不同類別的像素分配不同的權重。數量較少的類別（通常是我們更關心的前景物件）會被賦予更高的權重，使其在總損失中的貢獻更大。
    - **U-Net 原始論文中的加權方案**：
        - 在其原始論文中，Olaf Ronneberger 等人使用了一種特殊的加權交叉熵。他們計算了一個預先定義的權重圖 (weight map)，這個權重圖會給予**位於兩個接觸物件邊界處的像素更高的權重**。
        - **目的**：這樣做的目的是讓模型更努力地區分緊密接觸的物件實例，這是細胞分割等任務中的一個常見挑戰。權重計算基於形態學操作，考慮了到最近細胞邊界和第二近細胞邊界的距離。
        - w(x)=wc​(x)+w0​⋅exp(−2σ2(d1​(x)+d2​(x))2​)
            - wc​(x) 是類別平衡權重（例如，基於類別頻率）。
            - d1​(x) 是像素 x 到最近細胞邊界的距離。
            - d2​(x) 是像素 x 到第二近細胞邊界的距離。
            - w0​ 和 σ 是超參數。
    - **更簡單的加權方式**：直接根據每個類別的像素數量反比來設置權重。例如，如果前景像素佔10%，背景佔90%，則前景像素的權重可以是9，背景像素的權重可以是1。
    - **優點**：有效緩解類別不平衡問題，可以引導模型關注特定區域或類別。
    - **缺點**：權重的選擇可能需要仔細調整，U-Net 原始論文中的權重圖計算相對複雜。
3. **Dice 損失 (Dice Loss)**
    
    - **基本原理**：Dice 損失直接源於 Dice 係數 (Dice Coefficient)，Dice 係數是衡量兩個樣本（在這裡是預測分割和真實分割）相似度的常用指標，通常用於評估分割性能。Dice 係數可以看作是 F1-score 的一種形式。
    - Dice 係數 (DSC) 公式：DSC=∣X∣+∣Y∣2⋅∣X∩Y∣​，其中 X 是預測集，Y 是真實集，∣X∩Y∣ 是它們的交集，∣X∣ 和 ∣Y∣ 是它們的元素數量（在這裡是像素數量）。
    - Dice 損失公式：LDice​=1−DSC
    - **計算方式**：在實踐中，為了使其可微且適用於神經網路的概率輸出，通常使用 "soft" Dice Loss： LDice​=1−∑i=1N​pi2​+∑i=1N​gi2​+ϵ2∑i=1N​pi​gi​+ϵ​ 或者 LDice​=1−∑i=1N​pi​+∑i=1N​gi​+ϵ2∑i=1N​pi​gi​+ϵ​ 其中 pi​ 是模型對像素 i 屬於前景的預測概率，gi​ 是真實標籤 (0或1)，N 是總像素數，ϵ 是一個很小的平滑常數，用於防止分母為零並穩定梯度。
    - **優點**：
        - **對類別不平衡不敏感**：Dice 損失天然地處理了類別不平衡問題，因為它關注的是預測和真實標籤之間的重疊程度，而不是像素的絕對數量。
        - 直接優化分割指標：訓練目標與評估指標更接近。
    - **缺點**：
        - **梯度問題**：當預測與真實標籤重疊很小或沒有重疊時（即分子接近於0），梯度可能會變得非常小或不穩定，導致訓練困難。
        - 對於小目標的分割，如果預測稍微偏離，Dice 係數可能會急劇下降，導致損失值和梯度波動較大。
        - 不適合極端情況，例如真實標籤全為0（沒有前景）。
4. **Jaccard / IoU 損失 (Intersection over Union Loss)**
    
    - **基本原理**：與 Dice 損失非常相似，Jaccard 損失基於 Jaccard 指數（也稱為 IoU，Intersection over Union）。IoU 也是評估分割性能的常用指標。
    - Jaccard 指數 (IoU) 公式：IoU=∣X∪Y∣∣X∩Y∣​=∣X∣+∣Y∣−∣X∩Y∣∣X∩Y∣​
    - Jaccard 損失公式：LIoU​=1−IoU
    - **計算方式**：類似於 Dice Loss，也有 "soft" IoU Loss 的形式。
    - **優點和缺點**：與 Dice Loss 非常相似。它們之間可以相互轉換 (DSC=IoU+12⋅IoU​)。在實踐中，兩者通常表現相近。
5. **Focal Loss**
    
    - **基本原理**：Focal Loss 是對標準交叉熵損失的一種改進，最初是為了解決物件偵測中前景和背景類別極度不平衡以及難易樣本不平衡的問題。它也可以應用於語義分割。
    - **核心思想**：通過一個調節因子 (modulating factor) (1−pt​)γ 來降低容易分類樣本（即高置信度正確預測的樣本 pt​ 較大）的權重，從而使模型更專注於訓練那些難以分類的樣本（pt​ 較小）。
    - Focal Loss 公式（以二元交叉熵為例）：LFocal​=−αt​(1−pt​)γlog(pt​)
        - pt​：對於真實類別的預測概率。
        - γ≥0：聚焦參數 (focusing parameter)，γ>0 時會降低容易樣本的權重。當 γ=0 時，Focal Loss 退化為加權交叉熵損失。
        - αt​：平衡因子，類似於加權交叉熵中的類別權重，用於平衡正負樣本的重要性。
    - **優點**：
        - **有效處理極端類別不平衡**。
        - **關注難學樣本**，有助於提升對困難情況的分割精度。
    - **缺點**：
        - 引入了額外的超參數 α 和 γ，需要調整。
6. **組合損失 (Combined Losses)**
    
    - **基本原理**：將多種損失函數結合起來，以期利用各自的優點並彌補不足。
    - **常見組合**：
        - **BCE Loss + Dice Loss** (或 CCE Loss + Dice Loss)：這是一個非常流行的組合。BCE/CCE Loss 提供了平滑的梯度和對數概率的良好特性，而 Dice Loss 則能很好地處理類別不平衡並直接優化分割重疊度。通常是兩者加權求和：LTotal​=λ1​LBCE/CCE​+λ2​LDice​。
        - **Focal Loss + Dice Loss**：結合 Focal Loss 對難學樣本和類別不平衡的處理能力，以及 Dice Loss 對重疊度的關注。
    - **優點**：通常比單一損失函數更魯棒，性能更好。
    - **缺點**：需要調整組合權重，增加了模型的複雜性。

**如何選擇損失函數？**

選擇哪種損失函數取決於具體的應用場景和數據特性：

- **類別平衡情況**：如果類別相對平衡，標準的交叉熵損失可能就足夠了。
- **類別不平衡情況**：加權交叉熵、Dice Loss、Focal Loss 或它們的組合通常是更好的選擇。
- **對邊界精度要求高**：U-Net 原始論文中的加權交叉熵，或者一些專門設計的邊界損失 (Boundary Loss) 可能有幫助。
- **小目標分割**：Dice Loss 或 IoU Loss 可能會遇到梯度不穩定的問題，此時 Focal Loss 或 BCE/CCE Loss 的穩定性可能更重要，或者可以嘗試 Dice Loss 的變體（如 Generalized Dice Loss）。
- **評估指標**：如果最終的評估指標是 Dice 係數或 IoU，那麼直接優化 Dice Loss 或 IoU Loss（或它們的組合）通常是個好主意。

在實踐中，**BCE/CCE + Dice Loss 的組合**因其在多數情況下的良好表現而廣泛應用於 U-Net 及其他分割模型中。然而，沒有萬能的損失函數，通常需要根據實際情況進行實驗和選擇。

---

### 2. **優化方法 (Optimization Method)**

U-Net 的優化過程使用現代優化技術，這些技術可以加速模型收斂並提高分割效果。

#### (1) **優化器 (Optimizer)**

- **Adam Optimizer**:
    - Adam 是最常用的優化器之一，結合了動量法與自適應學習率方法。

- **SGD + Momentum**:
    - 使用 Stochastic Gradient Descent (SGD) 配合 Momentum，提升收斂穩定性。

#### (2) **學習率調度 (Learning Rate Scheduler)**

- **Step Decay**:
    - 每隔幾個 epoch 將學習率減小固定比例，幫助模型更穩定地收斂。
- **Cosine Annealing**:
    - 在訓練過程中使用餘弦退火逐漸降低學習率。
- **Warm-up Scheduler**:
    - 訓練初期逐步增大學習率，避免模型不穩定。

#### (3) **批量大小 (Batch Size)**

- 批量大小依硬體資源而定，通常會在 8 到 32 之間選擇。

---

### 總結

1. **損失函數**:
    
    - 二元分割：Binary Cross-Entropy Loss、Dice Loss
    - 多類分割：Categorical Cross-Entropy Loss、Dice Loss
    - 不平衡數據：Tversky Loss、Hybrid Loss (如 BCE + Dice)
2. **優化方法**:
    
    - **優化器**: Adam Optimizer（首選），或 SGD + Momentum
    - **學習率調度**: Cosine Annealing、Step Decay、Warm-up Scheduler

這些設置確保 U-Net 模型在不同的分割場景中表現出色，尤其是在醫學影像分割中。