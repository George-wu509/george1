
MMPose [Github](https://github.com/open-mmlab/mmpose)


|                              |     |
| ---------------------------- | --- |
| [[#### MMPose 全面詳解]]         |     |
| [[#### Pose estimation後續分析]] |     |


```
請中文詳細解釋MMPose, 特別注重在keypoint detection model跟 pose estimation model的所有技術細節包括輸入(譬如是image or video或detection box), 輸出譬如是每個keypoint的座標or bbox, Model architecture, 如何進行訓練, 主要應用並用具體案例詳細一步步說明. 也請列表比較其他主流keypoint detection model跟 pose estimation models.
```

這就為您詳細解釋MMPose，並涵蓋您所要求的各個技術細節。

#### MMPose 全面詳解

MMPose 是一款基於 PyTorch 的開源姿態估計（Pose Estimation）工具箱，是 OpenMMLab 項目的一部分。它以其模組化設計、豐富的算法庫、高效能和靈活性而聞名，廣泛應用於學術研究和工業界。

MMPose 不僅僅是一個單一的模型，而是一個集成了數據處理、模型構建、訓練、評估和部署等完整流程的框架。它可以處理多種姿態估計(pose estimation)任務，包括單人/多人2D姿態估計、3D人體姿態估計、手部姿態估計、臉部關鍵點檢測、動物關鍵點檢測，甚至服裝和物體的關鍵點檢測。

---

### 1. 關鍵點檢測 (Keypoint Detection) 與姿態估計 (Pose Estimation) 模型技術細節

在MMPose的框架下，"Keypoint Detection Model" 和 "Pose Estimation Model" 通常指的是同一個模型，其核心任務就是從輸入中檢測出物體（如人體）的關鍵點（如關節）座標。整個流程可以細分為**「自頂向下 (Top-down)」**和**「自底向上 (Bottom-up)」**兩種主要方法。

#### **自頂向下 (Top-down) 方法詳解**

這是MMPose中最常用也最精準的方法。其核心思想是「先檢測，後估計」。

1. **輸入 (Input):**
    
    - **原始輸入:** 一張靜態圖片 (Image) 或影片中的一幀 (Video Frame)。
        
    - **第一階段 - 目標檢測:** 首先，需要一個獨立的目標檢測器 (Object Detector)，例如 YOLO, Faster R-CNN, 或 MMDetection 庫中的任何模型。這個檢測器會在圖片上識別人體，並輸出每個人的邊界框 (Bounding Box, bbox)，格式通常是 `[x_min, y_min, x_max, y_max]`。
        
    - **第二階段 - 姿態估計模型的真正輸入:** MMPose會將目標檢測器輸出的每個人體邊界框從原圖中裁剪 (crop) 出來，並將這些裁剪後的人體圖像縮放 (resize) 到一個固定的尺寸（例如 `256x192` 或 `384x288` 像素）。**因此，姿態估計模型的直接輸入是這些標準化後的人體圖像塊 (cropped and resized human image patches)。**
        
2. **模型架構 (Model Architecture):** MMPose中的姿態估計模型通常由三個核心部分組成：
    
    - **Backbone (骨幹網路):** 負責從輸入的人體圖像塊中提取特徵。這些通常是強大的卷積神經網絡 (CNN)。
        
        - **常用模型:** ResNet, HRNet (High-Resolution Net), MobileNet (輕量級), ResNeSt等。
            
        - **HRNet** 特別適合姿態估計，因為它能在整個過程中保持高解析度的特徵圖，這對於精確定位關鍵點至關重要。傳統的CNN（如ResNet）在深層時解析度會降低，丟失空間細節。
            
    - **Neck (頸部):** 雖然在某些簡單的模型中不存在，但在更複雜的模型中，Neck 用於融合和增強從Backbone不同層級輸出的特徵圖。例如，特徵金字塔網絡 (FPN)。
        
    - **Head (頭部):** 這是模型的最終輸出部分，負責根據提取到的特徵預測關鍵點的位置。
        
        - **主流設計 - 熱力圖 (Heatmap):** 大多數先進的模型（包括MMPose中的主流模型）不直接回歸 (regression) 關鍵點的 `(x, y)` 座標。而是為每個關鍵點（例如，鼻子、左肩、右膝）生成一張**熱力圖 (Heatmap)**。
            
        - 一張熱力圖的尺寸通常是輸入圖像的 `1/4`（例如 `64x48`），其上的每個像素值代表該位置是對應關鍵點的「可能性」或「置信度」。在理想情況下，熱力圖在真實關鍵點位置的像素值最高（通常用一個二維高斯分佈來表示），離得越遠值越低。
            
3. **輸出 (Output):**
    
    - **模型直接輸出:** 一個批次 (batch) 的熱力圖。如果模型要檢測 `K` 個關鍵點，那麼輸出就是 `K` 張熱力圖。例如，COCO數據集定義了17個人體關鍵點，模型就會輸出17張熱力圖。
        
    - **後處理 (Post-processing):** 為了得到最終的座標，MMPose會對每張熱力圖進行後處理：
        
        1. 找到熱力圖上響應值最高的像素點 `(u, v)`。
            
        2. 為了提高亞像素 (sub-pixel) 精度，通常會對最高點周圍的像素值進行微調（例如，二次插值或從高響應值向次高響應值偏移1/4像素）。
            
        3. 將計算出的熱力圖座標 `(u, v)` **映射回原始圖像的座標系**，得到最終的關鍵點座標 `(x, y)` 和一個置信度分數 (confidence score)，這個分數通常就是熱力圖上的最高響應值。
            
    - **最終輸出格式:** 對於輸入圖像中的每個人，輸出是一個包含 `K` 個關鍵點的列表，每個關鍵點的格式通常是 `[x, y, confidence]`。
        
4. **訓練過程 (Training):**
    
    - **標籤 (Ground Truth):** 在訓練時，我們需要有標註好的數據，即每個關鍵點的真實座標 `(x_gt, y_gt)`。
        
    - **生成目標熱力圖 (Target Heatmap):** 我們不能直接用座標來監督熱力圖的生成。因此，對於每個關鍵點，我們會手動創建一張「目標熱力圖」。具體做法是在 `(x_gt, y_gt)` 對應到熱力圖上的位置 `(u_gt, v_gt)` 為中心，生成一個二維的高斯核，作為模型的學習目標。
        
    - **損失函數 (Loss Function):** 訓練的目標是讓模型預測出的熱力圖與我們生成的目標熱力圖盡可能相似。最常用的損失函數是**均方誤差損失 (Mean Squared Error, MSE Loss)**，它會逐像素計算預測熱力圖和目標熱力圖之間的差異。
        
    - **數據增強 (Data Augmentation):** 為了提高模型的泛化能力，MMPose會使用大量的數據增強技術，包括：
        - 隨機翻轉 (Random Flip)
        - 隨機旋轉 (Random Rotation)
        - 隨機縮放 (Random Scale)
        - 仿射變換 (Affine Transformation)

#### **自底向上 (Bottom-up) 方法簡述**

這種方法與Top-down相反，它「先找關節，再組裝成人」。

1. **輸入:** 整張原始圖片，無需預先檢測人體。
    
2. **模型過程:**
    - 模型首先檢測出圖像中**所有**的關鍵點（例如，所有的手腕、所有的膝蓋）。
    - 同時，模型會學習預測**部件親和場 (Part Affinity Fields, PAFs)**，這是一種2D向量場，表示不同關節之間的連接方向和可能性。
        
3. **輸出與後處理:**
    - 在得到所有候選關鍵點和PAFs後，通過一個複雜的圖匹配算法（例如，貪心算法），將屬於同一個人的關節連接起來，組裝成完整的人體骨架。
        
4. **優缺點:**
    - **優點:** 速度快，因為只需對整張圖進行一次推斷，不受人數影響。
    - **缺點:** 擁擠場景中，精度通常不如Top-down方法，因為關節匹配容易出錯。
        

### 2. 主要應用與具體案例說明

MMPose的應用非常廣泛，涵蓋了需要理解人體、動物或物體姿態的各個領域。

#### **案例：健身動作姿態分析系統**

假設我們要開發一個深蹲 (Squat) 動作的輔助教練系統，以確保用戶動作標準。

**第一步：數據採集與準備**
- **輸入:** 來自手機或電腦攝影機的即時影片流 (Video Stream)。
- **數據處理:** 從影片流中逐幀讀取圖像。

**第二步：Top-down姿態估計流程**

1. **人體檢測:**
    - 將當前幀圖像輸入到一個預訓練的目標檢測模型（例如，YOLOv3）。
    - 該模型會返回偵測到的每個人的邊界框 `bbox`。如果畫面中只有一個人，就取置信度最高的那個 `bbox`。
2. **圖像預處理:**
    - 使用MMPose的數據處理流程，根據上一步得到的 `bbox` 從原始幀中裁剪出人體圖像。
    - 將裁剪出的圖像縮放到MMPose姿態估計模型（例如，HRNet-w48）所要求的輸入尺寸，比如 `256x192`。
3. **姿態估計推斷:**
    - 將預處理後的人體圖像輸入到加載好的MMPose模型中。
    - 模型輸出17張熱力圖（對應COCO數據集的17個關鍵點）。
4. **後處理與座標獲取:**
    - MMPose對熱力圖進行解碼，找到每個關鍵點（如腳踝、膝蓋、髖部、肩膀）在原始圖像中的 `(x, y)` 座標和置信度。
    - **輸出:** 我們得到一個包含17個 `[x, y, confidence]` 的列表，代表了這個人的完整骨架。

**第三步：應用層邏輯分析**

- **角度計算:** 利用得到的關節座標，我們可以計算出動作的關鍵角度。例如，通過髖部、膝蓋和腳踝三個點的座標，可以利用向量的夾角公式計算出膝蓋的彎曲角度。
        
- **規則判斷:**
    - **深度判斷:** 判斷深蹲的最低點時，髖關節是否低於膝關節。
    - **膝蓋位置:** 判斷膝蓋是否超過腳尖（可以通過比較膝關節和腳踝的水平座標）。
    - **背部姿態:** 判斷肩膀和髖部的連線是否保持相對挺直。
        
- **反饋與互動:**
    
    - 在畫面上即時繪製骨架和角度。
    - 當檢測到動作不標準時（例如，膝蓋彎曲角度不夠），系統可以發出語音或文字提示，如「請再蹲低一點」。
    - 完成一次有效的深蹲後，計數器加一。
        

**其他應用場景:**

- **人機交互:** 通過手勢姿態估計來控制虛擬物件或操作系統。
- **運動分析:** 分析運動員（如高爾夫球手、游泳運動員）的動作以優化表現。
- **安防監控:** 檢測異常行為，如跌倒檢測。
- **動畫與電影特效 (VFX):** 無標記點的動作捕捉 (Markerless Motion Capture)，將真人動作遷移到虛擬角色上。
- **醫療健康:** 康復訓練輔助、步態分析等。

---

### 3. 與其他主流模型的比較

|特性/模型|MMPose (框架)|OpenPose|AlphaPose|Detectron2|
|---|---|---|---|---|
|**主要方法**|包含 Top-down 和 Bottom-up，以 Top-down 為主|Bottom-up|Top-down|主要為 Top-down|
|**核心優勢**|**模組化、靈活性高、算法庫全面**，支持多種任務（人、手、臉、動物），易於擴展和研究|**開創性的 Bottom-up 方法**，在多人擁擠場景下速度快|**高精度**，引入了區域人體提議網絡 (SPPE) 和參數化姿態非極大值抑制 (PGNMS)|作為一個**通用的目標檢測/分割框架**，其姿態估計功能是其中一部分，與分割等任務結合緊密|
|**代表模型**|**HRNet**, ResNet, Lite-HRNet, MobileNet|VGG-based (原始模型)|ResNet-based|ResNet, FPN 等|
|**輸入方式**|**靈活**，可對接任何檢測器 (如MMDetection) 輸出的人體框，或直接處理圖像 (Bottom-up)|整張圖輸入|需要先進行人體檢測，自帶檢測器|內置檢測流程，先檢測後估計|
|**輸出**|關鍵點座標 `[x, y, conf]`|關鍵點座標 + 關聯信息 (JSON格式)|關鍵點座標 `[x, y, conf]` (JSON格式)|統一的數據結構，包含bbox、分割掩碼和關鍵點|
|**訓練友好度**|**非常高**，基於MMEngine，配置化驅動，訓練、評估、測試流程標準化|訓練過程相對複雜，不如MMPose模組化|提供了訓練腳本，但靈活性和擴展性不如MMPose|訓練流程清晰，但主要為Facebook生態服務，定製化可能需要更深入的了解|
|**社區與生態**|**非常活躍**，背靠強大的OpenMMLab生態，與MMDetection, MMClassification等庫無縫集成|影響力深遠，但近年來更新和擴展速度放緩|擁有穩定的用戶群體，在學術界有較高引用|**非常龐大**，是Meta (Facebook AI) 的官方庫，文檔和社區支持都很好|
|**適用場景**|學術研究、工業級項目開發、需要**高度定製化和多任務**的場景|需要**處理大量擁擠人群**且對實時性要求高的場景|追求**最高精度**的單人/多人姿態估計任務|當姿態估計是**一個更大的視覺理解系統**（如實例分割+姿態估計）的一部分時|

#### **總結比較:**

- 如果你是一個**研究者**或需要一個**靈活、強大、可擴展**的姿態估計框架來支持多種任務和模型，**MMPose是首選**。它的代碼質量和文檔都非常出色。
    
- **OpenPose** 更像是一個已經成型的、開箱即用的工具，它的 Bottom-up 方法在特定場景下（如舞台、廣場等多人密集場景）依然有其不可替代的優勢。
    
- **AlphaPose** 在精度上做了很多優化，如果你追求極致的姿-態估計準確率，它是一個非常強的競爭者。
    
- **Detectron2** 則是一個更宏大的框架，姿態估計只是其功能之一。當你需要同時處理檢測、分割、姿態估計等多個任務時，使用Detectron2可以避免在不同框架間切換。





#### Pose estimation後續分析

```
所以pose estimation或mmpose的最後結果是image or video每一frame的每個人的一組key point detection. 那要如何用這個結果連結到後續的application, 譬如action detection, Intention Inference, Gaze estimation等等. 有哪些常用的後續分析跟applications, 以及中文詳細解釋後續分析的個別model, 輸入輸出跟model architecture跟如何training. 並具體舉例說明
```

MMPose 或任何姿態估計模型的最終結果，就是**結構化的時間序列數據**——為影片中的每一幀、每一個人，提供一組描述其身體姿態的關鍵點座標。這一步完成了從**非結構化的像素數據**到**結構化的骨架數據**的轉換。譬如video的某個frame裡某人的17個keypoints的 [ x, y, confidence ]

這個結構化的骨架數據是後續所有高級應用的基石。它去除了背景、光照、衣著顏色等無關噪音，只保留了最核心的姿態和運動信息，使得後續分析變得更簡單、更高效、也更具魯棒性。

接下來，我們詳細探討如何利用這些骨架數據，以及後續分析模型的細節。

---

### 從關鍵點到高級應用的橋樑：特徵工程

直接使用原始的 `(x, y)` 座標序列有時效果不佳，因為它們受限於人在畫面中的位置和大小。因此，在送入後續模型前，通常會進行**特徵工程 (Feature Engineering)**：

1. **歸一化 (Normalization):**
    - **平移不變性:** 以某個中心點（如人體骨盆或軀幹中心）為原點 `(0,0)`，將所有其他關鍵點的座標進行相對位移。這樣，無論這個人站在畫面的哪個位置，其相對骨架姿態都是一樣的。
    - **尺度不變性:** 計算骨架的某個固定長度（如軀幹長度）作為單位長度 "1"，然後將所有關節點之間的距離按此比例縮放。這樣，無論這個人離鏡頭遠近，其標準化的骨架大小都是一致的。
        
2. **動態特徵提取:**
    
    - **位移 (Displacement):** 計算每個關節點在相鄰幀之間的移動向量（即速度）。
    - **加速度 (Acceleration):** 計算速度的變化。
    - **關節角度 (Joint Angles):** 計算相鄰骨骼之間的夾角，例如膝關節角度、肘關節角度。
    - **骨骼長度與方向 (Bone Length and Orientation):** 直接將骨骼表示為向量。

經過處理後，每一幀的輸入不再是2D座標，而是一個包含了位置、速度、角度等多維度信息的高維特徵向量。一個長度為 `T` 幀的影片片段，其輸入就是一個 `(T, K, C)` 的張量，其中 `T` 是時間序列長度，`K` 是關鍵點數量，`C` 是每個點的特徵維度。

---

### 後續分析模型與應用詳解

#### 1. 動作識別 (Action Recognition)

這是最常見的後續應用，旨在識別一個人在一段時間內（一個影片片段）所做的動作是什麼。

- **具體應用:** 智慧安防（識別打鬥、跌倒）、運動分析（識別投籃、揮拍）、人機互動（識別揮手、鼓掌）。
    
- **模型輸入:** 一個固定長度的骨架序列。例如，一個 `(32, 17, 3)` 的張量，代表32幀、17個關鍵點、每個點有 `(x, y, confidence)` 3個維度。
    
- **模型輸出:** 一個表示動作類別的標籤，例如 "走路", "跑步", "跌倒"。
    
- **主流模型架構:**
    
    - **RNN (Recurrent Neural Networks) / LSTM / GRU:**
        
        - **架構:** 這些模型天生適合處理序列數據。它們有一個「記憶」單元，可以按時間步（逐幀）讀取骨架數據，並不斷更新其隱藏狀態 (hidden state)。在讀取完整個序列後，最終的隱藏狀態就包含了整個動作的時序信息，最後接一個全連接層進行分類。
            
        - **訓練:** 使用帶有動作標籤的骨架序列數據集（如 NTU-RGB+D, Kinetics-Skeleton）進行訓練。損失函數通常是**交叉熵損失 (Cross-Entropy Loss)**。
            
    - **ST-GCN (Spatio-Temporal Graph Convolutional Network):**
        
        - **核心思想:** 這是目前基於骨架的動作識別領域的**SOTA (State-of-the-art)** 方法。它將人體骨架看作一個**圖 (Graph)**，其中關節是**節點 (Node)**，骨骼是**邊 (Edge)**。
            
        - **架構:**
            
            1. **空間圖卷積 (Spatial Graph Convolution):** 在單一幀內，模型通過圖卷積操作來聚合相鄰關節的特徵。例如，「手」的特徵會從「手腕」和「手臂」的特徵中學習，模擬了卷積神經網絡(CNN)中感受野的概念，但作用於非歐幾里得的圖結構上。
                
            2. **時間卷積 (Temporal Convolution):** 在時間維度上，模型使用標準的1D卷積核來捕捉同一個關節在時間上的運動模式。
                
            
            - 通過交替堆疊這兩種卷積層，ST-GCN可以同時學習身體部件的空間結構關係和它們隨時間變化的動態模式。
                
        - **訓練:** 同樣使用交叉熵損失進行分類訓練。ST-GCN的優勢在於它能顯式地利用人體的自然拓撲結構，比RNN更有效地學習局部肢體協同運動。
            

#### 2. 意圖推斷 / 軌跡預測 (Intention Inference / Trajectory Prediction)

預測一個人的短期未來行為或移動路徑。

- **具體應用:** 自動駕駛（預測行人是否會橫穿馬路）、智慧零售（預測顧客下一步會走向哪個貨架）、人機協作（預測工人的操作意圖以避免碰撞）。
    
- **模型輸入:** 過去一段時間（例如過去2秒）的骨架序列和/或邊界框(bbox)位置序列。
    
- **模型輸出:** 未來一段時間（例如未來3秒）的骨架姿態序列或bbox位置序列。
    
- **主流模型架構 (Seq2Seq 模型):**
    
    - **Encoder-Decoder 架構 (編碼器-解碼器):**
        
        - **Encoder (編碼器):** 通常是一個LSTM或GRU。它負責讀取並「理解」輸入的歷史軌跡序列，將其壓縮成一個固定長度的「上下文向量 (context vector)」，這個向量代表了對過去運動的總結。
            
        - **Decoder (解碼器):** 另一個LSTM或GRU。它接收編碼器輸出的上下文向量，然後逐個時間步地生成未來的姿態或位置。上一步的輸出會作為下一步的輸入，直到生成所需長度的未來序列。
            
    - **Transformer:** 近年來，基於自註意力機制的Transformer模型也在此類任務中表現出色，因為它能更好地捕捉序列中長距離的依賴關係。
        
- **訓練:**
    
    - **數據:** 需要大量的行人/物體運動軌跡數據集（如 a JAAD, PIE）。
        
    - **損失函數:** 訓練目標是讓模型預測的未來序列與真實的未來序列盡可能接近。因此，損失函數通常是**均方誤差 (Mean Squared Error, MSE)** 或 **L2損失**，計算預測座標和真實座標之間的歐氏距離。
        

#### 3. 視線估計 (Gaze Estimation)

這個應用比較特殊，它高度依賴臉部關鍵點，而不是全身關鍵點。MMPose同樣支持臉部關鍵點檢測。

- **具體應用:** 注意力分析（分析駕駛員或學生的注意力是否集中）、輔助技術（幫助殘障人士用眼睛控制設備）、市場研究（分析顧客在看哪個廣告）。
    
- **模型輸入:**
    
    1. 高解析度的**臉部關鍵點**，特別是眼睛、瞳孔、眼角、鼻子和臉部輪廓的座標。
        
    2. （可選）裁剪出的雙眼區域的圖像塊 (image patch)。
        
- **模型輸出:** 一個三維向量 (v![](data:image/svg+xml;utf8,<svg%20xmlns="http://www.w3.org/2000/svg"%20width="0.471em"%20height="0.714em"%20style="width:0.471em"%20viewBox="0%200%20471%20714"%20preserveAspectRatio="xMinYMin"><path%20d="M377%2020c0-5.333%201.833-10%205.5-14S391%200%20397%200c4.667%200%208.667%201.667%2012%205
    3.333%202.667%206.667%209%2010%2019%206.667%2024.667%2020.333%2043.667%2041%2057%207.333%204.667%2011
    10.667%2011%2018%200%206-1%2010-3%2012s-6.667%205-14%209c-28.667%2014.667-53.667%2035.667-75%2063
    -1.333%201.333-3.167%203.5-5.5%206.5s-4%204.833-5%205.5c-1%20.667-2.5%201.333-4.5%202s-4.333%201
    -7%201c-4.667%200-9.167-1.833-13.5-5.5S337%20184%20337%20178c0-12.667%2015.667-32.333%2047-59
    H213l-171-1c-8.667-6-13-12.333-13-19%200-4.667%204.333-11.333%2013-20h359
    c-16-25.333-24-45-24-59z"></path></svg>)) 或兩個角度 (θ,ϕ)，代表視線在三維空間中的方向。
    
- **主流模型架構:**
    
    - **基於幾何的方法:** 早期方法，通過眼睛、瞳孔等關鍵點的幾何關係來估算視線方向。不夠魯棒。
        
    - **基於學習的方法:**
        
        - **CNN (卷積神經網絡):** 如果輸入包含眼睛圖像塊，通常會用一個輕量級的CNN（如ResNet-18的變體）來提取圖像特徵。
            
        - **MLP (多層感知機):** 將臉部關鍵點的**相對座標**（例如，瞳孔相對於眼角的座標）和CNN提取的圖像特徵拼接起來，送入一個MLP中，最終回歸出視線向量。
            
- **訓練:**
    
    - **數據:** 需要在嚴格控制的實驗室環境下採集的數據集，其中包含用戶頭部姿態、眼睛圖像和他們正在注視的屏幕上的真實點位（如MPIIGaze）。
        
    - **損失函數:** 通常是**角度損失 (Angular Loss)**，計算預測視線向量和真實視線向量之間的夾角差異，或者直接用MSE Loss計算向量的差異。
        

---

### 具體案例：基於骨架的智能安防跌倒檢測系統

我們以**跌倒檢測**為例，一步步說明整個流程。

**目標:** 當監控畫面中的老人發生跌倒時，系統自動發出警報。

**第一步：姿態數據流的獲取 (MMPose)**

- 系統持續接收監控攝影機的影片流。
    
- 對於每一幀，運行MMPose的Top-down流程（例如 YOLOv3 + HRNet）。
    
- **輸出:** 為畫面中的每個人生成一組17個關鍵點的座標 `[x, y, conf]`。這些數據被持續存儲起來。
    

**第二步：為動作識別模型準備輸入**

- **滑動窗口:** 系統維護一個長度為 `T`（例如`T=32`幀，約1-2秒）的滑動窗口。每當有新的一幀數據進來，就將其加入窗口，並移除最老的一幀。
    
- **數據預處理:**
    
    1. 從窗口中提取一個人的32幀骨架數據，形成一個 `(32, 17, 3)` 的張量。
        
    2. **歸一化:** 在每一幀中，找到髖關節中心點的座標 `(x_hip, y_hip)`，然後將所有17個關節點的座標減去這個中心點座標。這使得數據與該人在畫面中的絕對位置無關。
        
    3. **特徵選擇:** 為了簡化，我們只使用歸一化後的 `(x, y)` 座標，輸入張量變為 `(32, 17, 2)`。
        

**第三步：使用ST-GCN模型進行推斷**

- **模型加載:** 系統預先加載一個已經在大型動作識別數據集（如NTU-RGB+D）上預訓練過，並在特定的跌倒數據集上進行了**微調 (Fine-tuning)** 的ST-GCN模型。該模型被訓練來區分三種動作："站立/行走", "坐/躺", "正在跌倒"。
    
- **模型輸入:** 將上一步準備好的 `(32, 17, 2)` 張量送入ST-GCN模型。
    
- **模型內部工作流程 (簡化):**
    
    1. 模型首先在空間維度上，通過圖卷積分析每一幀的姿態。例如，當膝蓋彎曲且身體重心急劇下降時，空間圖卷積層會捕捉到這種 "不穩定" 的身體結構特徵。
        
    2. 接著，在時間維度上，時間卷積層會分析這些特徵隨時間的劇烈變化。例如，它會識別出人體質心在垂直方向上的速度突然變得非常大，這正是跌倒的典型特徵。
        
- **模型輸出:** 模型會輸出一組概率值，例如：`{'站立/行走': 0.1, '坐/躺': 0.05, '正在跌倒': 0.85}`。
    

**第四步：觸發應用邏輯**

- **決策:** 系統設定一個閾值，例如0.8。
    
- 當 "正在跌倒" 類別的概率超過0.8時，系統判定發生了跌倒事件。
    
- **執行動作:** 系統立即觸發警報，例如向家庭成員或護理中心發送帶有事件截圖的通知。
    

這個例子完整地展示了從MMPose的原始像素分析，到骨架數據提取，再到利用先進的時空模型（ST-GCN）進行高級行為理解的全過程。
