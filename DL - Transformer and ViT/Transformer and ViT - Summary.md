

|                                            |                                                                                                             |
| ------------------------------------------ | ----------------------------------------------------------------------------------------------------------- |
| [[Transformer and ViT - architecture]]     |                                                                                                             |
|                                            | [[Transformer and ViT - positional encoding]]                                                               |
| [[Transformer and ViT - encoder &decoder]] |                                                                                                             |
| Encoder                                    | [[Transformer and ViT - ViT Encoder block]]<br><br>Multi-head self attention<br>Feed Forward neural network |
|                                            | [[Transformer and ViT - Attention]]                                                                         |
|                                            |                                                                                                             |
| Decoder                                    | Masked Multi-head                                                                                           |
|                                            |                                                                                                             |
| [[### QA list]]                            |                                                                                                             |

![[transformer.webp]]



詳細解釋一下Vision Transformer (ViT) 在圖像分割中全局注意力的意義，以及Q, K, V 在圖像 Patch 中的角色。

**1. 圖像分 Patch 與 Token**

在 Vision Transformer 中，一張圖像首先會被分割成一系列固定大小的小圖像塊 (Patches)。例如，一張 224x224 像素的圖像，如果 Patch 大小是 16x16 像素，那麼就會得到 (224/16) * (224/16) = 14 * 14 = 196 個 Patches。

每個 Patch 接著會被展平 (flatten) 並通過一個線性投射層 (Linear Projection Layer) 轉換成一個固定維度的向量，這個向量就被視為一個 "Token"，類似於自然語言處理 (NLP) 中的詞彙 Token。此外，還會加入位置編碼 (Positional Encoding) 來保留 Patch 之間的空間位置信息。

**2. 两两计算全局注意力的真实意义是什么？是指两个图片 Patch 之间的相似性吗？**

是的，可以將全局注意力計算的過程理解為**衡量任意兩個圖像 Patch 之間在特定上下文中的“相關性”或“相似性”**。但這裡的“相似性”並非簡單的像素級別的直接相似，而是更高維度、更抽象的語義或特徵層面的相似性。

- **真實意義：** 全局注意力的核心思想是讓模型中的每個 Patch (Token) 都能夠“看到”並“評估”圖像中的所有其他 Patches，從而決定哪些 Patches 對於理解當前 Patch 的內容和上下文最為重要。它允許模型捕捉圖像中長距離的依賴關係。 對於一個特定的 Patch A，全局注意力機制會計算 Patch A 與圖像中所有其他 Patches (包括它自己) 的注意力分數。這個分數越高，代表其他 Patches 對於更新 Patch A 的表徵 (representation) 越重要。
    
- **如何計算（簡化版）：** 對於一個查詢 Patch Q (Query)，它會和所有其他 Patches 的鍵 K (Key) 進行比較（通常是點積運算）。這個比較結果會經過一個 Softmax 函數，轉換成一組權重。這些權重就代表了 Q 對於所有其他 Patches 的注意力分佈。權重越大的 K 所對應的 Patch，就被認為與 Q 更“相關”或“相似”。
    

**舉個簡單例子：** 想像一張圖裡有一隻貓和一個球。

- 一個包含貓耳朵的 Patch，在計算注意力時，可能會對包含貓頭部其他部分的 Patches、貓身體的 Patches 產生較高的注意力分數，因為它們在語義上高度相關（都屬於“貓”這個物體）。
- 它對遠處那個球的 Patch 可能注意力分數較低，除非在特定任務中，貓和球的互動關係很重要。
- 它對背景草地的 Patch 注意力分數可能更低。

所以，這種“相似性”是模型在學習過程中動態學習到的，服務於當前任務（如圖像分割）的特徵層面的相關性。

**3. 这跟 Image Segmentation 有什么关系？**

全局注意力機制對於圖像分割任務非常有幫助，主要體現在以下幾點：

- **捕捉長距離依賴關係 (Long-Range Dependencies)：** 圖像分割的目標是為圖像中的每個像素（或每個 Patch）分配一個類別標籤。很多時候，同一個物體的不同部分可能在圖像中相距較遠（例如，一條蛇的頭和尾巴）。傳統的卷積神經網絡 (CNN) 由於其局部感受野的限制，需要堆疊很多層才能捕捉到這種長距離關係。而全局注意力機制允許任何兩個 Patch 直接交互，無論它們在圖像中的距離有多遠。這使得模型能更好地理解物體的完整結構，即使物體被部分遮擋或形狀不規則。
    
- **理解全局上下文 (Global Context)：** 一個 Patch 的語義不僅取決於其自身內容，還取決於它在整個圖像中的上下文。例如，一個棕色的圓形 Patch，如果周圍的 Patches 是樹幹和樹葉，它可能被識別為樹木的一部分；如果周圍的 Patches 是車輪和車窗，它可能被識別為汽車的輪胎。全局注意力允許每個 Patch 整合來自整個圖像的信息，從而做出更準確的判斷。
    
- **提升分割一致性：** 通過讓屬於同一物體的不同 Patches 之間產生強烈的相互注意力，模型可以學習到這些 Patches 應該被賦予相同的分割標籤，從而提高分割結果的內部一致性和平滑性。例如，如果模型確定某個 Patch 是“汽車”，那麼與之高度相關的其他 Patches（如車門、車窗、車輪）也更有可能被正確標記為“汽車”。
    
- **區分相似但不同類別的物體：** 有時，不同的物體可能在局部看起來很相似。全局上下文可以幫助區分它們。例如，一個灰色的 Patch 可能是路面，也可能是建築物的牆壁。通過觀察與這個 Patch 相關聯的其他 Patches（是天空還是其他車輛？），模型可以更準確地判斷其類別。
    

**4. Multi-Head Self-Attention 的 Q, K, V 在图片 Patch 的意义是什么？**

在自注意力機制 (Self-Attention) 中，每個輸入的 Patch Token 會生成三個不同的向量：查詢 (Query, Q)，鍵 (Key, K)，和值 (Value, V)。這三個向量是通過將原始的 Patch Embedding 乘以三個不同的、可學習的權重矩陣 (W_q, W_k, W_v) 得到的。

- **Query (Q) - 查詢向量：**
    
    - **意義：** 代表當前 P<mark style="background: #FFF3A3A6;">atch 正在“尋找什麼信息”或“對什麼感興趣</mark>”。可以把它想像成當前 Patch 提出的一個問題或一個查詢請求。
    - **例子：** 一個包含貓眼睛的 Patch，它的 Q 向量可能編碼了“<mark style="background: #FFF3A3A6;">我是一個物體的關鍵部分，請告訴我與我相似的其他部分在哪裡</mark>，或者與我構成同一物體的其他部分在哪裡”這樣的信息。它想知道其他 Patches 中有哪些特徵與“貓眼”這個概念相關。
- **Key (K) - 鍵向量：**
    
    - **意義：** 代表該 Patch “<mark style="background: #ABF7F7A6;">擁有什麼樣的信息”或“能提供什麼樣的標識特徵</mark>”。它是用來和其他 Patches 的 Q 向量進行匹配的。
    - **例子：** 還是那個貓眼睛的 Patch，它的 K 向量可能編碼了“<mark style="background: #ABF7F7A6;">我包含貓眼特有的紋理、形狀和顏色</mark>”這樣的信息。對於一個包含貓耳朵的 Patch，它的 K 向量則會編碼貓耳朵的特徵。
- **Value (V) - 值向量：**
    
    - **意義：** 代表該 Patch “<mark style="background: #FFB86CA6;">實際攜帶的內容或信息</mark>”。一旦 Q 和 K 匹配成功（即注意力權重較高），那麼對應的 V 向量就會被用來更新查詢 Patch 的表徵。它是實際被提取和聚合的信息。
    - **例子：** 貓眼睛 Patch 的 V 向量攜帶了關於貓眼睛的豐富特徵表示。如果貓耳朵 Patch 的 Q 與貓眼睛 Patch 的 K 高度匹配，那麼貓眼睛 Patch 的 V 向量就會在很大程度上貢獻給貓耳朵 Patch 更新後的特徵表示。

**計算過程簡述：**

1. 對於一個特定的 Patch (稱為 Patch_i)，生成其 Q_i。
2. 對於圖像中的所有 Patches (包括 Patch_i 自身，稱為 Patch_j)，生成它們的 K_j 和 V_j。
3. 計算 Q_i 與所有 K_j 的點積，得到“注意力分數”（`score_ij = Q_i · K_j`）。這個分數衡量了 Patch_i 的查詢與 Patch_j 的鍵之間的匹配程度。
4. 將這些分數進行縮放（通常除以 Q、K 向量維度的平方根）並通過 Softmax 函數歸一化，得到注意力權重 (attention weights, `alpha_ij`)。`alpha_ij` 表示 Patch_i 應該對 Patch_j 的信息 V_j 投入多少注意力。
5. 用這些注意力權重 `alpha_ij` 對所有 V_j 進行加權求和，得到 Patch_i 的新表徵：`Output_i = Σ (alpha_ij * V_j)`。

**Multi-Head Self-Attention (多頭自注意力)：** 多頭機制不是只有一組 Q, K, V 的權重矩陣，而是有多組（例如 8 組或 12 組）。每一組（稱為一個“頭”）都獨立地執行上述的自注意力計算。

- **意義：**
    
    - **從不同子空間學習信息：** 不同的頭可以學會關注 Patch 之間不同類型的關係或特徵。例如，一個頭可能專注於紋理相似性，另一個頭可能專注於顏色相似性，還有一個頭可能專注於空間位置關係或者物體的組成部分關係。
    - **更豐富的表徵能力：** 將多個頭的輸出拼接起來（Concat）然後再進行一次線性變換，可以讓模型從多個角度、多個方面捕捉 Patch 之間複雜的依賴關係，從而得到更豐富、更魯棒的特徵表徵。
- **例子（續貓和球）：**
    
    - **頭1 (紋理頭)：** 貓耳朵 Patch 的 Q 可能在尋找相似毛髮紋理。它會高度關注貓身體其他毛髮 Patch 的 K，並聚合它們的 V。
    - **頭2 (形狀頭)：** 貓耳朵 Patch 的 Q 可能在尋找符合貓科動物輪廓的形狀。
    - **頭3 (顏色頭)：** 如果貓是特定顏色的，這個頭可能關注顏色一致性。
    - **頭4 (長距離關係頭)：** 可能會學習到“頭部”和“尾部”雖然形態不同，但經常共同出現，屬於同一物體。

最終，通過這種全局多頭自注意力機制，每個 Patch 的表徵都融合了來自圖像中所有其他 Patches 的、在多個語義層面上的相關信息。這些經過豐富和優化的 Patch 表徵隨後被送入後續的網絡層（通常是前饋神經網絡），並最終用於圖像分割任務中的像素級（或 Patch 級）分類。

總結來說，Vision Transformer 中的全局注意力機制通過計算 Patch 之間的動態相關性，使得模型能夠理解圖像的整體結構和上下文信息，這對於需要精確識別和分割物體的圖像分割任務至關重要。而 Q, K, V 的引入，則為這種相關性計算提供了一個靈活且強大的框架，多頭機制進一步增強了其從不同維度捕捉信息的能力。


### QA list

| Q                                                                       | Ans                                                                                                                                                                                                            |
| ----------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ViT将图像分割为固定大小的patch后，如何将这些patch转换为适合Transformer处理的序列？能否详细描述预处理流程中的关键步骤？ | 对patch进行线性embedding，维度变成模型hidden dims，并且与1D可学习的位置编码直接相加（element-wise add）。<br><br>预处理过程中要考虑恰当的patch大小，经过embedding之后，为了和NLP中的架构尽可能相似，会额外添加一个 `class token`与 `patch embeddings`进行拼接，最后和1D可学习的位置编码参数直接相加作为编码层的输入。 |
| ViT的位置编码与传统NLP中的位置编码有何不同？如果不使用位置编码，会对模型性能产生什么影响？                        | Transformer中使用的位置编码是根据位置信息计算正余弦值，而ViT中选择1D位置编码参数，是可学习的；论文中对不使用位置编码的情况进行了对比实验，会导致模型性能稍有下降（大约下降3个百分点）。                                                                                                           |
| ViT的"分类token"（class token）在模型中的作用是什么？为什么需要将它添加到patch序列中？                | 分类token在前向计算过程中会融合图像的全局信息，最终作为图像特征输入到分类头中，完成分类任务。<br><br>原论文中提到为了和NLP（BERT模型）中的设置尽可能相似，将其与patches进行拼接，作为编码器的输入，不过实验证明，如果不使用 `class token`、而是为分类头输入编码器的输出，也可得到相近的性能。                                            |
| ViT与CNN的核心区别是什么？为什么ViT在大规模数据集上表现更好，但在小数据集上容易过拟合？                        | CNN中依赖卷积操作提取特征，其中有局部性、平移不变性等先验知识，而ViT中仅引入了非常少的图像先验知识，完全依靠模型学习图像patch之间的关系。                                                                                                                                     |
| 自注意力机制在图像处理中可能面临哪些计算效率问题？ViT如何通过patch划分缓解这一问题？                          | 如果将图像中的每个像素看作token输入到模型中，QKV的计算时会面临大矩阵乘法问题，导致计算效率降低，而ViT通过划分patch、将每个patch作为token，减少了token的序列长度，从而缓解了上面的大矩阵乘法问题。<br><br>当输入图像分辨率与预训练模型不匹配时，ViT需要如何调整？试解释位置编码插值的实现逻辑<br><br>在原始的预训练得到的位置编码的基础上，根据图像位置的关系进行2D插值。 |
| 从ViT的实验结果来看，为什么它在ImageNet-21k等大规模数据集上的表现优于CNN？这种优势是否能迁移到小规模数据集？         | ViT通过全局自注意力机制，可以在不引入先验知识的情况下，学习到图像的通用特征，因此在大规模数据上性能优于CNN；同时如果使用少样本微调，ViT的性能也好过CNN。<br><br>在工业场景（如医学影像分析或自动驾驶）中部署ViT时，可能面临哪些实际挑战？列举三种优化策略并说明原理                                                                  |
| self attention和 cross attention 的区别                                     |                                                                                                                                                                                                                |
| LoRA的实现原理                                                               |                                                                                                                                                                                                                |
| 了解 Transformer 吗，编码器和解码器的注意力有什么区别，在计算注意力中时除以 \sqrt{d_k} 的原因是什么          |                                                                                                                                                                                                                |
| 简单介绍下Transformer                                                        |                                                                                                                                                                                                                |
| 大概讲一下Transformer的Encoder模块？                                             |                                                                                                                                                                                                                |
| 为什么transformer块使用LayerNorm而不是BatchNorm？                                 |                                                                                                                                                                                                                |
| Transformer为何使用多头注意力机制？                                                 |                                                                                                                                                                                                                |
| Coding>> **手写multi-head attention**                                     |                                                                                                                                                                                                                |
| Coding>> **手写self attention**                                           |                                                                                                                                                                                                                |
| 解释什么是自注意力机制                                                             |                                                                                                                                                                                                                |
| 很多大模型 decoder-only原理                                                    |                                                                                                                                                                                                                |
| Cross-attention用法                                                       |                                                                                                                                                                                                                |
| 注意力的公式                                                                  |                                                                                                                                                                                                                |
| 幾種position embedding                                                    |                                                                                                                                                                                                                |
| 為什麼大模型LLM 最主流的預訓練方式是自回歸                                                 |                                                                                                                                                                                                                |
| Attention的原理, Attention有什么缺点                                            |                                                                                                                                                                                                                |
| VIT直接用于分割检测等预测密集型的任务上存在什么问题？                                            |                                                                                                                                                                                                                |
| VIT中对输入图像的处理是如何将patch变化为token的？                                         |                                                                                                                                                                                                                |
| Transformer的注意力机制常用softmax函数，可以使用sigmoid代替吗？                            |                                                                                                                                                                                                                |
| ViT、DEIT是如何处理变长序列输入的？                                                   |                                                                                                                                                                                                                |
| 局部注意力如何实现                                                               |                                                                                                                                                                                                                |
| 神经网络引入注意力机制后效果降低的原因                                                     |                                                                                                                                                                                                                |
| Attention计算复杂度以及如何改进                                                    |                                                                                                                                                                                                                |
| 为什么transformer中使用LayerNorm而不是BatchNorm                                  |                                                                                                                                                                                                                |
| Transformer为何使用多头注意力机制                                                  |                                                                                                                                                                                                                |
