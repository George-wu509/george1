
Ref1: [multi-view-3d-reconstruction github](https://github.com/XYZ-qiyh/multi-view-3d-reconstruction)

**空間校準（Spatial Calibration）**、**機器視覺（Machine Vision）**、**深度感測（Depth Sensing）**、**3D幾何（3D Geometry）**、**多視角幾何（Multi-view Geometry）**、**立體相機（Stereo Camera）**、**飛行時間（Time of Flight, ToF）**、**多相機系統（Multi-camera Systems）**、**物體追蹤（Object Tracking）**等技術，同時結合**角膜交聯設備（Corneal Cross-linking Device）

三维重建根据所用传感器的不同，可以分为**主动式三维重建**和**被动式三维重建**。

主动式三维重建根据传感器去主动探测深度信息，常用的传感器包括激光雷达（LiDAR），结构光（Structured Light）和ToF（Time-of-Fight, 飞行时间）主动式三维重建适用的场景受限，而且通常硬件设备价格昂贵。

而被动式三维重建通常只需要相机，而且适用场景较为广泛. 被动式三维重建根据算法输入视图数目的不同，可以分为单目深度估计(Monocular Depth Estimation)、双目立体匹配(Stereo matching)和多视图三维重建(MVS, Multi-View Stereo)三种方式。

主动式三维重建  -    激光雷达（LiDAR）
                结构光（Structured Light）
                ToF（Time-of-Fight, 飞行时间）
            
被动式三维重建  -    单目深度估计(Monocular Depth Estimation)
				双目立体匹配(Stereo matching)
				多视图三维重建(MVS, Multi-View Stereo)



**Stereo Matching和MVS的区别**

- 数据获取：立体匹配通常使用双目相机进行拍摄，而MVS采集的数据通常为相机在不同视角下拍摄的多视角图像（或在连续视频流中采样得到的视频帧）
- 输入视图数：顾名思义，立体匹配的输入为两幅（校正后的）图像，计算视差后通过相机基线 _b_ 和焦距 _f_ 将视差 _disparity_ 转为深度值 _depth_
- 更具体地，MVS的步骤中涉及视图选择，即选取哪些邻域视图用于相似性搜索（图像之间的夹角以及稀疏点之间的重叠度）。

### 基于多视角图像的三维重建

基于图像的三维重建系统的输入是一组具有重叠区域的多视角图像，首先通过**运动恢复结构**（Structure-fromMotion, SfM） 为输入图像进行相机位姿估计，同时得到场景的稀疏点云信息。然后**多视图立体匹配**算法（Multi-view Stereo, MVS） 用于稀疏重建的稠密化， 重建结果为场景的稠密点云模型。 如需获得三维场景的表面网格模型，则需要对重建得到的点云进行**表面重建**（Suface Reconstruction）。

![[incremental-sfm.png]]

### **思路 1：基於 SfM 和 MVS 的三維重建，結合 PointNet++ 進行 3D 分割**

#### **流程**：

1. **多視圖三維重建：**
	基於 SfM（Structure from Motion）和 MVS（Multi-View Stereo）的三維重建方法是一種傳統且精確的計算機視覺技術。這種方法通過多視角圖像提取相機位姿和三維結構（點雲），再進一步生成稠密點雲和表面模型。
	
    - 使用傳統的 **Structure from Motion (SfM)** 提取多視角圖像的相機位姿和稀疏點雲。
    - 使用 **Multi-View Stereo (MVS)** 從稀疏點雲生成稠密點雲。
    - 通過表面重建（例如 Poisson Surface Reconstruction）生成三維網格模型。
    
1. **AI 驅動的 3D 分割：**
    - 將重建的稠密點雲輸入 **PointNet++**（一種針對點雲的深度學習網絡），進行語義分割或實例分割。
    - 使用分割結果標記 3D 網格上的區域。

#### **模型和技術**：
- 重建模型：OpenMVG（SfM）、COLMAP（MVS）、Meshlab（表面重建）。
- 分割模型：PointNet++。
#### **優點**
- 傳統三維重建方法成熟，結果精度高。
- PointNet++ 對點雲有很強的處理能力，分割準確。
#### **缺點**：
- 計算量大，對大規模場景需要高性能硬件。
- 對於不均勻密度點雲的分割可能有挑戰。

#### **2. 流程詳細步驟**

##### **步驟 1：特徵點提取與匹配**

- **原理**：對每張圖像提取特徵點（例如使用 SIFT、SURF、ORB 等算法），並在圖像之間進行匹配以找到共視點。
- **輸入**：多張帶有重疊區域的圖像。
- **輸出**：特徵點及其匹配對。
- **工具**：OpenMVG, COLMAP。
- **示例**：
    - 圖像 1 與圖像 2 的重疊區域找到特徵點 A 和 B，匹配對應位置。

##### **步驟 2：相機姿態估計（SfM）**

- **原理**：使用特徵點匹配對，估算圖像之間的相對相機位姿（內參和外參），同時計算稀疏點雲。
- **輸入**：特徵點匹配對。
- **輸出**：相機位姿（內參、外參矩陣）和稀疏三維點雲。
- **工具**：COLMAP, VisualSFM。
- **示例**：
    - 圖像 1 的相機位姿為 (R1, t1)，圖像 2 的相機位姿為 (R2, t2)，稀疏點雲包含 [X, Y, Z]。
##### **步驟 3：稠密點雲生成（MVS）**

- **原理**：利用 SfM 提供的相機位姿，對所有圖像進行立體匹配，生成稠密點雲。
- **輸入**：相機位姿、圖像序列。
- **輸出**：稠密點雲。
- **工具**：OpenMVS, COLMAP。
- **示例**：
    - 稀疏點雲包含幾千個點，稠密點雲可能包含數百萬個點。
##### **步驟 4：表面重建**

- **原理**：使用稠密點雲生成物體表面，常用方法包括 Poisson Surface Reconstruction 或 Delaunay Triangulation。
- **輸入**：稠密點雲。
- **輸出**：3D 表面網格（mesh）。
- **工具**：Meshlab, PoissonRecon。
- **示例**：
    - 生成物體如雕像的網格模型，包含連續的表面。
##### **步驟 5：AI 分割**

- **原理**：使用深度學習模型（例如 PointNet++）對點雲進行語義分割或實例分割。
- **輸入**：稠密點雲。
- **輸出**：分割後的點雲（帶標籤）。
- **工具**：PointNet++。
- **示例**：
    - 將雕像點雲分為「底座」、「人物」兩部分。

### **思路 3：基於神經重建和神經場（NeRF）技術，結合 Transformer-based 分割模型**

#### **流程**：

1. **多視圖三維重建：**
    NeRF（Neural Radiance Field）是一種基於深度學習的神經重建技術。它學習一個連續的三維場景表示，通過神經網絡對空間中的每一點的顏色和密度進行建模，並支持高品質的渲染與重建。
    - 使用 **NeRF（Neural Radiance Field）** 技術從多視角圖像生成一個連續場表示，學習場景中每個位置的顏色和密度。
    - 通過場景渲染生成點雲或網格。
2. **AI 驅動的 3D 分割：**

    - 使用 Transformer-based 模型（例如 **Segmenter3D** 或其他基於 Vision Transformer 的 3D 分割模型）對重建的 3D 數據進行分割。
#### **模型和技術**：
- 重建模型：Instant-NGP 或 NeRF。
- 分割模型：Segmenter3D，可能結合 Vision Transformer。

#### **優點**：
- NeRF 能生成高品質的 3D 表示，適合處理複雜場景。
- Transformer-based 模型對大尺度 3D 數據有良好的捕捉能力。
#### **缺點**：
- NeRF 訓練時間長，推理需要大量計算資源。
- Transformer-based 模型對於大規模點雲需要優化內存使用。

#### **2. 流程詳細步驟**

##### **步驟 1：數據準備**

- **原理**：從多視角圖像中提取相機參數（內參和外參），這些參數用於指導網絡學習。
- **輸入**：多視角 RGB 圖像及其相機位姿。
- **輸出**：相機參數矩陣。
- **工具**：COLMAP（提取相機參數）。
- **示例**：
    - 圖像 1：相機位姿 (R1, t1)，圖像 2：相機位姿 (R2, t2)。
##### **步驟 2：神經場建模**

- **原理**：設計一個神經網絡（MLP），學習映射三維坐標 (x,y,z)(x, y, z)(x,y,z) 到 RGB 和密度值。
    - 輸入：三維空間坐標 (x,y,z)(x, y, z)(x,y,z) 和觀察方向 (θ,ϕ)(\theta, \phi)(θ,ϕ)。
    - 輸出：顏色 (R,G,B)(R, G, B)(R,G,B) 和密度值 σ\sigmaσ。
- **輸入**：三維坐標和相機位姿。
- **輸出**：三維輻射場。
- **工具**：Instant-NGP（NeRF 加速版本）。
- **示例**：
    - 一點坐標 (1.2, 3.4, 5.6) 被映射為 (R, G, B, σ\sigmaσ) = (0.9, 0.8, 0.7, 0.1)。
##### **步驟 3：圖像渲染與點雲提取**

- **原理**：使用體積渲染（Volume Rendering）將神經場中的輻射場轉換為圖像或點雲。
- **輸入**：學習到的輻射場。
- **輸出**：重建的圖像或點雲。
- **工具**：NeRF 原生渲染模組。
- **示例**：
    - 從某一視角生成的圖像與真實圖像一致。
##### **步驟 4：表面提取**

- **原理**：將輻射場轉換為點雲，再進一步重建網格表面。
- **輸入**：神經場輸出的點雲。
- **輸出**：三維網格（mesh）。
- **工具**：PoissonRecon 或 Open3D。
- **示例**：
    - 雕像的表面網格被完整重建。
##### **步驟 5：AI 分割**

- **原理**：使用 Transformer-based 分割模型（如 Segmenter3D），對 3D 數據進行語義分割。
- **輸入**：三維網格或點雲。
- **輸出**：分割後的三維數據。
- **工具**：Segmenter3D, 3D Swin Transformer。
- **示例**：
    - 將雕像分為「頭部」、「身體」和「基座」。

---

### **比較**

| **方法**                 | **重建精度** | **分割效果** | **計算成本** | **硬件需求** | **適用場景**      |
| ---------------------- | -------- | -------- | -------- | -------- | ------------- |
| 思路 1（SfM+PointNet++）   | 高        | 高        | 中        | 中至高      | 小型或中型場景，物體清晰  |
| 思路 2（RGB-D+3D UNet）    | 中        | 中至高      | 中至高      | 高        | 中型場景，深度圖質量高   |
| 思路 3（NeRF+Transformer） | 非常高      | 高        | 高        | 非常高      | 高度複雜的場景，視覺細節多 |

這三種思路根據場景需求和硬件資源進行選擇。如果硬件資源允許且場景複雜，建議使用思路 3；如果硬件有限，可以考慮思路 1 或 2。