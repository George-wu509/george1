
|                                       |     |
| ------------------------------------- | --- |
| [[###**1. MobileNet**]]               |     |
| [[###**2. EfficientNet**]]            |     |
| [[###- **YOLOv5-Lite/YOLOv8-Nano:**]] |     |
| [[###SSD-MobileNet]]                  |     |






**輕量化 CNN 的核心概念：**

輕量化 CNN 的設計目標是在保持模型準確度的前提下，大幅降低模型的計算複雜度和參數數量，使其能夠在資源有限的裝置（例如手機、嵌入式系統）上高效運行。

### **1. MobileNet**

詳細介紹一下 MobileNet 系列模型的特點。MobileNet 是一系列針對移動設備和嵌入式設備等資源受限環境設計的輕量級卷積神經網路 (CNN) 架構。其核心目標是在保持可接受精度的前提下，顯著減小模型尺寸和提高推理速度。

以下是 MobileNet 系列模型 (包括 V1, V2, V3 等) 的主要共同特點和一些隨著版本迭代而引入的改進：

**1. 深度可分離卷積 (Depthwise Separable Convolution): 核心組件**

- **分解標準卷積:** 傳統的標準卷積操作會同時學習空間相關性 (在卷積核的寬度和高度上) 和通道相關性 (跨輸入和輸出通道)。深度可分離卷積(Depthwise Separable Convolution)將這個過程分解為兩個獨立的步驟：
    
    - **深度卷積 (Depthwise Convolution):** 對每個輸入通道獨立地應用一個二維卷積核。每個卷積核的大小通常是 Dk​×Dk​×1，其中 Dk​ 是卷積核的空間維度。如果輸入有 M 個通道，那麼這個步驟會產生 M 個輸出通道。
    - **逐點卷積 (Pointwise Convolution / 1x1 Convolution):** 使用 1x1 的卷積核對深度卷積的輸出進行線性組合，以跨通道建立相關性並改變通道數。如果深度卷積的輸出有 M 個通道，並且我們希望輸出 N 個通道，那麼我們會使用 N 個 1×1×M 的卷積核。
- **計算成本降低:** 深度可分離卷積顯著降低了計算成本和模型參數數量。假設輸入特徵圖的大小為 DF​×DF​×M，輸出通道數為 N，卷積核大小為 Dk​×Dk​。
    
    - **標準卷積的計算複雜度:** Dk​×Dk​×M×N×DF​×DF​
    - **深度可分離卷積的計算複雜度:** (Dk​×Dk​×M×1×DF​×DF​)+(1×1×M×N×DF​×DF​)
    - 計算複雜度的比率約為：N1​+Dk2​1​。在實際應用中，N 和 Dk2​ 通常都遠大於 1，因此深度可分離卷積可以大幅減少計算量。

**2. 輕量級網路架構:**

- MobileNet 系列模型通常採用精簡的網路結構，包含較少的層和更小的卷積核。
- 早期版本 (V1) 主要由堆疊的深度可分離卷積層構成。

**3. 超參數以平衡精度和效率:**

- **寬度乘數 (Width Multiplier, α):** 這是一個介於 0 和 1 之間的超參數，用於控制網路的寬度 (即每個層的通道數)。通過減小 α，可以構建更小、更快的模型，但精度可能會有所下降。
- **分辨率乘數 (Resolution Multiplier, ρ):** 這也是一個介於 0 和 1 之間的超參數，用於控制輸入圖像和內部特徵圖的分辨率。降低分辨率可以減少計算量，但同樣可能影響精度。

補充:

|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |     |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --- |
| **Width Multiplier (α) 和 Resolution Multiplier (ρ) 這兩個超參數最初是 MobileNet (V1) 論文中提出的，用於在模型尺寸、速度和精度之間進行權衡。**<br><br>EfficientNet 確實也使用了超參數來進行模型縮放，但它採用的是一種**複合縮放 (Compound Scaling)** 的方法，**同時**調整網路的**深度 (depth, d)、寬度 (width, w) 和分辨率 (resolution, r)**，並使用一個統一的**縮放因子 (ϕ)** 來控制縮放的程度。EfficientNet 的縮放規則如下：<br><br>depth:dwidth:wresolution:r​=αϕ=βϕ=γϕ​<br><br>其中 α,β,γ 是通過在一個小型基準模型上進行網格搜索確定的常數，它們分別控制著深度、寬度和分辨率的縮放比例。ϕ 是一個用戶可調整的係數，用於控制模型的整體大小和計算預算。<br><br>**因此，雖然 MobileNet 和 EfficientNet 都使用超參數來調整模型的尺寸和性能，但它們的方法是不同的：**<br><br>- **MobileNet:** 主要使用 **Width Multiplier (α)** 來調整通道數，以及 **Resolution Multiplier (ρ)** 來調整輸入圖像和內部特徵圖的大小，這兩個是相對獨立的超參數。<br>- **EfficientNet:** 使用一個**複合縮放因子 (ϕ)**，**同時**按比例調整模型的深度、寬度和分辨率，其比例由 α,β,γ 這三個常數決定。<br><br>我為之前的混淆表示歉意，感謝您的指正！這是一個重要的區別。<br><br>**總結來說：**<br><br>- **Width Multiplier 和 Resolution Multiplier 最初是 MobileNet 的概念。**<br>- **EfficientNet 採用的是更複雜的複合縮放方法，同時調整深度、寬度和分辨率。** |     |

**MobileNet V2 的改進:**

- **反向殘差塊 (Inverted Residual Block):** MobileNet V2 引入了反向殘差結構。與傳統的殘差塊 (窄通道 -> 寬通道 -> 窄通道) 不同，反向殘差塊的結構是：窄通道 (通過 1x1 卷積擴展通道數) -> 深度卷積 -> 窄通道 (通過 1x1 卷積縮減通道數)。
- **線性瓶頸 (Linear Bottleneck):** 在反向殘差塊的最後一個 1x1 卷積層之後，MobileNet V2 **不使用** ReLU 激活函數。這是因為 ReLU 在低維空間中可能會丟失過多的信息。這個沒有激活函數的層稱為線性瓶頸。
- **跳躍連接 (Skip Connection):** 與 ResNet 類似，MobileNet V2 的反向殘差塊也使用了跳躍連接，但只在輸入和輸出具有相同通道數時才使用。這有助於梯度傳播和模型訓練。

**MobileNet V3 的改進:**

- **網路架構搜索 (Network Architecture Search, NAS):** MobileNet V3 的架構部分是由 NAS 算法自動搜索得到的，旨在找到在精度和效率之間更優的平衡。
- **硬激勵函數 (Hard-swish and Hard-sigmoid):** MobileNet V3 引入了硬 swish (h-swish) 和硬 sigmoid (h-sigmoid) 激活函數，它們是 swish 和 sigmoid 的近似，但計算成本更低，更適合在移動設備上部署。
    - h−swish(x)=x6ReLU6(x+3)​
    - h−sigmoid(x)=6ReLU6(x+3)​
- **Squeeze-and-Excitation (SE) 模塊:** MobileNet V3 在某些層中引入了 SE 模塊，用於自適應地重新校準通道的特徵響應，提高模型的表示能力。
- **更精細的架構設計:** MobileNet V3 針對不同的使用場景 (例如小資源 vs. 高精度) 設計了不同的模型變體 (MobileNetV3-Small 和 MobileNetV3-Large)。

**MobileNet 系列的共同優點:**

- **模型尺寸小:** 由於使用了深度可分離卷積和精簡的架構，MobileNet 模型通常比傳統的 CNN (例如 VGG, ResNet) 小得多，更容易部署在記憶體受限的設備上。
- **推理速度快:** 深度可分離卷積的計算成本遠低於標準卷積，加上更小的模型尺寸，使得 MobileNet 模型在推理時速度更快，延遲更低，更適合實時應用。
- **良好的精度與效率平衡:** MobileNet 系列模型在保持相對較高精度的同時，實現了顯著的效率提升，為移動設備上的視覺任務提供了一個很好的折衷方案。
- **廣泛的應用:** MobileNet 模型被廣泛應用於圖像分類、目標檢測、語義分割等各種移動視覺任務中。
- **易於部署:** 許多深度學習框架 (例如 TensorFlow Lite, PyTorch Mobile) 都對 MobileNet 模型有良好的支持，方便在移動和嵌入式平台上進行部署和優化。

**總結 MobileNet 系列的主要特點:**

- **核心組件：深度可分離卷積** - 顯著降低計算成本和模型參數。
- **輕量級網路架構** - 包含較少的層和更小的卷積核。
- **寬度和分辨率乘數** - 提供靈活的精度和效率trade-off。
- **MobileNet V2 的改進：反向殘差塊和線性瓶頸** - 提高效率和精度。
- **MobileNet V3 的改進：NAS搜索、硬激勵函數和 SE 模塊** - 進一步優化性能。
- **模型尺寸小、推理速度快、精度與效率平衡良好** - 非常適合資源受限的移動和嵌入式設備。


### **2. EfficientNet**

- **核心特點：**
    - **複合縮放（Compound Scaling）：** EfficientNet 提出了一種統一的縮放方法，同時調整模型的深度（depth）、寬度（width）和解析度（resolution），以達到最佳的效能。
    - 使用 AutoML 自動搜尋最佳模型結構。
    - 使用 MobileNetV2 所使用的反向殘差結構(Inverted residuals)與壓縮激活(Squeeze-and-Excitation Networks)。
- **與傳統 CNN 的差異：**
    - 傳統 CNN 通常只單獨調整模型的某個維度（例如深度）。
    - EfficientNet 的複合縮放方法能夠更有效地平衡模型的各個維度，提高效能。
- **應用場景：**
    - 需要高精度和高效率的圖像分類任務。

好的，針對您關於 EfficientNet 調整模型深度、寬度、解析度的問題，詳細解釋如下：

**1. EfficientNet 如何調整深度 (Depth)、寬度 (Width) 和解析度 (Resolution)？是 AutoML 嗎？**

EfficientNet 的調整方法是一個巧妙的兩階段過程，**不完全是**直接用 AutoML 同時調整您提到的所有參數來得到每個版本的模型 (EfficientNet-B1, B2, ...)。

- **階段一：使用神經架構搜索 (NAS) 找到優秀的基線模型 (Baseline Model - EfficientNet-B0)**
    
    - EfficientNet 的作者首先使用了一種神經架構搜索（NAS，這是一種 AutoML 技術）來找到一個在準確率和計算效率（以 FLOPS 衡量）之間取得良好平衡的**基礎網路架構**，命名為 EfficientNet-B0。
    - 這個 NAS 過程會探索不同的架構選擇，例如卷積塊的類型（主要是 MBConv 結構）、擴展比例、**卷積核大小 (kernel size)**、激活函數、連接方式等等，以自動發現一個高效的基礎骨幹網路。
- **階段二：提出並應用複合縮放方法 (Compound Scaling Method) 放大基線模型**
    
    - 在得到 EfficientNet-B0 這個優秀的基線後，作者提出了一個核心觀點：單獨放大網路的深度、寬度或解析度中任何一個維度都會遇到瓶頸，**同時、均衡地**放大這三個維度效果最好。
    - 他們定義了三個縮放係數：
        - `d = α^φ` (深度縮放因子)
        - `w = β^φ` (寬度縮放因子)
        - `r = γ^φ` (解析度縮放因子)
    - 這裡的 `φ` 是一個用戶指定的**複合係數 (compound coefficient)**，用來控制總體的放大程度。`α, β, γ` 是常數，代表了如何將資源分配給深度、寬度和解析度這三個維度。
    - 作者通過在**基線模型 EfficientNet-B0** 上進行小範圍的**網格搜索 (Grid Search)**（_而不是_ 再次使用 NAS/AutoML）來確定 `α, β, γ` 的最佳值（大約是 α ≈ 1.2, β ≈ 1.1, γ ≈ 1.15），目標是在稍微增加計算量（約 2 倍 FLOPS）的情況下最大化 B0 的準確率。
    - 對於後續的 EfficientNet-B1 到 B7 模型，他們**並不是**重新運行 NAS 或 AutoML。而是**固定**了通過網格搜索得到的 `α, β, γ` 值，然後**逐步增加**複合係數 `φ` (φ=1, 2, ..., 7)，並根據上述公式計算出 `d, w, r` 的值。
    - 接著，他們將這些縮放因子應用到 EfficientNet-B0 架構上：
        - **深度 (Depth) 縮放：** 增加網路中的**層數 (layer number)**，通常是增加主要模塊（MBConv blocks）的重複次數。
        - **寬度 (Width) 縮放：** 增加每一層的**通道數量 (channel number)**，通常是將 B0 中各層的通道數乘以 `w`。
        - **解析度 (Resolution) 縮放：** 增加輸入圖像的**高度和寬度**，通常是將 B0 的輸入解析度乘以 `r`。
    - **卷積核大小 (Kernel Size)** 通常是在階段一的 NAS 過程中確定的（作為 B0 架構的一部分），在階段二的複合縮放過程中**一般不改變**。縮放主要影響的是層的數量和通道數。

**總結 Q1：** EfficientNet 的方法是 **NAS (AutoML) 用於尋找基線架構 (B0) + 公式化的複合縮放 (Compound Scaling) 用於從 B0 放大到 B1-B7**。複合縮放本身不是 AutoML，而是一種基於固定比例、系統性地同時增加模型深度（層數）、寬度（通道數）和輸入解析度的方法。它不直接在縮放過程中調整卷積核大小。

**2. 最佳的效能 (Optimal Performance) 是指什麼？用什麼 Loss Functions？**

- **最佳效能的含義：**
    
    - 在 EfficientNet 的論文和實踐中，「最佳效能」主要指的是在給定的**計算資源預算 (Computational Budget)**（通常用 FLOPS 或模型參數數量來衡量）下，實現**最高的模型準確率 (Accuracy)**（例如，在 ImageNet 數據集上的 Top-1 準確率）。
    - 複合縮放方法的目標就是找到一種比單獨縮放任一維度更**高效**的策略，即用更少的計算量（FLOPS）或更少的參數來達到相同的準確率，或者在相同的計算量下達到更高的準確率。
- **Loss Functions (損失函數)：**
    
    - 在**訓練**任何特定 EfficientNet 模型（無論是 B0 還是 B1-B7）進行圖像分類任務時（如在 ImageNet 上訓練），所使用的**主要損失函數**是**交叉熵損失 (Cross-Entropy Loss)**。這是多類別分類問題的標準損失函數。
    - 在階段一使用 NAS 尋找 EfficientNet-B0 架構時，其優化的**獎勵函數 (Reward Function)** 可能是一個**複合目標**，不僅僅是準確率，可能還會直接或間接地考慮模型的 FLOPS 或延遲，以鼓勵找到既準確又高效的架構。但這是在架構搜索階段使用的內部評估指標。
    - 對於最終模型的公開訓練和評估，標準的交叉熵損失是用於優化模型權重的。

**總結 Q2：** EfficientNet 追求的「最佳效能」主要是指**在計算成本（FLOPS）約束下達到最高的分類準確率**。其模型在進行圖像分類任務訓練時，使用的標準損失函數是**交叉熵損失 (Cross-Entropy Loss)**。







**3. 常用的輕量物件偵測模型：**

### - **YOLOv5-Lite/YOLOv8-Nano:**
    - YOLO 系列的輕量版本，針對行動裝置和嵌入式系統進行了優化。
    - 使用輕量化的骨幹網路和高效的偵測頭。
    - 輕量化模型在精度上可能稍遜於大型模型，但在速度上具有明顯優勢。
- **SSD-MobileNet:**
    - 結合 SSD（Single Shot MultiBox Detector）物件偵測框架和 MobileNet 骨幹網路。
    - 在速度和準確度之間取得了良好的平衡。

**輕量化 CNN 的共同特點：**

- **更少的參數和計算量：** 適用於資源有限的裝置。
- **更快的推理速度：** 實現即時應用。
- **模型壓縮技術：**
    - 例如模型剪枝(model pruning)與量化(quantization)，常常被應用在輕量化模型上面，更加減少模型的參數與計算。

**與一般 CNN 的差異總結：**

- 一般 CNN 追求更高的準確度，通常具有更深的網路結構和更多的參數。
- 輕量化 CNN 追求更高的效率，通過創新的網路結構和優化方法，在保持一定準確度的前提下，大幅降低計算成本。


### SSD-MobileNet 

理解 SSD-MobileNet 的結構，需要將兩個關鍵組件分開來看，然後理解它們是如何結合的：SSD（Single Shot MultiBox Detector）和 MobileNet。

**1. SSD（Single Shot MultiBox Detector）**

- **核心特點：**
    - SSD 是一種單階段（single-shot）目標偵測器，這意味著它直接從輸入圖像預測目標的邊界框和類別，而不需要像兩階段偵測器（例如 Faster R-CNN）那樣先生成候選區域。
    - **多尺度特徵圖（Multi-scale Feature Maps）：** SSD 的一個關鍵特點是使用來自網路不同層次的多個特徵圖進行預測。這使其能夠檢測不同尺寸的目標。較淺的層次可以檢測較小的目標，而較深的層次可以檢測較大的目標。這某種程度上達到類似Feature Pyramid Network的效果，但是作法上稍有不同。
    - **預設框（Default Boxes）：** SSD 在每個特徵圖單元位置放置一組預設框，這些預設框具有不同的尺寸和長寬比。模型學習調整這些預設框，以更好地匹配真實目標。
- **與 Feature Pyramid Network (FPN) 的關係：**
    - 雖然 SSD 通過使用多個特徵圖實現了多尺度檢測，但它與 FPN 的方法有所不同。FPN 通過自上而下的方式構建特徵金字塔，增強了低層次特徵的語義資訊。
    - SSD 直接利用網絡前向運算的各層Feature map, FPN則是將高層的語意特徵向底層傳遞。
    - 因此，可以說 SSD 具有類似 FPN 的多尺度檢測能力，但實現方式不同。

**2. MobileNet**

- **核心特點：**
    - MobileNet 是一種輕量級的 CNN 架構，專為行動和嵌入式裝置設計。
    - 它使用深度可分離卷積，顯著減少了計算成本和參數數量。

**3. SSD-MobileNet 的結合**

- **結構：**
    - SSD-MobileNet 將 MobileNet 作為其骨幹網路（backbone network）。這意味著 MobileNet 負責從輸入圖像中提取特徵。
    - 然後，SSD 的多尺度預測層被添加到 MobileNet 的不同層次的特徵圖上。
    - 具體來說，在 MobileNet 的幾個關鍵層次之後，添加了額外的卷積層，這些層輸出的特徵圖用於預測目標的邊界框和類別。
    - 因此MobileNet負責輕量化的特徵提取，SSD負責多尺度的目標檢測。
- **優勢：**
    - 這種結合使得 SSD-MobileNet 能夠在資源有限的裝置上實現實時目標檢測。
    - MobileNet 提供了輕量級的特徵提取，而 SSD 提供了高效的多尺度目標檢測能力。

**總結：**

- SSD-MobileNet 是一種高效的目標檢測模型，通過結合 MobileNet 的輕量級特徵提取和 SSD 的多尺度預測能力，實現了在資源有限的裝置上的實時目標檢測。
- SSD 本身透過利用深淺不同的feature map 來達到多尺度檢測的效果，因此，跟FPN的作用類似，但是實作方式不同。