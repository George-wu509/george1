

在深度學習與機器學習中，L1 Loss、L2 Loss、L1 Regularization 和 L2 Regularization 是兩組常見的損失函數與正則化技術，它們在模型訓練與優化過程中有不同的應用場景與特性。

- **L1 Loss 更適合異常值多的回歸問題**
- **L2 Loss 更適合一般回歸問題，能夠讓大誤差受到懲罰**
- **L1 Regularization 適合高維數據，<mark style="background: #FF5582A6;">能讓部分權重變 0</mark>，做特徵選擇**
- **L2 Regularization 適合防止過擬合，讓所有權重變小但不為 0**

---

## **L1 Loss (絕對誤差, Mean Absolute Error, MAE)**

L1 Loss 計算的是預測值與真實值之間的 **絕對誤差**，公式如下：

$\Huge L1\_Loss = \frac{1}{N} \sum_{i=1}^{N} |y_i - \hat{y}_i|$

其中：

- yi​ 是真實值 (ground truth)
- y^​i​ 是模型的預測值
- N 是樣本數量

### **特性**

1. **對異常值（outliers）較為穩健**：
    - 由於 L1 Loss 是計算絕對值，因此誤差不會因為平方運算而放大，所以對於含有異常值的數據集較為穩健。
2. **梯度恆定，不會因為誤差變大而變大**：
    - L1 Loss 的導數是固定的 +1 或 −1，這意味著當誤差較大時，梯度更新的速度不會像 L2 Loss 那樣受誤差的影響而變大。

### **適用場景**

- **對異常值敏感的回歸問題**（如房價預測、金融數據分析）就是如果dataset有很多outliers->用L1
- **需要模型更具魯棒性時**

---

## **L2 Loss (均方誤差, Mean Squared Error, MSE)**

L2 Loss 計算的是預測值與真實值之間的 **平方誤差**，公式如下：

$\Huge L2\_Loss = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2$

### **特性**

1. **對異常值較敏感**：
    
    - 由於誤差的平方會放大較大的誤差，因此如果數據中有極端異常值，L2 Loss 會受到較大影響。
2. **導數與誤差成正比**：
    
    - L2 Loss 的梯度計算為： $\Huge \frac{\partial L2\_Loss}{\partial \hat{y}_i} = 2 (y_i - \hat{y}_i)∂y^​i$​
    - 當誤差大時，梯度更新的幅度也會變大，有助於快速收斂，但也可能導致不穩定。

### **適用場景**

- **異常值影響較小的數據集**  就是如果dataset很少outliers->用L2
- **希望對較大誤差進行懲罰的回歸問題**
- **大多數神經網絡的回歸問題，如影像去噪、超解析度**

---

## **L1 Regularization（Lasso 正則化）**

L1 正則化（L1 Regularization）是透過對模型的參數 w 添加 L1 項來限制參數的大小，公式如下：

$\Huge L_{reg} = \lambda \sum_{j=1}^{m} |w_j|$

其中：

- λ是正則化超參數，控制 L1 項的強度
- wj​ 是模型的參數
- m 是參數數量

### **特性**

1. **會使部分權重變為零（模型變稀疏）**：
    - L1 正則化的梯度為固定值 +1 或 −1，這導致有些權重在訓練過程中被強制為 0，從而進行特徵選擇（Feature Selection）。
2. **適用於高維特徵數據集**：
    - 當數據集有很多特徵時，L1 正則化可以幫助選擇最重要的特徵並忽略不相關的特徵。

### **適用場景**

- **高維數據（如文本分類、基因數據）**
- **希望進行特徵選擇，讓模型變得更簡單**
- **線性回歸中的 LASSO 回歸**

---

## **L2 Regularization（Ridge 正則化）**

L2 正則化（L2 Regularization）是透過對模型的參數 w 添加 L2 項來限制參數的大小，公式如下：

$\Huge L_{reg} = \lambda \sum_{j=1}^{m} w_j^2$​

### **特性**

1. **不會使權重變為零，而是讓權重更小**：
    
    - L2 正則化的梯度計算為 2λwj，這意味著即使參數值很小，也仍然會受到約束。
2. **防止過擬合（overfitting）**：
    
    - L2 正則化可以防止模型過度依賴某些特徵，減少過擬合的可能性。

### **適用場景**

- **防止過擬合（如影像分類、語音識別）**
- **需要所有特徵的影響都存在，而不是完全忽略某些特徵（如 Ridge Regression）**
- **深度學習模型（如 CNN、RNN）常使用 L2 正則化**

---

## **L1 vs. L2 Loss 與 L1 vs. L2 Regularization 比較**

||L1 Loss|L2 Loss|L1 Regularization|L2 Regularization|
|---|---|---|---|---|
|**計算公式**|( \sum|y_i - \hat{y}_i|)|∑(yi−y^i)2\sum (y_i - \hat{y}_i)^2∑(yi​−y^​i​)2|
|**對異常值的影響**|穩健（影響小）|敏感（影響大）|-|-|
|**梯度性質**|梯度恆定（+1 / -1）|梯度與誤差成正比|梯度恆定（+1 / -1）|梯度與權重成正比|
|**對模型影響**|適合異常值多的情況|適合誤差需平方懲罰的情況|讓部分權重變 0，適合特徵選擇|讓權重變小，但不變 0，適合防止過擬合|
|**應用場景**|異常值多的回歸|平滑誤差的回歸|特徵選擇、高維數據|深度學習模型防止過擬合|

---

## **何時使用 L1 Loss, L2 Loss, L1 Regularization, L2 Regularization？**

1. **L1 Loss**
    
    - 適合處理 **含有異常值的回歸問題**
    - 適合 **模型需要對異常值具有魯棒性** 的場景
    - 如：金融數據、房價預測
2. **L2 Loss**
    
    - 適合處理 **誤差分布較平滑的回歸問題**
    - 適合 **希望懲罰大誤差** 的場景
    - 如：影像去噪、深度學習回歸問題
3. **L1 Regularization**
    
    - 適合 **特徵數量多且需要選擇最重要特徵** 的情況
    - 適合 **希望部分特徵權重為零（LASSO）**
    - 如：文本分類、基因數據分析
4. **L2 Regularization**
    
    - 適合 **防止過擬合，讓模型更加平滑** 的場景
    - 適合 **深度學習（如 CNN, RNN）**
    - 如：影像分類、語音識別

---

## **結論**

- **L1 Loss 更適合異常值多的回歸問題**
- **L2 Loss 更適合一般回歸問題，能夠讓大誤差受到懲罰**
- **L1 Regularization 適合高維數據，能讓部分權重變 0，做特徵選擇**
- **L2 Regularization 適合防止過擬合，讓所有權重變小但不為 0**

在實際應用中，L1 和 L2 正則化經常結合（Elastic Net），以達到更好的效果！