
|                                 |                 |
| ------------------------------- | --------------- |
| 01. Pytorch实现注意力机制、多头注意力        | [[##### Ans01]] |
| 02. Numpy广播机制实现矩阵间L2距离的计算       | [[##### Ans02]] |
| 03. Conv2D卷积的Python和C++实现       | [[##### Ans03]] |
| 04. Numpy实现bbox_iou的计算          | [[##### Ans04]] |
| 05. Numpy实现Focal loss           | [[##### Ans05]] |
| 06. Python实现非极大值抑制nms、softnms   | [[##### Ans06]] |
| 07. Python实现BN批量归一化             | [[##### Ans07]] |
| 08. Pytorch手写Conv+Bn+Relu，及如何合并 | [[##### Ans08]] |
| 09. 描述图像resize的过程并实现            | [[##### Ans09]] |
| 10. PyTorch卷积与BatchNorm的融合      | [[##### Ans10]] |
| 11. 分割网络损失函数Dice Loss代码实现       | [[##### Ans11]] |
多模态大模型与深度学习高阶面试题：新颖、高频且有深度，数百道题覆盖六大专题 - Mark·AI的文章 - (OK)
https://zhuanlan.zhihu.com/p/676239271


##### Ans01
Q: 01. Pytorch实现注意力机制、多头注意力

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# --- 注意力机制 (Scaled Dot-Product Attention) ---
class SelfAttention(nn.Module):
    def __init__(self, embed_dim, dropout=0.0):
        super().__init__()
        self.embed_dim = embed_dim
        self.scale = embed_dim**-0.5 # 1/sqrt(d_k)
        self.dropout = nn.Dropout(dropout)

    def forward(self, query, key, value, mask=None):
        """
        Args:
            query (torch.Tensor): Query tensor, shape (..., L, E).
            key (torch.Tensor): Key tensor, shape (..., S, E).
            value (torch.Tensor): Value tensor, shape (..., S, E).
            mask (torch.Tensor, optional): Attention mask, shape (..., L, S).
                                           True indicates a position to be masked (set to -inf).
        Returns:
            torch.Tensor: Output tensor, shape (..., L, E).
            torch.Tensor: Attention weights, shape (..., L, S).
        """
        # (..., L, E) @ (..., E, S) -> (..., L, S)
        scores = torch.matmul(query, key.transpose(-2, -1)) * self.scale

        if mask is not None:
            scores = scores.masked_fill(mask, -torch.inf)

        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # (..., L, S) @ (..., S, E) -> (..., L, E)
        output = torch.matmul(attn_weights, value)
        return output, attn_weights

# --- 多头注意力 (Multi-Head Attention) ---
class MultiHeadSelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads, dropout=0.0):
        super().__init__()
        assert embed_dim % num_heads == 0, "embed_dim must be divisible by num_heads"
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False)
        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False)
        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False)
        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)

        self.attention = SelfAttention(self.head_dim, dropout=dropout)

    def _split_heads(self, x, batch_size):
        """ (B, L, E) -> (B, L, N_H, D_H) -> (B, N_H, L, D_H) """
        return x.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)

    def _combine_heads(self, x, batch_size):
        """ (B, N_H, L, D_H) -> (B, L, N_H, D_H) -> (B, L, E) """
        return x.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)

    def forward(self, query, key, value, mask=None):
        """
        Args:
            query (torch.Tensor): Query tensor, shape (B, L, E).
            key (torch.Tensor): Key tensor, shape (B, S, E).
            value (torch.Tensor): Value tensor, shape (B, S, E).
            mask (torch.Tensor, optional): Attention mask, shape (B, L, S).
                                           True indicates a position to be masked (set to -inf).
        Returns:
            torch.Tensor: Output tensor, shape (B, L, E).
            torch.Tensor: Attention weights from all heads, shape (B, N_H, L, S).
        """
        batch_size = query.size(0)

        # Project and split heads
        q = self._split_heads(self.q_proj(query), batch_size) # (B, N_H, L, D_H)
        k = self._split_heads(self.k_proj(key), batch_size)   # (B, N_H, S, D_H)
        v = self._split_heads(self.v_proj(value), batch_size) # (B, N_H, S, D_H)

        # Apply attention for each head
        # mask needs to be broadcastable to (B, N_H, L, S)
        if mask is not None:
            # Add head dimension for broadcasting if mask is (B, L, S)
            if mask.dim() == 3:
                mask = mask.unsqueeze(1) # (B, 1, L, S)
        
        attn_outputs, attn_weights = self.attention(q, k, v, mask=mask) # (B, N_H, L, D_H), (B, N_H, L, S)

        # Combine heads and project back to original embed_dim
        output = self._combine_heads(attn_outputs, batch_size) # (B, L, E)
        output = self.out_proj(output) # (B, L, E)

        return output, attn_weights
```



##### Ans02
Q: 02. Numpy广播机制实现矩阵间L2距离的计算

```python
import numpy as np

def pairwise_l2_distance(matrix_a, matrix_b):
    """
    使用 NumPy 广播机制计算两个矩阵之间所有行向量对的 L2 距离 (欧几里得距离)。

    Args:
        matrix_a (np.ndarray): 第一个矩阵，形状为 (M, D)。
        matrix_b (np.ndarray): 第二个矩阵，形状为 (N, D)。
                                M 和 N 是向量数量，D 是向量维度。

    Returns:
        np.ndarray: 形状为 (M, N) 的矩阵，其中 element[i, j] 是 matrix_a 的第 i 行
                    和 matrix_b 的第 j 行之间的 L2 距离。
    """
    # 扩展维度，使得 matrix_a 变为 (M, 1, D)，matrix_b 变为 (1, N, D)
    # 这样在减法时，NumPy 会自动广播，得到 (M, N, D) 的差值矩阵
    diff = matrix_a[:, np.newaxis, :] - matrix_b[np.newaxis, :, :]

    # 计算差值平方和
    sq_diff = np.sum(diff**2, axis=2)

    # 开方得到 L2 距离
    l2_dist = np.sqrt(sq_diff)

    return l2_dist
```


##### Ans03
Q: Conv2D卷积的Python和C++实现

```python
import numpy as np

def conv2d_python(image, kernel, padding=0, stride=1):
    """
    纯 Python (NumPy) 实现的 2D 卷积。

    Args:
        image (np.ndarray): 输入图像，形状 (H, W) 或 (H, W, C)。
        kernel (np.ndarray): 卷积核，形状 (kH, kW) 或 (kH, kW, C_in, C_out)。
        padding (int or tuple): 填充大小 (上, 下, 左, 右)。如果是 int，则所有方向填充相同。
        stride (int or tuple): 步长 (垂直, 水平)。如果是 int，则垂直和水平步长相同。

    Returns:
        np.ndarray: 卷积后的特征图。
    """
    if image.ndim == 2:
        image = image[:, :, np.newaxis] # 转换为 (H, W, 1)

    if kernel.ndim == 2:
        kernel = kernel[:, :, np.newaxis, np.newaxis] # 转换为 (kH, kW, 1, 1)
    elif kernel.ndim == 3: # (kH, kW, C_in) for C_out = 1
        kernel = kernel[:, :, :, np.newaxis] # 转换为 (kH, kW, C_in, 1)

    H_in, W_in, C_in = image.shape
    kH, kW, C_in_k, C_out = kernel.shape

    assert C_in == C_in_k, "Input image channels must match kernel input channels."

    if isinstance(padding, int):
        pad_h, pad_w = padding, padding
    else:
        pad_h, pad_w = padding[0], padding[1] if len(padding) > 1 else padding[0]

    if isinstance(stride, int):
        stride_h, stride_w = stride, stride
    else:
        stride_h, stride_w = stride[0], stride[1]

    # 填充图像
    padded_image = np.pad(image, ((pad_h, pad_h), (pad_w, pad_w), (0, 0)), mode='constant')

    H_out = (H_in + 2 * pad_h - kH) // stride_h + 1
    W_out = (W_in + 2 * pad_w - kW) // stride_w + 1

    output = np.zeros((H_out, W_out, C_out))

    for c_out in range(C_out):
        for h_out in range(H_out):
            for w_out in range(W_out):
                h_start = h_out * stride_h
                h_end = h_start + kH
                w_start = w_out * stride_w
                w_end = w_start + kW

                # 提取图像区域
                image_region = padded_image[h_start:h_end, w_start:w_end, :]
                
                # 执行卷积 (点积)
                output[h_out, w_out, c_out] = np.sum(image_region * kernel[:, :, :, c_out])
    return output
```

```c++
#include <vector>
#include <iostream>
#include <numeric> // For std::accumulate

// 简单的 2D 卷积函数 (无通道处理，单通道输入，单通道输出)
// 为了简洁，这里只处理单通道灰度图像，不包括多通道和批处理。
// 实际生产环境中会使用Eigen, OpenCV, 或者自己实现更优化的矩阵库。

std::vector<std::vector<double>> conv2d_cpp_single_channel(
    const std::vector<std::vector<double>>& image,
    const std::vector<std::vector<double>>& kernel,
    int padding = 0,
    int stride = 1) {

    int H_in = image.size();
    int W_in = (H_in > 0) ? image[0].size() : 0;
    int kH = kernel.size();
    int kW = (kH > 0) ? kernel[0].size() : 0;

    if (H_in == 0 || W_in == 0 || kH == 0 || kW == 0) {
        return {}; // 返回空结果
    }

    // 计算输出尺寸
    int H_out = (H_in + 2 * padding - kH) / stride + 1;
    int W_out = (W_in + 2 * padding - kW) / stride + 1;

    // 初始化输出特征图
    std::vector<std::vector<double>> output(H_out, std::vector<double>(W_out, 0.0));

    // 创建填充图像 (为了简洁，这里直接在循环中处理边界，不创建单独的填充矩阵)
    // 更高效的实现会创建一个新的填充矩阵

    for (int h_out = 0; h_out < H_out; ++h_out) {
        for (int w_out = 0; w_out < W_out; ++w_out) {
            double sum = 0.0;
            for (int kh = 0; kh < kH; ++kh) {
                for (int kw = 0; kw < kW; ++kw) {
                    int h_in = h_out * stride + kh - padding;
                    int w_in = w_out * stride + kw - padding;

                    // 检查边界
                    if (h_in >= 0 && h_in < H_in && w_in >= 0 && w_in < W_in) {
                        sum += image[h_in][w_in] * kernel[kh][kw];
                    }
                }
            }
            output[h_out][w_out] = sum;
        }
    }
    return output;
}

// 示例：更通用的 Conv2D 结构 (示意，未完整实现多通道、批处理等)
// 实际中会用更高级的库或自定义矩阵类
/*
class Conv2D {
public:
    Conv2D(int in_channels, int out_channels, int kernel_h, int kernel_w, int stride = 1, int padding = 0)
        : in_channels_(in_channels), out_channels_(out_channels),
          kernel_h_(kernel_h), kernel_w_(kernel_w),
          stride_(stride), padding_(padding) {
        // 初始化权重 (kernel) 和偏置
        // weights_ = std::vector<std::vector<std::vector<std::vector<double>>>>(out_channels, std::vector<std::vector<std::vector<double>>>(in_channels, ...));
        // biases_ = std::vector<double>(out_channels);
    }

    // 假设输入是 NCHW 格式，输出也是 NCHW
    std::vector<std::vector<std::vector<std::vector<double>>>> forward(
        const std::vector<std::vector<std::vector<std::vector<double>>>>& input_tensor) {
        // ... 卷积逻辑，遍历批次、输入通道、输出通道、图像空间维度 ...
        // 这将涉及大量的嵌套循环和矩阵乘法
        // 通常会依赖于BLAS/cuBLAS/Eigen等高性能库
        // 例如：
        // for (int n = 0; n < batch_size; ++n) {
        //     for (int co = 0; co < out_channels_; ++co) {
        //         for (int ci = 0; ci < in_channels_; ++ci) {
        //             // 对每个输入通道和输出通道应用单通道卷积
        //             // 然后将结果累加到最终输出
        //         }
        //     }
        // }
        // ...
        return {}; // 占位符
    }

private:
    int in_channels_;
    int out_channels_;
    int kernel_h_;
    int kernel_w_;
    int stride_;
    int padding_;
    // std::vector<std::vector<std::vector<std::vector<double>>>> weights_;
    // std::vector<double> biases_;
};
*/
```

##### Ans04
Q: Numpy实现bbox_iou的计算

```python
import numpy as np

def bbox_iou(box1, box2):
    """
    计算两个边界框的IoU (Intersection over Union)。
    
    Args:
        box1 (np.ndarray): 边界框坐标，形状为 [x1, y1, x2, y2]。
        box2 (np.ndarray): 边界框坐标，形状为 [x1, y1, x2, y2]。
        
    Returns:
        float: IoU值。
    """
    # 获取交集区域的坐标
    x1 = np.maximum(box1[0], box2[0])
    y1 = np.maximum(box1[1], box2[1])
    x2 = np.minimum(box1[2], box2[2])
    y2 = np.minimum(box1[3], box2[3])

    # 计算交集区域的面积
    inter_width = np.maximum(0, x2 - x1)
    inter_height = np.maximum(0, y2 - y1)
    inter_area = inter_width * inter_height

    # 计算两个边界框的面积
    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])

    # 计算并集区域的面积
    union_area = box1_area + box2_area - inter_area

    # 计算IoU，避免除以零
    iou = inter_area / (union_area + 1e-6) # 添加一个小的epsilon防止除零
    
    return iou
```

##### Ans05
Q: Numpy实现Focal loss

```python
import numpy as np

def focal_loss(y_true, y_pred, alpha=0.25, gamma=2.0, epsilon=1e-7):
    """
    Focal Loss 的 NumPy 实现。

    Args:
        y_true (np.ndarray): 真实标签，形状为 (N, C) 的 one-hot 编码。
        y_pred (np.ndarray): 预测概率，形状为 (N, C)。通常是经过 sigmoid 或 softmax 激活后的输出。
        alpha (float): 平衡参数，用于平衡正负样本的权重。默认 0.25。
        gamma (float): 聚焦参数，用于调整难易样本的权重。默认 2.0。
        epsilon (float): 避免对数运算中出现 log(0) 的小常数。

    Returns:
        float: 计算得到的 Focal Loss。
    """
    # 确保预测概率在 (epsilon, 1 - epsilon) 范围内，以避免 log(0)
    y_pred = np.clip(y_pred, epsilon, 1. - epsilon)

    # 计算交叉熵
    bce = -y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)

    # 计算预测概率 Pt
    # Pt 是模型正确分类的概率
    # 如果 y_true 为 1 (正样本)，则 Pt = y_pred
    # 如果 y_true 为 0 (负样本)，则 Pt = 1 - y_pred
    Pt = y_true * y_pred + (1 - y_true) * (1 - y_pred)

    # 计算 modulating factor (1 - Pt)^gamma
    modulating_factor = (1 - Pt)**gamma

    # 计算 alpha_t
    # 如果 y_true 为 1 (正样本)，则 alpha_t = alpha
    # 如果 y_true 为 0 (负样本)，则 alpha_t = 1 - alpha
    alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)

    # 计算 Focal Loss
    loss = alpha_factor * modulating_factor * bce

    # 对所有样本和所有类别求平均
    return np.mean(np.sum(loss, axis=-1))
```

##### Ans06
Q: Python实现非极大值抑制nms、softnms

```python
import torch

def bbox_iou(box1, box2):
    """
    计算两个边界框的IoU (Intersection over Union)。
    Args:
        box1 (torch.Tensor): [x1, y1, x2, y2]
        box2 (torch.Tensor): [x1, y1, x2, y2]
    Returns:
        torch.Tensor: IoU值
    """
    # 获取交集区域的坐标
    x1 = torch.max(box1[0], box2[0])
    y1 = torch.max(box1[1], box2[1])
    x2 = torch.min(box1[2], box2[2])
    y2 = torch.min(box1[3], box2[3])

    # 计算交集区域的面积
    inter_width = torch.clamp(x2 - x1, min=0)
    inter_height = torch.clamp(y2 - y1, min=0)
    inter_area = inter_width * inter_height

    # 计算两个边界框的面积
    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])

    # 计算并集区域的面积
    union_area = box1_area + box2_area - inter_area

    # 计算IoU
    iou = inter_area / (union_area + 1e-6) # 加一个小的eps防止除零
    return iou

def nms(boxes, scores, iou_threshold):
    """
    非极大值抑制 (NMS)。
    Args:
        boxes (torch.Tensor): 边界框坐标，形状为 (N, 4)，每行 [x1, y1, x2, y2]。
        scores (torch.Tensor): 对应边界框的置信度分数，形状为 (N,)。
        iou_threshold (float): IoU阈值。
    Returns:
        torch.Tensor: 选定边界框的索引。
    """
    # 按分数降序排序
    _, order = scores.sort(0, descending=True)

    keep = []
    while order.numel() > 0:
        if order.numel() == 1:
            i = order.item()
            keep.append(i)
            break
        
        i = order[0].item()
        keep.append(i)

        # 计算当前最高分数框与其他所有框的IoU
        ious = torch.tensor([bbox_iou(boxes[i], boxes[j]) for j in order[1:]])
        
        # 移除IoU超过阈值的框
        inds = torch.where(ious <= iou_threshold)[0]
        order = order[inds + 1] # +1 因为ious是基于order[1:]计算的

    return torch.tensor(keep, dtype=torch.long)


def soft_nms(boxes, scores, iou_threshold, sigma=0.5, score_threshold=0.0):
    """
    Soft-NMS。
    Args:
        boxes (torch.Tensor): 边界框坐标，形状为 (N, 4)，每行 [x1, y1, x2, y2]。
        scores (torch.Tensor): 对应边界框的置信度分数，形状为 (N,)。
        iou_threshold (float): IoU阈值，用于判断是否衰减分数。
        sigma (float): 高斯衰减函数的参数。
        score_threshold (float): 最终保留框的最低分数阈值。
    Returns:
        torch.Tensor: 最终保留框的索引。
    """
    num_boxes = boxes.shape[0]
    # 创建一个可修改的 scores 副本
    scores_copy = scores.clone()
    
    keep = []
    
    for i in range(num_boxes):
        if scores_copy.numel() == 0:
            break

        # 找到当前分数最高的框
        max_score_idx = torch.argmax(scores_copy)
        max_score = scores_copy[max_score_idx]

        # 如果最高分数低于最终保留阈值，则停止
        if max_score < score_threshold:
            break
        
        keep.append(max_score_idx.item())

        # 获取当前最高分数的框
        current_box = boxes[max_score_idx]

        # 将当前最高分数的框从处理列表中移除 (通过设置分数负无穷)
        scores_copy[max_score_idx] = -torch.inf 

        # 计算当前框与其他所有框的IoU
        # 注意: 这里计算的是原始 boxes 和 scores_copy 对应的 IoU，
        # 因为我们只是在 scores_copy 上做操作，原始 boxes 的索引不变。
        ious = torch.tensor([bbox_iou(current_box, boxes[j]) for j in range(num_boxes)])

        # 对其他框的分数进行衰减
        for j in range(num_boxes):
            if j == max_score_idx: # 跳过自身
                continue

            # 获取当前框的IoU
            ov = ious[j]

            # 衰减函数
            if ov >= iou_threshold:
                # 线性衰减
                scores_copy[j] *= (1 - ov)
            # else:  # 如果是高斯衰减，则无论 IoU 多少都会衰减
            #     scores_copy[j] *= torch.exp(-(ov * ov) / sigma)

    # 过滤掉分数低于 score_threshold 的框
    final_keep_indices = []
    for idx in keep:
        if scores[idx] >= score_threshold: # 使用原始分数进行最终过滤
             final_keep_indices.append(idx)

    return torch.tensor(final_keep_indices, dtype=torch.long)
```

##### Ans07
Q: Python实现BN批量归一化

```python
import torch
import torch.nn as nn

class BatchNorm(nn.Module):
    def __init__(self, num_features, eps=1e-5, momentum=0.1):
        super().__init__()
        self.num_features = num_features
        self.eps = eps
        self.momentum = momentum
        self.weight = nn.Parameter(torch.ones(num_features))
        self.bias = nn.Parameter(torch.zeros(num_features))
        self.register_buffer('running_mean', torch.zeros(num_features))
        self.register_buffer('running_var', torch.ones(num_features))

    def forward(self, x):
        if x.dim() == 4:  # NCHW
            mean_dims = (0, 2, 3)
        elif x.dim() == 2:  # NC
            mean_dims = 0
        else:
            raise NotImplementedError("Only 2D (N, C) or 4D (N, C, H, W) tensors are supported.")

        if self.training:
            mean = x.mean(mean_dims, keepdim=True)
            var = x.var(mean_dims, keepdim=True, unbiased=False) # Use unbiased=False for population variance
            
            # Update running statistics
            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean.squeeze()
            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var.squeeze()
        else:
            mean = self.running_mean.reshape(1, self.num_features, 1, 1) if x.dim() == 4 else self.running_mean.reshape(1, self.num_features)
            var = self.running_var.reshape(1, self.num_features, 1, 1) if x.dim() == 4 else self.running_var.reshape(1, self.num_features)

        x_norm = (x - mean) / torch.sqrt(var + self.eps)
        
        # Reshape weight and bias for broadcasting
        if x.dim() == 4:
            weight = self.weight.reshape(1, self.num_features, 1, 1)
            bias = self.bias.reshape(1, self.num_features, 1, 1)
        else: # x.dim() == 2
            weight = self.weight.reshape(1, self.num_features)
            bias = self.bias.reshape(1, self.num_features)

        return weight * x_norm + bias
```

##### Ans08
Q: Pytorch手写Conv+Bn+Relu，及如何合并

```python
import torch
import torch.nn as nn
from torch.nn.utils.fusion import fuse_conv_bn_eval

class ConvBnReLU(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=False):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)
        self.bn = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        return self.relu(self.bn(self.conv(x)))

def fuse_conv_bn_relu(conv_bn_relu_module):
    """
    Fuses a ConvBnReLU module into a single Conv2d layer with ReLU.
    The module must be in evaluation mode (model.eval()).
    """
    assert isinstance(conv_bn_relu_module, ConvBnReLU), "Input must be an instance of ConvBnReLU."
    
    # Ensure the module is in eval mode before fusion
    conv_bn_relu_module.eval()

    # Fuse Conv and BatchNorm
    fused_conv = fuse_conv_bn_eval(conv_bn_relu_module.conv, conv_bn_relu_module.bn)

    # Return a Sequential module with the fused conv and ReLU
    # PyTorch's fuse_conv_bn_eval only fuses conv and bn, so ReLU needs to be explicitly added back.
    return nn.Sequential(fused_conv, conv_bn_relu_module.relu)
```

##### Ans09
Q: 描述图像resize的过程并实现
使用 OpenCV (推荐)
```python
import cv2

def resize_image_cv2(image, new_width, new_height, interpolation=cv2.INTER_LINEAR):
    """
    使用 OpenCV 调整图像大小。

    Args:
        image (numpy.ndarray): 输入图像 (OpenCV 格式)。
        new_width (int): 目标宽度。
        new_height (int): 目标高度。
        interpolation (int): 插值方法，例如 cv2.INTER_NEAREST, cv2.INTER_LINEAR (默认), cv2.INTER_CUBIC, cv2.INTER_LANCZOS4。

    Returns:
        numpy.ndarray: 调整大小后的图像。
    """
    return cv2.resize(image, (new_width, new_height), interpolation=interpolation)

# # 示例用法 (请自行添加图像加载和保存代码)
# # image = cv2.imread('input.jpg')
# # resized_image = resize_image_cv2(image, 200, 150)
# # cv2.imwrite('output_resized.jpg', resized_image)
```

##### Ans10
Q: PyTorch卷积与BatchNorm的融合

```python
import torch
import torch.nn as nn
from torch.nn.utils.fusion import fuse_conv_bn_eval

def fuse_conv_and_bn(conv_layer, bn_layer):
    """
    Fuses a convolutional layer and a BatchNorm layer into a single convolutional layer.
    Both layers must be in evaluation mode (model.eval()).
    """
    assert isinstance(conv_layer, (nn.Conv1d, nn.Conv2d, nn.Conv3d)), \
        "conv_layer must be a Conv1d, Conv2d, or Conv3d instance."
    assert isinstance(bn_layer, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)), \
        "bn_layer must be a BatchNorm1d, BatchNorm2d, or BatchNorm3d instance."
    
    conv_layer.eval()
    bn_layer.eval()

    fused_conv = fuse_conv_bn_eval(conv_layer, bn_layer)
    return fused_conv

def fuse_model_conv_bn(model):
    """
    Recursively fuses Conv and BatchNorm layers within a PyTorch model.
    The model must be in evaluation mode (model.eval()).
    """
    model.eval() # Ensure the model is in eval mode for fusion

    for name, module in model.named_children():
        if isinstance(module, nn.Sequential):
            # Process Sequential modules
            new_sub_module = nn.Sequential()
            i = 0
            while i < len(module):
                current_layer = module[i]
                if isinstance(current_layer, (nn.Conv1d, nn.Conv2d, nn.Conv3d)):
                    if i + 1 < len(module) and isinstance(module[i+1], (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
                        # Fuse Conv and BatchNorm
                        fused_layer = fuse_conv_and_bn(current_layer, module[i+1])
                        new_sub_module.add_module(str(len(new_sub_module)), fused_layer)
                        i += 2 # Skip the BatchNorm layer
                    else:
                        new_sub_module.add_module(str(len(new_sub_module)), current_layer)
                        i += 1
                else:
                    # Recursively apply fusion to sub-modules
                    fused_sub_module = fuse_model_conv_bn(current_layer)
                    new_sub_module.add_module(str(len(new_sub_module)), fused_sub_module)
                    i += 1
            setattr(model, name, new_sub_module)
        else:
            # Recursively apply fusion to other container modules
            fuse_model_conv_bn(module)
    return model
```


##### Ans11
Q: 分割网络损失函数Dice Loss代码实现

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class DiceLoss(nn.Module):
    def __init__(self, smooth=1e-6):
        super(DiceLoss, self).__init__()
        self.smooth = smooth

    def forward(self, inputs, targets):
        if inputs.dim() != targets.dim():
            raise ValueError("Input and target dimensions must match.")

        if inputs.dim() == 4:
            if inputs.shape[1] == 1:
                inputs = torch.sigmoid(inputs)
            else:
                inputs = F.softmax(inputs, dim=1)
        elif inputs.dim() == 3:
            inputs = torch.sigmoid(inputs.unsqueeze(1))
            targets = targets.unsqueeze(1)
        else:
            raise NotImplementedError("Only 3D or 4D tensors are supported.")

        inputs = inputs.view(inputs.size(0), inputs.size(1), -1)
        targets = targets.view(targets.size(0), targets.size(1), -1)

        intersection = (inputs * targets).sum(dim=2)
        union = inputs.sum(dim=2) + targets.sum(dim=2)

        dice_coefficient = (2. * intersection + self.smooth) / (union + self.smooth)
        dice_loss = 1 - dice_coefficient.mean()

        return dice_loss
```