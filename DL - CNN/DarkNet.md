

**DarkNet 的特點:**

1. 3x3 kernel, 
2. leaky ReLU, 
3. Residual connections, 
4. no fully connection layer

|                               |     |
| ----------------------------- | --- |
| [[### DarkNet 介紹]]            |     |
| [[### 比較CNN,ResNet, DarkNet]] |     |
|                               |     |

DarkNet53
![[Pasted image 20250515011756.png]]
DarkNet 53 模型的架構。 Res-n block 表示 n 個 Residual block，它先經過一個 half block 將特徵數減半，然後再經過 n 個 ResB block。在模型的最後一部分，存在一個平均池和一個 FC 層。對於AvgPool層來說，無論輸入大小是多少，都會轉換為1*1*out_chanel，經過FC層和Softmax操作之後，變成需要分類的類別數。

DarkNet19
![[Pasted image 20250515103855.png]]
### DarkNet 介紹

DarkNet 是一個由 Joseph Redmon (YOLO 系列的作者) 開發的開源深度學習框架，它本身也指代基於這個框架設計的一系列網路結構，最著名的就是用於 YOLO (You Only Look Once) 目標檢測算法的骨幹網路 (backbone)。DarkNet 的設計哲學強調**速度、效率和簡潔性**。

以下是 DarkNet 及其網路結構的一些主要特點：

1. **簡潔的架構和設計:**
    
    - DarkNet 的網路結構通常比一些流行的 CNN 架構 (如 VGG) 更簡潔，層數相對較少，但設計上更注重效率。
    - 它避免了過度複雜的模塊和大量的超參數。
2. **大量使用 3×3 卷積核:**
    
    - DarkNet 的設計受到 VGG 網路的影響，大量使用 3×3 的卷積核。作者認為 3×3 卷積核可以在保持較小參數量的同時，有效地提取圖像的局部特徵。
3. **沒有池化層 (在某些版本中):**
    
    - 在一些 DarkNet 的變體 (例如 DarkNet-53) 中，下採樣 (減少特徵圖尺寸) 並不是通過專門的池化層 (如 Max Pooling) 完成的，而是通過**步長 (stride) 大於 1 的卷積層**來實現。這樣做的目的是讓網路自己學習如何進行下採樣，而不是使用固定的池化操作。
4. **Leaky ReLU 激活函數:**
    
    - DarkNet 廣泛使用 Leaky ReLU 作為激活函數，而不是傳統的 ReLU。Leaky ReLU 對於負輸入會給予一個小的非零斜率 (例如 f(x)=x if x>0, f(x)=0.1x if x≤0)，這樣可以避免 ReLU 可能導致的“死亡 ReLU”問題 (某些神經元永遠不會被激活)。
5. **殘差連接 (在較深的 DarkNet 中):**
    
    - 較深的 DarkNet 變體 (例如 DarkNet-53，YOLOv3 的骨幹網路) 引入了**殘差連接 (Residual Connections)**，這與 ResNet 的核心思想相同。殘差連接允許網路學習殘差映射，有助於訓練更深的網路，並緩解梯度消失的問題。
6. **沒有全連接層 (在特徵提取部分):**
    
    - 作為目標檢測網路 YOLO 的骨幹網路，DarkNet 在提取圖像特徵的部分通常不包含全連接層。最後的輸出會被直接用於預測邊界框和類別概率。
7. **強調速度和效率:**
    
    - DarkNet 的設計目標之一是實現快速的推理速度，這對於實時目標檢測至關重要。因此，它在網路結構的設計上會考慮計算效率。

**DarkNet 與一般 CNN 的差別:**

- **架構哲學:** 一般的 CNN 架構種類繁多，設計目標也各不相同 (例如追求最高的分類準確度)。DarkNet 則更專注於速度和效率，特別是對於目標檢測任務。
- **下採樣方式:** 許多傳統 CNN 使用池化層進行下採樣，而一些 DarkNet 變體則使用步長卷積。
- **激活函數:** 雖然 ReLU 是許多 CNN 的常用激活函數，但 DarkNet 傾向於使用 Leaky ReLU。
- **網路深度和複雜度:** 相較於一些追求極高準確度的 CNN (例如一些用於 ImageNet 分類的深層網路)，DarkNet 的結構可能更簡潔。

**DarkNet 與 ResNet 的差別:**

- **主要設計目標:** ResNet 的主要目標是解決深層網路的梯度消失和退化問題，從而可以訓練非常深的網路並提高分類準確度。雖然較深的 DarkNet 也使用了殘差連接來訓練更深的網路，但其整體設計仍然更偏向於速度和效率，以適應實時目標檢測的需求。
- **網路深度:** ResNet 通常具有非常深的網路結構 (例如 ResNet-50, ResNet-101, ResNet-152)，而 DarkNet 的深度 (例如 DarkNet-19, DarkNet-53) 相對較淺，但結構更緊湊。
- **下採樣策略:** ResNet 主要使用 Max Pooling 進行下採樣，而 DarkNet (在某些版本中) 使用步長卷積。
- **激活函數的選擇:** ResNet 主要使用 ReLU 激活函數，而 DarkNet 主要使用 Leaky ReLU。
- **應用領域的側重:** ResNet 在圖像分類等任務上表現出色，而 DarkNet (及其骨幹網路) 主要被設計用於目標檢測任務 YOLO。

**總結:**

DarkNet 是一個為速度和效率優化的 CNN 框架及其網路結構。它通過簡潔的設計、大量使用 3×3 卷積、Leaky ReLU 激活函數以及在較深版本中引入殘差連接等特點，在目標檢測任務上取得了很好的平衡。與一般的 CNN 相比，DarkNet 更注重速度。與 ResNet 相比，雖然都使用了殘差連接來訓練更深的網路，但 DarkNet 的整體設計仍然以效率為優先考量，並且在下採樣和激活函數的選擇上有所不同，其主要應用領域也更側重於實時目標檢測。



### 比較CNN,ResNet, DarkNet

詳細解釋和比較一般卷積神經網路 (CNN)、ResNet (殘差網路) 和 Darknet 之間的差別。

### 1. 一般卷積神經網路 (General CNN)

**核心概念：** 卷積神經網路（Convolutional Neural Network, CNN）是一種深度學習模型，其設計靈感來源於動物視覺皮層的結構。它特別擅長處理具有網格狀拓撲結構的數據，例如圖像（2D網格的像素）和時間序列數據（1D網格）。

**典型結構層：** 一個基本的 CNN 通常由以下幾種類型的層堆疊而成：

1. **卷積層 (Convolutional Layer)：**
    
    - **核心操作**：使用稱為「濾波器」(filter) 或「卷積核」(kernel) 的小窗口在輸入數據上滑動，進行卷積運算。
    - **目的**：提取局部特徵。例如，在圖像中，初級卷積層可能學習檢測邊緣、角點、顏色斑塊等基本特徵；更深層的卷積層則基於這些低級特徵組合成更複雜的特徵，如物體的某些部分。
    - **特性**：
        - **局部感受野 (Local Receptive Fields)**：每個神經元只連接到前一層輸入的一個小區域。
        - **權重共享 (Weight Sharing)**：同一個濾波器在整個輸入數據上共享相同的權重，大大減少了模型的參數數量，並使其具有平移不變性。
        - **多個濾波器**：通常使用多個不同的濾波器來提取多種特徵。
2. **激活函數層 (Activation Function Layer)：**
    
    - **目的**：在卷積運算之後引入非線性，使得網路能夠學習更複雜的函數。如果沒有非線性激活，多層網路的效果就等同於一個單層網路。
    - **常用函數**：ReLU (Rectified Linear Unit) 及其變體 (Leaky ReLU, PReLU)、Sigmoid、Tanh 等。ReLU 是目前最常用的激活函數之一，因其計算簡單且能有效緩解梯度消失問題。
3. **池化層 (Pooling Layer)：**
    
    - **目的**：對特徵圖進行下採樣 (downsampling)，以減少數據的空間維度（寬度和高度），從而減少計算量、控制過擬合，並使特徵具有一定的尺度不變性。
    - **常用操作**：
        - **最大池化 (Max Pooling)**：選取感受野內的最大值。
        - **平均池化 (Average Pooling)**：計算感受野內的平均值。
    - 通常在連續的幾個卷積層之後插入一個池化層。
4. **全連接層 (Fully Connected Layer)：**
    
    - **目的**：在經過多個卷積和池化層提取特徵後，全連接層通常位於網路的末端，用於將學習到的分佈式特徵表示映射到樣本標記空間，進行最終的分類或回歸。
    - **操作**：該層的每個神經元都與前一層的所有神經元相連接。

**傳統 CNN 的問題 (尤其在深度網路中)：** 隨著網路深度的增加，傳統的「純粹」CNN（或稱為 Plain Network，即簡單堆疊卷積層）會遇到以下問題：

- **梯度消失/爆炸 (Vanishing/Exploding Gradients)**：在反向傳播過程中，梯度經過多個層的連乘後，可能會變得極小（梯度消失）或極大（梯度爆炸），導致淺層網路的權重無法有效更新，模型難以訓練。
- **網路退化 (Degradation Problem)**：令人驚訝的是，即使梯度消失問題得到一定程度的緩解（例如通過更好的初始化或歸一化層），研究者發現，當網路深度增加到一定程度後，訓練準確率反而會飽和甚至下降。這並非由過擬合引起，因為在訓練集上的表現也會變差。

### 2. ResNet (Residual Network)

**核心概念：** ResNet (Residual Network，殘差網路) 由微軟研究院的何愷明等人於2015年提出，旨在解決深度 CNN 中的網路退化問題，使得訓練非常深的網路成為可能。

![[Resnet_block.jpg]]

**核心思想與機制：**

1. **殘差學習 (Residual Learning)**：
    
    - ResNet 的核心思想是引入「殘差塊」(Residual Block)。與其讓網路直接學習一個目標映射 H(x)（其中 x 是輸入），不如讓它學習一個殘差函數 F(x)=H(x)−x。原始的目標映射則變為 H(x)=F(x)+x。
    - 研究者假設，學習殘差 F(x) 通常比學習原始的複雜映射 H(x) 更容易。特別是，如果某個恆等映射（即 H(x)=x）是最優的，那麼將殘差 F(x) 推向零比通過一堆非線性層擬合恆等映射要容易得多。
2. **快捷連接 (Shortcut Connection / Skip Connection)**：
    
    - 這是實現殘差學習的關鍵。在一個殘差塊中，輸入 x 會通過一條「快捷路徑」直接跳過一層或多層（這些層構成 F(x)），然後將其自身加到這些層的輸出上。
    - 數學表示：y=F(x,{Wi​})+x，其中 y 是殘差塊的輸出，F(x,{Wi​}) 是需要學習的殘差映射。
    - **作用**：
        - **緩解梯度消失(vanishing gradient)**：快捷連接提供了一條梯度的「高速公路」，使得梯度可以直接反向傳播到較淺的層，避免了梯度在深層網路中因連乘而衰減過快的問題。
        - **解決退化(Degradation problem)問題**：即使中間的權重層 F(x) 什麼也沒學到（例如權重都為零），由於快捷連接的存在，網路至少可以執行恆等映射，不會比更淺的網路表現更差。這使得網路更容易向更深的方向優化。
    
|特性|梯度消失問題 (Vanishing Gradient)|網路退化問題 (Degradation Problem)|
|:--|:--|:--|
|**問題本質**|反向傳播過程中，梯度信號因逐層衰減而變得過於微弱。|深度網路的優化極其困難，導致其性能（包括訓練性能）劣於較淺的網路。|
|**主要表現**|淺層網路權重更新緩慢或停滯，模型訓練困難。|更深的網路在**訓練集和測試集**上的準確率均低於較淺的網路。|
|**直接原因**|激活函數導數的連乘效應、不當的權重初始化。|優化器難以在非常深的網路中找到好的解，特別是學習恆等映射等簡單功能都很困難。|
|**與網路深度的關係**|深度增加時問題更易出現和加劇。|深度增加到一定程度後顯現，表明增加層數反而有害。|
|**是否是過擬合**|不是。是訓練本身的問題。|不是。訓練誤差本身就更高。|
|**主要解決思路**|改善梯度流（如ReLU、Batch Norm、快捷連接的部分作用）、更好的初始化。|改變網路學習目標（如殘差學習），使優化更容易進行（如ResNet的快捷連接）。|
|**聯繫**|梯度消失會使優化變得困難，是導致或**加劇**網路退化的一個潛在因素。|即使梯度消失問題得到一定緩解，退化問題仍可能存在，表明其核心是優化瓶頸。|

**結構特點：**

- ResNet 由許多堆疊的殘差塊構成。
- 常見的殘差塊有兩種：
    - **基本塊 (Basic Block)**：通常用於較淺的 ResNet (如 ResNet-18, ResNet-34)，包含兩個 3x3 卷積層。
    - **瓶頸塊 (Bottleneck Block)**：用於較深的 ResNet (如 ResNet-50, ResNet-101, ResNet-152)，包含一個 1x1 卷積（降維）-> 一個 3x3 卷積 -> 一個 1x1 卷積（升維）。這種設計可以在保持感受野大小的同時，有效減少參數數量和計算量。
- ResNet 的成功使得訓練數百層甚至上千層的網路成為現實，並在 ImageNet 等多個圖像識別競賽中取得了突破性成果。

### 3. Darknet

**核心概念：** Darknet 是一個開源的神經網路框架，同時也指代一系列專為 YOLO (You Only Look Once) 物件偵測算法設計的骨幹網路架構。它由 Joseph Redmon 開發。

![[Pasted image 20250515103855.png]]

**版本與演進：**

1. **Darknet-19 (用於 YOLOv2)**：
    
    - 包含 19 個卷積層和 5 個最大池化層。
    - 設計相對簡潔，旨在平衡速度與精度。
    - 主要使用 3x3 和 1x1 卷積濾波器。
    - 在 ImageNet 上進行預訓練，然後用於 YOLOv2 的特徵提取。
2. **Darknet-53 (用於 YOLOv3)**：
    
    - 這是一個更深、更強大的版本，包含 53 個卷積層。
    - **關鍵特性：引入了殘差連接。** Darknet-53 的設計大量借鑒了 ResNet 的思想，包含了許多殘差塊。這使得 Darknet-53 可以做得更深，從而提取更複雜的特徵，同時避免了網路退化問題。
    - 每個殘差塊通常包含一個 1x1 卷積層（用於減少通道數）和一個 3x3 卷積層（用於特徵提取），然後通過快捷連接將輸入加到輸出上。
    - 相較於 ResNet，Darknet-53 的一個特點是更傾向於使用步長為 2 的卷積層 (strided convolution) 來進行下採樣，而不是像 ResNet 中那樣頻繁使用池化層。這有助於保留更多的細粒度特徵信息，這對物件偵測任務可能更有利。
    - 目標是在保持較高偵測精度的同時，實現更快的推論速度，使其適用於實時物件偵測。

**Darknet 與 ResNet 的關係：** Darknet-53 可以被視為 ResNet 思想在物件偵測領域特定應用和優化的一個實例。它採用了 ResNet 核心的殘差學習機制來構建深度網路，但在具體的網路層配置（如卷積層的組合、下採樣策略）上，Darknet 進行了調整以更好地適應 YOLO 檢測框架的需求，特別是對於速度和效率的考量。

**後續發展 (如 CSPDarknet53 - YOLOv4)：** 在 Darknet-53 的基礎上，YOLOv4 引入了 CSPDarknet53，它結合了 CSPNet (Cross Stage Partial Network) 的思想，進一步優化了 Darknet 架構，減少了計算冗餘，增強了梯度流，從而在保持甚至提升精度的同時，進一步提高了效率。

### 總結比較

| 特性         | 一般 CNN (Plain Network)    | ResNet                        | Darknet (以 Darknet-53 為例)            |
| :--------- | :------------------------ | :---------------------------- | :----------------------------------- |
| **核心思想**   | 堆疊卷積層和池化層進行特徵提取           | 通過快捷連接實現殘差學習，解決深度網路訓練困難問題     | 為 YOLO 設計的高效特徵提取器，借鑒並應用殘差學習          |
| **深度限制**   | 較淺時有效；深度增加易出現梯度消失/爆炸和網路退化 | 能夠訓練非常深的網路 (數百上千層)            | 能夠構建較深的網路 (如53層)，得益於殘差結構             |
| **關鍵機制**   | 卷積、池化、激活                  | 跳躍連接 (skip Connection)**、殘差塊  | **殘差塊 (類似ResNet)**、1x1和3x3卷積組合       |
| **主要解決問題** | 基本的圖像特徵提取                 | **網路退化問題**、梯度消失問題             | 如何為實時物件偵測高效提取多尺度特徵                   |
| **典型應用**   | 早期圖像分類、簡單圖像任務             | 圖像分類 (ImageNet SOTA)、遷移學習骨幹網路 | **YOLO 物件偵測的骨幹網路**                   |
| **與其他關係**  | 基礎架構                      | 對 CNN 發展有里程碑意義，被後續架構廣泛借鑒      | 受 ResNet 啟發，採用其核心思想，並針對偵測任務優化        |
| **下採樣方式**  | 通常依賴池化層                   | 混合使用池化層和步長卷積                  | 更傾向於使用**步長卷積 (strided convolution)** |
| **設計側重**   | 通用性                       | 突破深度限制，提升模型容量和準確性             | 在 YOLO 框架下的**速度與精度平衡**               |

匯出到試算表

**簡而言之：**

- **一般 CNN** 是基礎，但在構建深度網路時遇到了瓶頸。
- **ResNet** 通過引入殘差學習和快捷連接，革命性地解決了深度網路的訓練難題，使得構建極深的網路成為可能，並顯著提升了性能。
- **Darknet** (尤其是 Darknet-53) 是為 YOLO 物件偵測系統量身定制的骨幹網路。它成功地將 ResNet 的核心思想（殘差學習）應用於其中，以構建一個既深層（能夠提取豐富特徵）又高效（適合實時偵測）的特徵提取器。

可以說，ResNet 提出了一種通用的解決方案來構建深度有效的網路，而 Darknet 則是這一方案在特定應用場景（物件偵測）下的一個成功實踐和調整。