
## 1. VGG

![[vgg.png]]
VGG是由牛津大學計算機視覺組Visual Geometry Group提出(這也是VGG名稱的由來)，並在2014年的ILSVRC比赛上獲得Localization Task (定位任務) 第一名和 Classification Task (分類任務) 第二名。
- 13層卷積層(convX-XXX)，第一個數字為卷積核大小，第二個數字為卷積層通道數
- 3層全連接層(FC-XXXX)
- 5層最大池化層(maxpool)

卷積層與全連接層具有權重係數，因此也被稱為權重層，總數目為13+3=16，這即是VGG16中16的來源。
#### 特點
- 卷積核大小(kernel size)統一為3x3，
- 最大池化層(maxpool)統一為2x2
- 利用較小的卷積核(ex：3x3)來替代較大的卷積核(ex：5x5，7x7)，在感受野相同的情況下提升網路深度，一個5x5的能用兩個3x3的替代，一個7x7能用三個3x3替代
#### 優/缺點
- VGG的架構簡單統一
- 證明較深的層數能提高效能
- 參數量龐大，計算資源需求高
- 訓練時間過長，難以調整參數

## 2. ResNet

ref: [直觀理解ResNet —簡介、 觀念及實作(Python Keras)](https://medium.com/@rossleecooloh/%E7%9B%B4%E8%A7%80%E7%90%86%E8%A7%A3resnet-%E7%B0%A1%E4%BB%8B-%E8%A7%80%E5%BF%B5%E5%8F%8A%E5%AF%A6%E4%BD%9C-python-keras-8d1e2e057de2)
ref: [Understanding ResNet-50 in Depth](https://wisdomml.in/understanding-resnet-50-in-depth-architecture-skip-connections-and-advantages-over-other-networks/)

[[ResNet]]
![[Pasted image 20241103111718.png]]

![[resnet50.webp]]

ResNet-50 深度殘差網絡是一種在影像辨識領域廣泛使用的深度學習模型，它由微軟亞洲研究院的何愷明等人於 2015 年提出。其核心創新在於引入了「殘差連接」（Residual Connection），有效地解決了深度神經網絡訓練時的梯度消失問題，使得構建更深的網絡成為可能。

以下是對 ResNet-50 架構的詳細解釋：

**1. 基本概念：殘差連接 (Residual Connection)**

- 傳統的深度神經網絡在加深層數時，容易出現梯度消失或梯度爆炸的問題，導致訓練困難。
- ResNet 的核心思想是讓網絡學習「殘差」，也就是輸入與輸出的差異。
- 通過「殘差連接」，將輸入直接添加到輸出上，使得網絡可以學習到一個「恆等映射」（Identity Mapping）。
- 這樣，即使網絡層數很深，也能夠有效地訓練。

**2. ResNet-50 的架構**

- ResNet-50 共有 50 個卷積層。
- 其基本結構可以分為以下幾個部分：
    - **初始卷積層 (Initial Convolution Layer)**: 處理輸入圖像的初始特徵提取。
    - **殘差塊 (Residual Blocks)**: 由多個卷積層和殘差連接組成，是 ResNet-50 的核心部分。
    - **平均池化層 (Average Pooling Layer)**: 將特徵圖轉換為固定大小的向量。
    - **全連接層 (Fully Connected Layer)**: 將向量映射到最終的類別輸出。

**3. 殘差塊的細節：**

ResNet-50 主要使用兩種殘差塊：

- **基礎殘差塊 (Basic Residual Block):**
    - 由兩個 3x3 卷積層組成。
    - 適用於較淺的 ResNet 模型。
- **瓶頸殘差塊 (Bottleneck Residual Block):**
    - 由三個卷積層組成：1x1 卷積層（用於降維）、3x3 卷積層、1x1 卷積層（用於升維）。
    - 1X1的卷積用於降維和升維可以有效的減少運算量。
    - 適用於較深的 ResNet 模型，如 ResNet-50。
    - 殘差連接會把第一層的輸入數值，與第三層的輸出數值作相加，以此做為殘差運算。

**4. ResNet-50 的特點與作用**

- **解決梯度消失問題:** 殘差連接使得梯度能夠更容易地傳播到較淺的層，從而解決了梯度消失問題。
- **允許構建更深的網絡:** 由於梯度消失問題的解決，ResNet 可以構建非常深的網絡，從而提高模型的性能。
- **提高影像辨識準確率:** ResNet 在 ImageNet 等影像辨識基準測試中取得了卓越的成績。
- **廣泛應用:** ResNet 已成為影像辨識、目標檢測、語意分割等領域的重要基礎模型。

**5. 簡單的圖示**

由於產生圖像的複雜度過高，在此提供簡單的文字圖示，讓您更容易理解。

```
ResNet-50 的大致結構：

[初始卷積層] -> [瓶頸殘差塊 x 16] -> [平均池化層] -> [全連接層]

瓶頸殘差塊的結構：

[1x1 卷積層] -> [3x3 卷積層] -> [1x1 卷積層]
  \____________________________________/
                   |
                [殘差連接]
```

希望這些資訊能幫助您更深入地了解 ResNet-50 深度殘差網絡的架構。

---

### 🧠 ResNet 的主要特點：

1. **使用殘差學習（Residual Learning）**：
    
    - ResNet引入了「殘差塊（Residual Block）」，透過**shortcut connection（捷徑連接）**，跳過一些層，讓輸入直接加到輸出：
        
        Output=F(x)+x\text{Output} = F(x) + xOutput=F(x)+x
        - 其中 F(x)F(x)F(x) 是某些卷積、BatchNorm、ReLU 等的轉換，xxx 是輸入。
            
        - 這樣做的好處是，如果學不到更好的轉換 F(x)F(x)F(x)，至少可以讓網路學會「保留輸入」，避免退化。
            
2. **可以訓練更深的網路（超過100層）**：
    
    - 傳統CNN在層數增加時，容易遇到**梯度消失/爆炸**(Vanishing gradient / Exploding gradient )或**準確率下降**的問題。
        
    - ResNet透過殘差結構，讓誤差可以更容易反向傳播回前面的層，解決了這個問題。
        
3. **效能強大**：
    
    - ResNet在ImageNet等多個資料集上表現優異。
        
    - 深度從18層、34層到152層（甚至更深）都有良好訓練效果。
        

---

### 🔍 ResNet 和一般 CNN 的不同之處：

|特徵|一般 CNN|ResNet（殘差網路）|
|---|---|---|
|結構設計|一層層疊加 Convolution、ReLU、Pooling|多了 Shortcut（跳連接）|
|深度限制|太深容易過擬合或梯度消失|可以非常深（>100層）|
|訓練穩定性|深網路訓練困難|殘差幫助反向傳播，更穩定|
|計算效率|結構簡單但可擴展性有限|加了捷徑但仍有效率且效果更好|
|解決的問題|適合淺層網路|解決「深層網路效果反而變差」的退化問題|

---

### 📝 為什麼 ResNet 的設計有效？

- 傳統CNN想學 H(x)H(x)H(x)（某種目標函數），但當網路很深時難以收斂。
    
- ResNet 改為學 F(x)=H(x)−xF(x) = H(x) - xF(x)=H(x)−x，換句話說：學的是「輸入和輸出差多少」。
    
- 若真的不需要額外學習，ResNet 會讓 F(x)=0F(x) = 0F(x)=0，那輸出就是 xxx，至少保留原始訊息，不會變得更糟。





殘差連接的核心作用確實主要體現在殘差塊（residual block）的內部。以下我將更詳細地說明：

**殘差連接的本質：**

- 殘差連接的主要目的是解決深度神經網絡中的梯度消失問題。
- 它通過在殘差塊內部創建一個「捷徑」（shortcut），將輸入直接添加到輸出上。
- 這樣，網絡可以學習的是輸入和輸出的「殘差」，也就是它們之間的差異。

**殘差連接與殘差塊：**

- 殘差連接是殘差塊這個結構的重要組成部分。
- 在一個殘差塊內部，通常包含多個卷積層，而殘差連接則將塊的輸入直接添加到這些卷積層的輸出上。
- 也就是說，這個「跳躍」的連接是包含在一個殘差塊的内部的。

**殘差塊與殘差塊之間的連接：**

- 殘差塊與殘差塊之間，是通過連續的卷積層連接的。
- 也就是說，一個殘差塊的輸出，會作為下一個殘差塊的輸入。
- 因此，block與block之間的連接是串連的，並非跳躍的殘差連接。

**總結：**

- 殘差連接的核心作用範圍在於殘差塊的內部。
- 它通過「捷徑」連接，使得殘差塊能夠更有效地學習輸入和輸出之間的差異。
- 殘差塊與殘差塊之間，並沒有使用殘差連接，而是使用標準的卷積串連模式。

