tracking-by-detection
1. YOLO + SORT, DeepSORT, ByteTrack (ReID)

tracking-by-attention
1. TrackFormer

2. GroundingDINO + Tracker



影片物件偵測 (Video Object Detection, VOD) 是電腦視覺中的一個核心且充滿挑戰的領域，尤其在手術影片這種專業場景下，其複雜性和要求更高。

以下我將為您詳細解釋VOD的主流方法、最新趨勢、與VideoMAE的比較，以及在手術影片偵測中的具體應用與優缺點。

---

### 一、 影片物件偵測 (VOD) 的主流與經典方法

VOD的目標是在影片的每一幀中定位並分類物件。與靜態圖像物件偵測不同，VOD的關鍵在於**如何有效利用時序資訊 (temporal information)** 來提升偵測的準確性和穩定性。經典方法主要分為兩大類：

#### 1. 基於靜態圖像檢測器的後處理 (Post-processing on Still-Image Detectors)

這是最直觀的方法，核心思想是「先偵測，再追蹤/關聯」。

- **工作流程**：
    
    1. 對影片的每一幀獨立運行一個高效的圖像物件偵測器（如 YOLO, Faster R-CNN, SSD）。
        
    2. 對得到的偵測框進行後處理，將屬於同一個物體的偵測框在時間維度上串聯起來，形成物體軌跡。
        
- **代表方法**：
    
    - **Detect and Track (D&T)**：結合偵測器和追蹤器（如 Kalman Filter, DeepSORT）。當某一幀的偵測失敗時，追蹤器可以根據物體的運動模型預測其位置，增加穩定性。
        
    - **Seq-NMS (Sequence Non-Maximum Suppression)**：傳統NMS只在一幀內操作，而Seq-NMS會考慮鄰近幀的偵測結果，抑制那些在時間上不連續、可能是誤報的偵測框。
        
- **優點**：
    
    - 實現簡單，可以靈活選用任何先進的圖像偵測器。
        
- **缺點**：
    
    - **資訊利用不充分**：偵測階段完全沒有利用時序資訊，對於因運動模糊、遮蔽而難以偵測的幀無能為力。
        
    - **錯誤累積**：追蹤器的錯誤會在後續幀中不斷累積。
        
    - **速度瓶頸**：對每一幀都運行重量級檢測器，計算成本高。
        

#### 2. 利用時序資訊增強特徵 (Feature Enhancement using Temporal Information)

這類方法的核心思想是，**當前幀的特徵可以被鄰近幀的特徵所增強**。

- **工作流程**：
    
    1. 對於一個目標幀（例如第 `t` 幀），同時提取其與鄰近參考幀（如 `t-k` 到 `t+k` 幀）的深度特徵。
        
    2. 透過某種對齊或聚合機制，將參考幀的特徵"借"給目標幀，從而豐富目標幀的特徵表示。
        
- **代表方法**：
    
    - **FGFA (Flow-Guided Feature Aggregation)**：這是一個里程碑式的工作。它首先計算參考幀與目標幀之間的**光流 (Optical Flow)**，光流描述了像素的運動軌跡。然後，根據光流將參考幀的特徵圖 (feature map) **扭曲 (warp)** 到目標幀的座標系下，最後加權聚合這些特徵。
        
- **優點**：
    
    - **顯著提升準確性**：如果一個物體在當前幀被遮擋或模糊，但它在鄰近幀是清晰的，FGFA可以將清晰的特徵傳遞過來，從而成功檢測。
        
- **缺點**：
    
    - **計算成本高**：光流的計算本身非常耗時。
        
    - **非端到端**：通常需要預先計算光流，流程較為割裂。
        

---

### 二、 最新且重要的影片物件偵測方法

近年來，隨著Transformer在視覺領域的成功，VOD的主流已經轉向了**端到端 (End-to-End)** 的架構。

#### 基於Transformer的端到端方法

這類方法將VOD視為一個序列預測問題，其革命性的思想是**物件查詢 (Object Query)** 的跨幀傳播。

- **核心思想**：
    
    - 在第一幀初始化一組可學習的 "Object Queries"，每個Query代表一個潛在的物體。
        
    - 在後續的每一幀，這些Queries會不斷與該幀的圖像特徵進行交互，更新自己的狀態，並預測物體的位置。
        
    - 前一幀的輸出Queries會直接作為後一幀的輸入，實現了物體身份的自然繼承和跨幀關聯。
        
- **代表方法**：
    
    - **Video-DETR / Deformable DETR for Video**：這是將DETR架構擴展到影片的早期探索，驗證了Query傳播機制的有效性。
        
    - **MeVi-Da / StreamYOLO / YOLOX-Track**：這些是近年來在平衡速度和精度上做得非常出色的模型。它們通常結合了DETR的思想和YOLO系列的高效率，透過精心設計的Query傳播和更新機制，實現了高效且高精度的端到端VOD。它們不僅僅是偵測，同時也完成了追蹤（MOT - Multi-Object Tracking）。
        
- **優點**：
    
    - **性能卓越**：端到端訓練使得模型能夠學習到更複雜的時空關聯，準確性通常最高。
        
    - **魯棒性強**：Query傳播機制對於短時遮蔽非常魯棒，因為Query攜帶著物體的"記憶"。
        
    - **流程簡潔**：無需後處理或額外的追蹤模塊。
        
- **缺點**：
    
    - **計算複雜度高**：Transformer的注意力機制計算成本較高，對硬體要求也高。
        
    - **訓練成本高**：需要大量的影片數據進行訓練。
        

---

### 三、 與 VideoMAE 方法的比較

這裡必須先釐清一個關鍵概念：**VOD方法與VideoMAE並不是競爭關係，而是互補關係。**

- **VideoMAE (Masked Autoencoders for Videos) 是什麼？**
    
    - VideoMAE是一種**自監督學習 (Self-supervised Learning)** 的**預訓練方法**。它的目標不是去完成偵測任務，而是從海量的**無標註影片**中學習通用的、強大的**影片特徵表示 (Video Representation)**。
        
    - 其工作方式是：隨機遮蓋 (mask) 掉影片中的大量小方塊 (patch)，然後訓練一個Transformer模型去重建這些被遮蓋的內容。透過這個過程，模型被迫學習到關於物體外觀、運動和場景的深層語義知識。
        
- **如何比較？** 正確的比較對象是：
    
    1. 一個VOD模型（如StreamYOLO），其骨幹網路使用常規方法（如在ImageNet上預訓練）進行初始化。
        
    2. **同一個VOD模型**，但其骨幹網路使用**VideoMAE在大量影片上預訓練好的權重**進行初始化，然後再在特定的VOD數據集上進行微調 (fine-tune)。
        
- **比較結論**：
    
    - **VideoMAE作為一種強大的預訓練範式，能夠極大地增強下游VOD模型的性能。**
        
    - 使用VideoMAE預訓練的模型通常收斂更快，並且在目標VOD任務上能達到更高的精度。
        
    - **優勢來源**：VideoMAE學到的不僅是靜態外觀，更是**運動和時序的模式**。這種先驗知識對於VOD任務來說是極其寶貴的，尤其是在標註數據有限的情況下，其優勢會更加明顯。
        

**總結：VideoMAE是為VOD模型提供更優「起跑線」的「教練」，而不是同場競技的「選手」。**

---

### 四、 在手術影片物件偵測的應用與優缺點

手術影片是VOD應用中一個極具挑戰性但價值巨大的場景。其挑戰主要來自：

- **物體挑戰**：手術器械尺寸小、移動快、外形相似；人體組織柔軟、易變形、紋理相似。
    
- **環境挑戰**：視角頻繁變化、鏡頭起霧、血液和煙霧造成的遮擋、手術燈導致的強烈反光。
    
- **任務要求**：對準確性和即時性要求極高。
    

以下是各類方法在手術VOD應用中的優缺點列表：

|方法類別|在手術影片偵測中的優點|在手術影片偵測中的缺點|
|---|---|---|
|**經典方法 (後處理/特徵增強)**|1. **概念相對簡單**，易於實現和調試。  <br>2. FGFA等方法利用光流，理論上能處理因快速移動導致的運動模糊問題。  <br>3. 可以直接利用在醫學圖像上微調過的強大靜態檢測器。|1. **對快速移動和頻繁遮擋極其敏感**，手術器械的快速進出視野很容易導致追蹤失敗。  <br>2. **光流計算昂貴**，且在紋理單一的人體組織或有反光的場景中容易失效。  <br>3. **錯誤累積問題嚴重**，一旦一個器械被跟丟，很難再重新關聯。  <br>4. 無法很好地區分外觀相似的不同器械。|
|**最新端到端方法 (Transformer)**|1. **對遮擋的魯棒性極佳**：Object Query的記憶功能使其在器械被短暫遮擋後仍能"記住"它，並在重現時立即識別。  <br>2. **能建模複雜互動**：端到端訓練能學習到器械與組織、器械與器械之間的時空關係。  <br>3. **準確性高**：能夠學習到更細微的特徵，區分外觀相似的器械。|1. **計算成本高**，可能難以滿足手術室對**即時性**的嚴苛要求。  <br>2. **需要大量高質量的標註數據**，而手術影片的標註極其耗時且需要高度專業知識，數據獲取成本極高。  <br>3. 對於縫合線等**極細小的物體**，如果模型設計不當，可能會忽略。|
|**結合VideoMAE預訓練的方法**|1. 🌟 **最大優勢：顯著降低對標註數據的依賴**。可以在海量**無標註**的手術影片上進行預訓練，學習手術場景通用的時空特徵。  <br>2. **特徵更魯棒**：預訓練學到的特徵對反光、煙霧、模糊等手術中常見的干擾有更強的抵抗力。  <br>3. **性能天花板更高**：用VideoMAE預訓練後再進行微調，通常比從零開始訓練或用ImageNet預訓練能達到更高的準確率和穩定性。  <br>4. **是目前學術界和工業界公認的最有潛力的方向**。|1. **預訓練階段的計算開銷極大**，需要非常強大的GPU集群和長時間的訓練。  <br>2. **推理速度受限於基礎VOD架構**：雖然提升了準確性，但推理時的計算量並沒有減少，即時性問題依然存在，需要依賴模型壓縮和硬體加速。|

匯出到試算表

### 結論與建議

對於手術影片物件偵測這樣的複雜任務：

- **經典方法**由於其固有的缺陷，已逐漸難以滿足高要求的應用。
    
- **基於Transformer的端到端方法**是當前性能的標杆，其處理遮擋和複雜場景的能力非常適合手術環境。
    
- **VideoMAE等自監督預訓練方法**則完美地解決了端到端方法「數據飢渴」的核心痛點。
    

因此，**目前最前沿、最重要且最被看好的方法，無疑是「採用VideoMAE在大量無標註手術影片上進行預訓練，然後在一個精標註的小規模數據集上對高效的端到端VOD模型（如StreamYOLO或改進的Video-DETR）進行微調」的技術路線。** 這條路徑結合了三者的優點，最大程度上平衡了準確性、魯棒性和數據依賴性，是推動AI在智慧手術領域應用的關鍵。
