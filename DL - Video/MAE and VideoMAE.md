

|                                 |     |
| ------------------------------- | --- |
| [[###### VideoMAE 的網路架構]]       |     |
| [[###### 例子解說MAE]]              |     |
| [[ ###### VideoMAE 模型細節]]       |     |
| [[###### VideoMAE V1 與 V2 的差異]] |     |
|                                 |     |
CVPR2023-VideoMAEv2-MAE视频化续作 - 赵晓程的文章 - 知乎
https://zhuanlan.zhihu.com/p/670040233

VideoMAE v2：训练数十亿参数的视频模型！ - 越洋飞机的文章 - 知乎
https://zhuanlan.zhihu.com/p/2906234248



###### VideoMAE 的網路架構

詳細解釋 VideoMAE 的網路架構，並與經典的 Vision Transformer (ViT) 進行深入比較，闡明 VideoMAE 究竟在架構上做了哪些關鍵性的擴展來高效處理影片數據。

### 總結性回答

首先，回答您最核心的問題：**是的，VideoMAE 的網路架構完全是基於 Vision Transformer (ViT) 的。** 您可以將 VideoMAE 的編碼器 (Encoder) 視為一個標準的 ViT。然而，VideoMAE 為了高效地進行影片的自監督學習，在**整體框架**和**數據處理流程**上引入了根本性的創新，這使得它與用於圖像分類的經典 ViT 有著天壤之別。

---

### 第一部分：經典 Vision Transformer (ViT) 架構回顧

要理解 VideoMAE 的創新，我們必須先快速回顧經典 ViT 的工作流程（以圖像分類為例）：

1. **輸入**：一張 2D 圖片。
    
2. **圖片分塊 (Patching)**：將圖片切割成一個個不重疊的小方塊 (Patch)，例如 16x16 像素。
    
3. **線性投影 (Linear Projection)**：將每個扁平化的 Patch 通過一個線性層，轉換為一個固定長度的向量 (Token)。
    
4. **位置編碼 (Positional Embedding)**：為每個 Token 添加位置資訊，讓模型知道每個 Patch 的原始相對位置。
    
5. **核心處理器 (Transformer Encoder)**：
    
    - 將**所有**的 Tokens（包括一個特殊的 `[class]` token）送入一個標準的 Transformer 編碼器。
        
    - 在編碼器內部，通過多層的自注意力機制 (Self-Attention)，讓**每一個 Patch Token 都能與所有其他的 Patch Token 進行信息交互**。
        
    - 目標是學習到整張圖片的全局特徵表示。
        
6. **輸出 (MLP Head)**：
    
    - 僅取出 `[class]` token 對應的最終輸出向量。
        
    - 將這個向量送入一個簡單的多層感知機 (MLP) 分類頭，最終輸出一張圖片的類別預測。
        

**經典 ViT 的核心特點**：它是一個**單一、對稱的編碼器架構**，其設計目標是處理**所有**輸入數據（100% 的 Patches），以完成監督學習下的分類任務。

---

### 第二部分：VideoMAE 網路架構詳解

VideoMAE 的目標不是進行分類，而是從海量的無標籤影片中進行**自監督預訓練 (Self-supervised Pre-training)**，學習通用的時空表徵。為此，它引入了革命性的**非對稱編碼器-解碼器 (Asymmetric Encoder-Decoder)** 架構。

VideoMAE github
https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/videomaev2/README.md

![[237352561-6204d743-8705-43f5-817f-0bc4907b88d0.png]]


![[Pasted image 20250723022819.png]]

#### VideoMAE 架構流程：

1. **輸入**：一段短影片 (Video Clip)。
    
2. **時空分塊 (Spatiotemporal Cubing)**：
    
    - 將影片切割成一系列 3D 的「立方體塊 (Cube)」。一個 Cube 不僅有空間維度（如 16x16 像素），還有時間維度（如連續 2 幀）。
        
3. **極高比例的隨機遮蔽 (Masking)**：
    
    - 這是 VideoMAE 的靈魂。它會隨機遮蔽掉**極高比例**（例如 90%）的 Cubes。
        
    - 只有一小部分（例如 10%）的 Cubes 是可見的。
        
4. **非對稱處理核心**：
    
    - **A. 強大的編碼器 (Heavy Encoder - 一個 ViT)**：
        
        - **只有那 10% 的可見 Cubes** 會被送入這個基於 ViT 的強大編碼器。
            
        - 編碼器的任務是深入理解這部分可見內容的時空關係，並輸出它們的特徵表示。
            
        - **關鍵點**：由於只處理極少量的數據，這個階段的計算成本被大大降低了。
            
    - **B. 輕量級的解碼器 (Lightweight Decoder)**：
        
        - 這是經典 ViT 所沒有的全新組件。
            
        - 它的輸入包括兩部分：
            
            1. 來自編碼器的、代表可見 Cubes 的特徵。
                
            2. 在所有被遮蔽的位置，放置可學習的「遮蔽標記 (Mask Token)」。
                
        - 解碼器的結構通常比編碼器更淺、更窄，因為它的任務相對簡單：重建像素。
            
        - 它會融合可見部分的上下文資訊，去預測並重建所有被遮蔽的 Cubes 的原始像素值。
            
5. **學習目標 (Loss Function)**：
    
    - 計算模型重建的 Cubes 與原始被遮蔽的 Cubes 之間的均方誤差 (MSE)，並以此作為損失來驅動整個模型的訓練。
        

---

### 第三部分：核心比較：VideoMAE 為處理 Video 多了什麼？

與經典 ViT 相比，VideoMAE 在網路架構上主要增加了以下三個針對影片特性和學習效率的關鍵設計：

#### 1. 輸入單元的升維：從 2D Patch 到 3D Cube

- **ViT**：處理的是靜態圖片，其基本單元是 2D 的 Patch，只包含空間資訊。
    
- **VideoMAE**：處理的是動態影片，其基本單元是 3D 的 Cube。通過將時間維度納入基本處理單元，模型從一開始就能**同時捕捉局部空間外觀和局部時間運動**，這是處理影片的根本前提。
    

#### 2. 架構的革新：從「對稱編碼器」到「非對稱編碼器-解碼器」

- **ViT**：採用單一編碼器處理全部數據，結構對稱。如果直接將此架構用於影片，每幀都提取大量 Cubes，計算成本會高到無法承受。
    
- **VideoMAE**：引入了**解碼器 (Decoder)**，並構建了**非對稱**的框架。這是 VideoMAE **最高效、最核心的架構創新**。
    
    - **為什麼這對影片至關重要？** 影片相鄰幀之間存在巨大的**時序冗餘 (Temporal Redundancy)**。強大的編碼器只需看到影片的吉光片羽（10%），就能推斷出大部分內容。讓輕量級的解碼器去完成「填空」的重建任務，極大地節省了算力，使得在海量影片上進行預訓練成為可能。
        

#### 3. 學習策略的轉變：從「全局理解」到「從殘缺中重建」

- **ViT**：其自注意力機制需要看到全局（所有 Patches）來進行關聯和理解。
    
- **VideoMAE**：通過**極高的遮蔽率 (High Masking Ratio)**，人為地製造了一個極難的重建任務。
    
    - **為什麼這對影片學習有效？** 如果遮蔽率很低，模型可以輕易地從相鄰的時空 Cube 中「抄作業」來重建，學不到深層知識。高達 90% 的遮蔽率強迫模型**必須去學習物體的運動規律、時空的因果關係**，而不是簡單地複製貼上。這使得模型學到的時空表徵更加魯棒和通用。
        

### 總結表格

|特性|經典 Vision Transformer (ViT)|VideoMAE|
|---|---|---|
|**基礎模型**|ViT Encoder|**ViT Encoder**|
|**輸入單元**|2D 圖像塊 (Patch)|**3D 時空立方體 (Cube)**|
|**核心架構**|單一、對稱的**編碼器 (Encoder-only)**|**非對稱**的**編碼器-解碼器 (Encoder-Decoder)**|
|**數據處理量**|處理 100% 的輸入 Patches|編碼器僅處理 **~10%** 的可見 Cubes|
|**新增的關鍵組件**|(無)|**輕量級的解碼器 (Lightweight Decoder)**|
|**學習目標**|監督學習下的**分類**任務|自監督學習下的**像素重建**任務|
|**為影片優化的核心**|(不適用)|1. **3D Cube** 捕捉時空資訊  <br>2. **非對稱架構** 應對時序冗餘  <br>3. **高遮蔽率** 強迫學習運動規律|

總之，VideoMAE 巧妙地保留了 ViT 作為其核心特徵提取引擎，但通過引入「非對稱編碼器-解碼器」架構和「高比例遮蔽」的學習策略，成功地將其改造為一個專為影片設計的、極其高效的自監督學習框架。










###### 例子解說MAE

**MAE 全稱：Masked Autoencoder**

> 一種基於掩碼的自編碼器，用於圖像或視頻的自監督學習。

它是 2021 年由 Facebook AI 提出的圖像自監督學習方法，核心思想是：

- 將輸入圖片隨機遮擋（mask）一部分 patch
    
- 模型只觀察未被遮擋的部分
    
- 學會重建被遮擋的部分
    

這種方式讓模型學會理解圖像的整體結構和內容，而不是簡單地記憶細節。  
類似於 NLP 裡的 **BERT 的 masked language modeling（MLM）**。

---

# 🎨 2️⃣ 什麼是圖片的「掩碼」？

在圖像/視頻任務裡：

> **掩碼 (mask)** 就是對輸入的某些區域做遮蔽/標記，讓模型看不見這些區域。

### 具體來說：

- 一張圖片通常會被劃分成很多個小 patch（例如 16×16 的塊）
    
- 隨機選取一定比例（如 75%）的 patch 遮擋掉
    
- 遮擋的 patch 不提供像素值，而是填充一個特殊的「掩碼標記」
    

例子：

複製編輯

`原圖： ██████████████ ██████████████  掩碼後： ██□□██□□████ □□██□□██□□`

其中 `□` 代表被掩碼的 patch。

---

# 🔷 3️⃣ MAE 的模型架構

MAE 是一種 **自編碼器架構 (autoencoder)**，由兩部分組成：

> **Encoder（編碼器）+ Decoder（解碼器）**

### 🎯 Encoder

- 輸入：**未被掩碼的 patch**
    
- 功能：學習這些可見 patch 的表示
    
- 通常是一個 ViT（Vision Transformer）
    

### 🎯 Decoder

- 輸入：編碼器的輸出 + 掩碼位置
    
- 功能：根據編碼器的輸出和掩碼提示重建整張圖像（特別是被遮擋的 patch）
    
- 通常是一個輕量的 Transformer
    

### 整個過程：

複製編輯

`1️⃣ 輸入圖片 2️⃣ 切分為 patch，隨機遮擋大部分 3️⃣ 編碼器：只看未遮擋的 patch → 得到特徵 4️⃣ 解碼器：根據特徵+掩碼位置 → 重建原始圖片 5️⃣ 計算重建損失（MSE）`



---

# 📹 4️⃣ VideoMAE 是什麼？和 MAE 有什麼不同？

**VideoMAE** 是在 MAE 基礎上擴展到視頻的版本，來自 2022 年的工作。

---

### 🎥 VideoMAE 的動機

視頻不僅有空間信息，還有時間信息。  
VideoMAE 將時間維度也考慮進去，讓模型學會從可見的空間+時間片段推理出被遮擋的時空片段。

---

### VideoMAE 的特點：

✅ 將輸入的視頻切分成 **時空 patch**  
✅ 在空間 + 時間兩個維度上都隨機遮蔽  
✅ Encoder 只看未遮擋的時空 patch  
✅ Decoder 重建完整的視頻 clip

---

### 🎯 MAE 和 VideoMAE 對比

|特徵|MAE|VideoMAE|
|---|---|---|
|適用數據|圖像（2D）|視頻（3D 時空）|
|輸入形式|空間 patch|時間 + 空間 patch|
|掩碼方式|隨機遮擋空間 patch|隨機遮擋時空 patch|
|重建目標|原圖|原視頻 clip|
|挑戰|圖像結構理解|時空動態理解|

---

### 技術細節：

VideoMAE 通常需要更大的掩碼比例（甚至 >90%），因為視頻有更冗餘的信息（時間上的相似性）。

---

# 📝 總結

|模型|核心思想|掩碼|編碼器|解碼器|
|---|---|---|---|---|
|MAE|隨機遮擋圖像 patch，自監督重建|空間掩碼|ViT|Transformer|
|VideoMAE|隨機遮擋時空 patch，自監督重建|時空掩碼|時空 ViT|時空 Transformer|

---

# 🚀 最佳理解

✅ MAE 讓模型學會「看到局部，推測全局」  
✅ 掩碼就是「把部分藏起來，讓模型自己猜」  
✅ VideoMAE 不只學會空間關係，還學會時間關係（動作的連貫性）




# MAE (Masked AutoEncoder for Images)

---
## 🌟 假設

輸入圖片：
- 一張 **224×224 RGB 彩色圖片**
- 圖片內容：一隻貓
---

## 📋 模型每一步的數據流

### 🔷 Step 1️⃣ 輸入圖像

數據：
- shape：**(224, 224, 3)**
- 像素值範圍：[0, 255]

---

### 🔷 Step 2️⃣ 切分成 patch

操作：
- 把圖片切成 **16×16** 大小的小塊
- 總共：22416×22416=14×14=196\frac{224}{16} × \frac{224}{16} = 14×14 = 19616224​×16224​=14×14=196 個 patch
- 每個 patch 展平成一維向量

數據：
- 每個 patch：**(16×16×3 = 768)**
- 全部 patch：**(196, 768)**       196個patch

---

### 🔷 Step 3️⃣ 隨機掩碼

操作
- 隨機選擇 75% 的 patch（147 個）做掩碼
- 僅保留 25%（49 個 patch）

數據：
- 可見的 patch：**(49, 768)**
- 掩碼的位置也被記錄下來（mask vector，形狀是 (196,)）

---

### 🔷 Step 4️⃣ 編碼器

操作
- ViT 編碼器只處理未掩碼的 patch
- 輸出：每個未掩碼 patch 的特徵向量

數據：
- shape：**(49, D)**
    - DDD 是 encoder 的輸出維度，比如 768。


---

### 🔷 Step 5️⃣ 解碼器

操作：
- 把掩碼位置補回來（用一個 <mark style="background: #BBFABBA6;">learnable token</mark>）
- 全部 196 個 patch 按順序排列
- 解碼器重建每個 patch 的像素

數據：
- 重建的 patch：**(196, 768)**
- 經過 linear projection，重建成像素值

### 🔷 Step 6️⃣ 輸出

重建後的圖片
- shape：**(224, 224, 3)**
- 和原圖比較，計算重建損失（通常是 MSE）

### ✨ 小結

| 階段      | 數據形狀          |
| ------- | ------------- |
| 原圖      | (224, 224, 3) |
| 切 patch | (196, 768)    |
| 掩碼後     | (49, 768)     |
| 編碼器輸出   | (49, D)       |
| 解碼器輸出   | (196, 768)    |
| 重建圖     | (224, 224, 3) |


# VideoMAE (Masked AutoEncoder for Videos)

---

## 🌟 假設

輸入視頻：
- 一段 **16 幀的視頻 clip**
- 每幀圖片是 **224×224 RGB**

---

## 📋 模型每一步的數據流

### 🔷 Step 1️⃣ 輸入視頻

數據：
- shape：**(16, 224, 224, 3)**
    - 16 幀，逐幀拼接

---

### 🔷 Step 2️⃣ 切分成時空 patch

操作：
- 在空間上切成 16×16 patch（和 MAE 一樣）
- 在時間上把 16 幀看作時間維度
- 結果是時空 patch

數據：
- patch 總數：16×14×14=3136
- 每個 patch：**(16×16×3 = 768)**
- 全部 patch：**(3136, 768)**

---

### 🔷 Step 3️⃣ 時空掩碼

操作：
- 在時空維度上隨機遮蔽，比如遮擋掉 90% 的 patch
- 只保留 10% 的時空 patch

數據
- 可見的 patch：**(313, 768)**
- 掩碼位置也記錄成一個 mask

---

### 🔷 Step 4️⃣ 編碼器

操作：
- 時空 Transformer 編碼器（或 TimeSformer）
- 處理可見的 patch

數據：
- shape：**(313, D)**

---

### 🔷 Step 5️⃣ 解碼器

操作：
- 補全 3136 個時空 patch
- 重建視頻 clip

數據：
- 重建的 patch：**(3136, 768)**
- 投影回時空圖像

---

### 🔷 Step 6️⃣ 輸出

重建後的視頻：
- shape：**(16, 224, 224, 3)**
- 與原始 clip 比較，計算重建損失

### ✨ 小結

|階段|數據形狀|
|---|---|
|原視頻|(16, 224, 224, 3)|
|時空 patch|(3136, 768)|
|掩碼後|(313, 768)|
|編碼器輸出|(313, D)|
|解碼器輸出|(3136, 768)|
|重建視頻|(16, 224, 224, 3)|

---

# 🔷 MAE 和 VideoMAE 對比

|特徵|MAE|VideoMAE|
|---|---|---|
|輸入|單張圖片|視頻 clip|
|patch 切法|空間 patch|時空 patch|
|掩碼|空間掩碼|時空掩碼|
|編碼器|ViT|時空 Transformer|
|重建目標|原圖|原視頻|
|挑戰|空間理解|時空動態理解|

---

# 🚀 重點總結

✅ MAE 和 VideoMAE 的核心思路一致：**遮住一部分 → 學會推理缺失部分**  
✅ 区别是 VideoMAE 多了一個 **時間維度的掩碼與建模**  
✅ 數據流的主要不同體現在：

- MAE: (196, …) 處理 2D patch
    
- VideoMAE: (3136, …) 處理 3D 時空 patch




 ###### VideoMAE 模型細節

詳細解說 VideoMAE 模型的各個環節，並用例子幫助您理解。

VideoMAE (Video Masked Autoencoders) 是一種為影片理解任務設計的自監督預訓練模型。它的核心思想是「在看的過程中學習」，就像人類一樣，我們不需要看到每一幀畫面的每一個細節，也能腦補出完整的動態過程。VideoMAE 模擬這個過程，故意遮擋影片中的大部分區塊（高達 90%-95%），然後讓模型去「腦補」被遮住的內容。透過這個「完形填空」的任務，模型就能學會影片中的時空特徵，例如物體的樣貌、動作的軌跡等。

下面我們來逐一拆解您提到的問題：

### 1. Encoder 與 Decoder 是否跟經典 Transformer 一樣？

不一樣，這也是 VideoMAE 設計的精髓之一，也就是**非對稱設計 (Asymmetric Design)**。

- **經典 Transformer (例如用於翻譯任務的 T5 或 BART 模型):**
    
    - **Encoder (編碼器):** 通常是一個完整且深層的 Transformer 結構，負責讀取輸入序列（例如一個英文句子），並將其壓縮成一個富含語義資訊的上下文向量 (Context Vector)。
        
    - **Decoder (解碼器):** 同樣是一個完整且深層的 Transformer 結構。它接收 Encoder 輸出的上下文向量，並逐字生成目標序列（例如翻譯後的中文句子）。
        
    - **對稱性:** 在經典設計中，Encoder 和 Decoder 的規模和複雜度通常是**相近的**，兩者都是模型的核心。
        
- **VideoMAE:**
    
    - **Encoder (編碼器):** 這是一個**標準且深層的 Vision Transformer (ViT)**。它的工作是處理**未被遮擋**的少量影片區塊 (patches)，並將它們轉換為**潛在表示 (latent representation)**。因為只處理一小部分 (例如 10%) 的輸入，所以即使它很龐大，計算開銷也是可控的。
        
    - **Decoder (解碼器):** 這是一個**非常輕量級、淺層的** Transformer 結構。它的規模遠小於 Encoder。它的工作是接收來自 Encoder 的少量潛在表示，以及代表所有被遮擋位置的**可學習遮罩權杖 (learnable mask token)**，然後重建出被遮擋區塊的原始像素。
        
    - **非對稱性:** Encoder 龐大但處理的資料少；Decoder 輕量但處理完整序列的權杖。這種設計大幅提升了預訓練的效率。
        

|特性|經典 Transformer (翻譯模型)|VideoMAE|
|---|---|---|
|**結構**|Encoder 和 Decoder 規模相當|Encoder 深而大，Decoder 淺而小|
|**Encoder 輸入**|完整的輸入序列|僅**未被遮擋**的影片區塊 (約 10%)|
|**Decoder 輸入**|Encoder 的完整輸出 + 已生成的目標序列|Encoder 的輸出 + **所有**被遮擋位置的 Mask Token|
|**核心目標**|將一個序列轉換為另一個序列|從少量可見區塊**重建**被遮擋的區塊|

---

### 2. 怎麼做 Cube Embedding (立方體嵌入)？

影片是 3D 資料（時間 `T`、高度 `H`、寬度 `W`）。Cube Embedding 就是將影片切割成不重疊的 3D 小方塊 (Cube/Patch)，並把它們轉換為模型可以處理的 1D 向量。

**步驟如下：**

1. **切割 (Spatiotemporal Tublets):** 首先，將輸入的影片片段（例如 16 幀）在時間和空間維度上都進行切割。假設一個影片片段的大小是 `16 x 224 x 224` (T x H x W)。我們可以設定一個立方體的大小為 `2 x 16 x 16`。這意味著在時間上每 2 幀切一次，在空間上每 `16x16` 像素切一次。
    
2. **展平 (Flatten):** 將每個 `2 x 16 x 16` 的 3D 立方體展平成一個一維向量。這個向量的維度就是 `2 * 16 * 16 * 3`（如果`是` RGB 圖像，所以乘以 3 個顏色通道）= `1536`。
    
3. **線性投射 (Linear Projection):** 這個 `1536` 維的向量太大了，所以會透過一個線性層（全連接層）將其投射到一個較小的、固定維度的向量空間中，例如 `768` 維。這個 `768` 維的向量就是這個 Cube 的**嵌入 (Embedding)**。
    
4. **加入位置資訊 (Positional Embedding):** 因為 Transformer 本身不具備順序概念，我們需要告訴模型每個 Cube 的原始位置。所以會為每個 Cube Embedding 加入**位置嵌入**。在 VideoMAE 中，這包括了**空間 (Spatial)** 和**時間 (Temporal)** 兩個維度的位置資訊，讓模型知道這個 Cube 來自於影片的第幾幀的哪個位置。
    

**例子：** 想像一部樂高電影，你想讓電腦理解一個樂高小人走路的動作。

- **切割：** 你不會一像素一像素地看，而是把小人走路的連續畫面（例如 16 幀）切成很多小方塊。某個方塊可能剛好包含了小人的腿在第 5-6 幀的樣子。
    
- **投射和嵌入：** 你將這個「包含腿的方塊」轉換成一組數字（向量），這組數字就代表了「第 5-6 幀時，位於畫面中央的腿」這個概念。
    

---

### 3. 怎麼做 Encoder Masking (編碼器遮罩)？

這是 VideoMAE 實現高效率的關鍵。遮罩是在 **Cube Embedding 之後、進入 Encoder 之前**進行的。

1. **高比例隨機遮罩 (High-Ratio Random Masking):** 模型會隨機選擇**極高比例**（例如 90%）的 Cube Embeddings 將它們「丟棄」。
    
2. **僅處理可見部分:** **只有剩下的 10% 未被遮擋的 Cube Embeddings 會被送入那個龐大、深層的 Encoder**。
    

**例子：** 假設一部 16 幀的影片被切成了 1000 個 Cubes。

- 模型隨機選擇其中的 900 個 Cubes 進行遮罩。
    
- 那隻有 100 個可見的 Cubes 會被送入 Encoder 進行複雜的特徵提取。
    
- 這就像給你一張拼圖，但只給你其中 10% 的碎片，讓你去想像整張圖的樣貌。Encoder 的工作就是深入理解這 10% 的碎片。
    

---

### 4. 甚麼是 Latent Representation (潛在表示)？

**潛在表示** 是指 Encoder 處理完輸入資料後，所輸出的**壓縮後、高度抽象化的特徵向量**。它不是原始的像素，而是對原始資訊的**深層理解**。

- 在 VideoMAE 中，Encoder 接收了那 10% 的可見 Cubes 後，會進行多層的自註意力計算，捕捉這些 Cubes 之間的時空關係。
    
- Encoder 最終為這 10% 的可見 Cubes 輸出的對應向量，就是它們的**潛在表示**。
    
- 這個表示捕捉了影片的**核心內容**。例如，如果可見的 Cubes 顯示了一隻鳥的頭部和翅膀的一部分，潛在表示就會濃縮「這可能是一隻正在飛翔的鳥」這樣的語義資訊。
    

---

### 5. 甚麼是 Learnable Mask Token (可學習遮罩權杖)？

這是一個特殊的、**共享的**向量。你可以把它想像成一個「空白」的標記，用來**佔據那些被遮罩的位置**。

- **可學習 (Learnable):** 這個向量不是固定的，它的數值會在模型訓練過程中不斷被更新和優化。模型會學習到一個「最好用的空白填充物」。
    
- **共享 (Shared):** 所有被遮罩的位置都使用**同一個** Mask Token。這告訴模型：「這些位置的資訊都缺失了，你需要根據上下文來填充它們」。
    

在 Decoder 階段，輸入序列由兩部分組成：

1. 來自 Encoder 的、代表可見區塊的**潛在表示**。
    
2. 在所有被遮罩的位置上，放上這個**可學習的 Mask Token**。
    

---

### 6. 甚麼是 Reconstructed Pixel (重建像素)？

這是 Decoder 的輸出目標。

- 輕量級的 Decoder 接收了潛在表示和 Mask Tokens 後，它的任務就是**預測出那些被遮罩的 Cube 的原始像素值**。
    
- Decoder 的最後一層通常是一個線性層，它會將每個 Mask Token 的最終輸出向量，重新映射回原始 Cube 的像素維度（例如 `1536` 維）。
    
- 這個預測出來的像素值，就是**重建像素**。
    
- 模型的損失函數（通常是均方誤差 MSE）就是計算**「重建像素」**和**「原始真實像素」**之間的差距，並依此來更新整個模型的權重（包括 Encoder、Decoder 和 Mask Token）。
    

---

### 7. Tube Masking (管道式遮罩)

這是 VideoMAE V2 中引入的改進策略，比 V1 的純隨機遮罩更能捕捉時間關聯性。

- **基本思想:** 與其完全隨機地遮罩 Cubes，不如在時間維度上遮罩掉**一整條「管道 (Tube)」**。也就是說，在影片的**連續幾幀**裡，**相同空間位置**的 Cube 會被**一起遮罩掉**。
    
- **通常橫跨幾個連續 frames:** 這個長度是個超參數，但通常會涵蓋幾個到十幾個連續的幀，形成一個時間上的 "Tube"。例如，可以設定為遮罩連續 4 幀或 8 幀的同一空間區域。
    
- **怎麼串連不同 frame 的同個物體:** 這正是 Tube Masking 的巧妙之處。
    
    - **例子:** 想像一個球從左向右滾動。在第 5 到第 8 幀，球正好滾過了畫面的中央區域。如果我們使用 Tube Masking，把第 5-8 幀中央區域的這個「管道」整個遮住。
        
    - **模型的挑戰:** 模型看不到第 5-8 幀中間球的樣子。為了重建這個被遮住的管道，它必須：
        
        1. 觀察第 1-4 幀，發現一個球**正要進入**這個區域。
            
        2. 觀察第 9-12 幀，發現一個球**剛離開**這個區域。
            
    - **學習結果:** 為了完成這個「填空題」，模型被迫學習到物體的**運動軌跡和一致性**。它必須推斷出：「哦，這個球是以某個速度從左邊滾過來的，所以 5-8 幀的時候它應該在中間這裡，並且外觀應該和前後幀保持一致」。這就強迫模型學會了跨幀串連同一個物體的能力。
        

---

### 8. 非對稱處理核心 (Asymmetric Processing Core)

這是對整個架構的高度概括，也是 VideoMAE 成功的核心理念。

- **核心思想:** 將計算資源**不成比例地**分配給 Encoder 和 Decoder。認識到在遮罩自編碼任務中，**「編碼」和「重建」是兩個難度不同的任務**。
    
    - **編碼 (Encoding):** 從稀疏的可見信號中提取抽象語義，是**困難的**。這需要一個強大、深層的模型 (Heavy Encoder)。
        
    - **重建 (Reconstruction):** 從抽象語義中恢復低層次的像素細節，是**相對簡單的**。這只需要一個輕量、淺層的模型 (Lightweight Decoder)。
        
- **例子來解釋：**
    
    - **情境：** 你需要處理一部影片，它被切成了 2000 個 Cubes。你決定遮罩 90% 的 Cubes。
        
    - **對稱處理 (傳統方法):** 將 2000 個 Cubes（包含大量的 Mask Token）全部送入一個巨大的 Encoder，讓它處理。計算量巨大，因為 Encoder 每一層都要處理 2000 個權杖。
        
    - **非對稱處理 (VideoMAE):**
        
        1. **Encoder 階段:** 只將 `2000 * (1 - 90%) = 200` 個可見 Cubes 送入巨大的 Encoder。Encoder 的計算量只跟這 200 個權杖有關，速度飛快。
            
        2. **Decoder 階段:** 將 Encoder 輸出的 200 個潛在表示，和 `2000 * 90% = 1800` 個輕量的 Mask Token 組合起來，送入一個微小的 Decoder。雖然 Decoder 要處理 2000 個權杖，但因為它本身結構簡單（層數少、維度低），所以計算量也很小。
            
- **結論:** 非對稱設計透過讓**强大的 Encoder 只做最關鍵的工作（理解可見部分）**，並讓**輕量的 Decoder 去做相對簡單的收尾工作（填充細節）**，極大地降低了整體計算成本，使得在海量影片資料上進行高效預訓練成為可能。


###### VideoMAE V1 與 V2 的差異

### VideoMAE vs. VideoMAE V2：主要差異總結

VideoMAE (V1) 成功地證明了「遮罩自編碼 (Masked Autoencoding)」這種方法在影片領域的可行性與高效性。而 VideoMAE V2 的核心目標是**「規模化 (Scaling)」**，也就是如何更有效率地訓練更大、更深的模型，處理更長的影片，從而獲得更強的性能。

V2 並沒有顛覆 V1 的核心架構，而是在**訓練策略**和**效率優化**上做出了關鍵改進。

|特性|VideoMAE (V1)|VideoMAE V2 (主要改進)|
|---|---|---|
|**遮罩策略**|**隨機遮罩 (Random Masking)**：隨機獨立地遮蓋影片立方體(Cubes)。|**管道式遮罩 (Tube Masking)**：在連續幀中，遮蓋相同空間位置的立方體，形成「管道」，迫使模型學習運動軌跡。|
|**訓練效率**|基礎的非對稱架構。|**雙重遮罩 (Dual Masking)**：引入兩種遮罩率，一個用於學習目標（高，如90%），一個用於 Encoder 輸入（中等，如60%），在不犧牲性能的前提下，讓 Encoder 看到更多上下文，同時保持高效。|
|**模型規模化**|主要在標準模型尺寸（如 ViT-Base/Large）上進行驗證。|專為巨大模型（ViT-Giant）和更長的影片（32-64幀）設計了優化方案，使其訓練穩定且高效。|
|**核心架構**|Encoder-Decoder 非對稱架構。|沿用 V1 的非對稱架構，但對其進行了規模化適配和優化。|

---

### 詳細模型結構差異 (Encoder & Decoder Blocks)

您對 Transformer Block 內部結構的理解基本正確，但 VideoMAE 的 Decoder 結構比您描述的要**簡單**。兩者都使用標準的 Transformer Block。

一個標準的 Transformer Block (在 ViT 和 MAE 中普遍使用 **Pre-LayerNorm** 變體) 結構如下： `輸入 -> Layer Normalization -> Multi-Head Attention -> 殘差連接 (Add) -> Layer Normalization -> Feed Forward Network (MLP) -> 殘差連接 (Add) -> 輸出`

**1. Encoder (編碼器):**

- **結構:** 就是一個標準的 Vision Transformer (ViT)。它由 N 個您描述的標準 Transformer Block 堆疊而成。
    
    - `LayerNorm -> Multi-Head Attention -> Add -> LayerNorm -> FFN -> Add`
        
- **Block 數量:** 與標準 ViT 配置相同。
    
    - **ViT-Base:** 12 個 Encoder Blocks
        
    - **ViT-Large:** 24 個 Encoder Blocks
        
    - **ViT-Huge/Giant:** 32 / 40 個 Encoder Blocks (V2 主要在這裡進行了擴展)
        

**2. Decoder (解碼器):**

- **結構:** **這點很重要**，VideoMAE 的 Decoder **不是** 經典翻譯模型那種複雜的 Decoder。它**沒有**您提到的「masked multi-head attention」和「cross-attention」的雙重注意力機制。它就是一個由 M 個**標準 Transformer Block** 組成的**更淺、更窄**的 Transformer。它的工作原理和 Encoder 完全一樣，只是更輕量。
    
    - `LayerNorm -> Multi-Head Attention -> Add -> LayerNorm -> FFN -> Add`
        
- **Block 數量:** 遠少於 Encoder。
    
    - **V1/V2 (ViT-Base/Large 配置):** 通常使用 **4 到 8 個** Decoder Blocks。
        
    - 其隱藏層維度 (hidden dimension) 也通常比 Encoder 小（例如 Encoder 是 768，Decoder 可能是 384 或 512）。
        

---

### 參數設定解說

您的理解是正確的，這裡再做個精確的區分：

1. **Spatiotemporal Tublets (時空立方體) / Clips (片段):**
    
    - **Clip Length:** 這是整個模型的輸入單位，是一個**超參數**，決定一次處理多少幀。例如，你可以設定為 `16` 幀或 `32` 幀。
        
    - **Tublet/Cube Size:** 這是切割 Clip 的基本單位，也是一個**超參數**。例如，`2x16x16` (時間x高度x寬度)。Clip Length 必須能被 Tublet 的時間維度整除。
        
2. **Tube Masking:**
    
    - **Tube Length:** 這是 V2 引入的**專門參數**，決定了在時間上要連續遮蓋多少幀的同一空間位置。例如，設定為 `4`，模型就會在連續 4 幀中遮蓋掉同一個位置的 Cubes。
        

---

### 數據流程詳解 (以 `32 x 224 x 224` 影片為例)

我們假設使用 ViT-Base (Encoder 維度 `D=768`) 和 `2x16x16` 的 Cube 尺寸。

**第一步：輸入與切割 (Input & Patching)**

- **輸入影片:** 1 個 `32 x 224 x 224 x 3` 的影片片段。
    
- **切割成立方體 (Cubes/Tublets):**
    
    - 時間維度: `32 / 2 = 16` 個切片
        
    - 空間維度: `(224 / 16) x (224 / 16) = 14 x 14 = 196` 個切片
        
    - **總立方體數量:** `16 x 196 = 3136` 個。
        
- **展平與投射 (Flatten & Projection):**
    
    - 每個 Cube 大小為 `2x16x16x3 = 1536`。
        
    - 通過一個線性層，將每個 1536 維的向量投射到 `D=768` 維。
        
    - **輸出:** `3136` 個 `768` 維的向量 (Tokens)。
        
- **加入位置嵌入 (Positional Embedding):** 為這 `3136` 個 tokens 加入時空位置資訊。
    

接下來，我們看 V1 和 V2 在此之後的流程差異。

---

### Video MAE (V1) 數據流程

**第二步：隨機遮罩 (Random Masking)**

- 設定一個極高的遮罩率，例如 **90%**。
    
- 從 `3136` 個 tokens 中，隨機選擇 `3136 * 90% ≈ 2822` 個進行遮罩。
    
- 剩下的 `3136 - 2822 = 314` 個 tokens 作為**可見 tokens**。
    

**第三步：Encoder 處理**

- **只有這 `314` 個可見 tokens** 被送入深層的 Encoder (例如 12 個 Block)。
    
- Encoder 進行複雜的自註意力計算，捕捉這些稀疏的可見塊之間的時空關係。
    
- **輸出:** `314` 個 `768` 維的**潛在表示 (Latent Representations)**。
    

**第四步：Decoder 輸入準備**

- 將 Encoder 輸出的 `314` 個潛在表示，與 `2822` 個**共享的可學習 Mask Tokens** 組合起來。
    
- 這些 tokens 會被放回它們在 `3136` 個位置中的**原始順序**。
    
- **輸出:** 一個完整的、長度為 `3136` 的序列。
    

**第五步：Decoder 重建**

- 這個完整的 `3136` 長度的序列被送入輕量級的 Decoder (例如 4 個 Block)。
    
- Decoder 利用可見 tokens 的上下文資訊，來預測被遮罩位置的內容。
    
- **輸出:** `3136` 個 `768` 維的重建後向量。
    

**第六步：像素重建與計算損失**

- 一個線性「重建頭」(Reconstruction Head) 將 Decoder 輸出的 `2822` 個對應被遮罩位置的向量，從 `768` 維重新投射回像素空間 `1536` 維。
    
- 計算這 `2822` 個重建後的 Cube 像素與原始像素之間的均方誤差 (MSE) 作為損失 (Loss)。
    
- **只對被遮罩的部分計算損失**。
    

---

### Video MAE v2 數據流程 (與 V1 的不同點)

**第二步 (不同點)：管道式遮罩 (Tube Masking) 與雙重遮罩**

- **Tube Masking:** 遮罩不再是完全隨機的。假設 Tube Length 為 4 幀（對應 `4/2=2` 個時間切片）。當模型決定遮罩空間位置 `(x=5, y=8)` 時，它會將 `(t=0, x=5, y=8)` 和 `(t=1, x=5, y=8)` 這兩個連續時間的 Cube **一起遮罩**。這會形成一條長長的「管道」被挖空。
    
- **雙重遮罩 (Dual Masking) - V2 的效率核心:**
    
    1. **學習目標遮罩率 (高):** 首先，定義一個高的遮罩率，比如 **90%**。這意味著我們的**最終目標**是重建這 `2822` 個被遮罩的 tokens。**損失函數會根據這 90% 來計算**。
        
    2. **Encoder 可見遮罩率 (中等):** 接著，從剩下 `10%` (`314` 個) 的可見 tokens 中，**再隨機丟棄一部分**。例如，再丟棄 50%。這意味著 Encoder **只會看到 `314 * 50% = 157` 個 tokens**！
        
- 這個策略的優點是：Encoder 的計算量變得更小，極其高效。同時，由於最終的重建目標仍然是 90% 的 tokens，模型被迫從更稀疏的線索中學習，增強了其表徵能力。
    

**第三步 (不同點)：Encoder 處理**

- **只有極少量的 `157` 個 tokens** 被送入 Encoder。計算效率比 V1 更高。
    
- **輸出:** `157` 個 `768` 維的潛在表示。
    

**第四步、第五步、第六步 (流程類似 V1，但輸入有變化):**

- Decoder 的輸入由 **`157` 個潛在表示** 和 `3136 - 157 = 2979` 個 Mask Tokens 組成 (注意，這裡的 mask tokens 數量更多了)。
    
- Decoder 仍然需要重建那**原始目標的 90%** (即 `2822` 個) 被遮罩的 tokens。
    
- 損失函數的計算方式不變，依然是比較 `2822` 個重建像素與原始像素的差異。
    

總結來說，V2 的數據流在遮罩階段引入了更聰明的 **Tube Masking** 來學習運動，並用 **Dual Masking** 策略讓 Encoder 的工作量變得極小，從而實現了以更低成本訓練更大模型和更長影片的目標。