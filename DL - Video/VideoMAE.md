
## VideoMAE 模型全解析：從核心架構到與 ViT 及 VideoMAE V2 的差異比較

在影片理解領域，VideoMAE (Video Masked Autoencoders) 作為一種高效的自監督學習框架，近年來備受矚目。它巧妙地將遮蔽自動編碼器（Masked Autoencoder, MAE）的思想從圖像領域擴展到影片，極大地提升了模型在影片數據上的學習效率和性能。本文將為您詳細拆解 VideoMAE 的模型架構，闡明其與經典的 Vision Transformer (ViT) 的核心差異，解析其處理影片的獨特之處，並比較其與後續版本 VideoMAE V2 的演進。

### 一、 VideoMAE 的核心模型架構

VideoMAE 的核心思想是「從殘缺中學習完整」。它通過對輸入影片進行大規模的隨機遮蔽，然後訓練模型去重建這些被遮蔽的內容，從而讓模型學習到影片的時空表徵。其整體架構採用了一種**非對稱的編碼器-解碼器（Asymmetric Encoder-Decoder）**設計，這是其高效性的關鍵所在。

#### 1. 影片分塊與遮蔽 (Patching & Masking)

- **時空立方體 (Spatiotemporal Cubes)**：與 ViT 將圖像分割成二維的 Patch（圖像塊）不同，VideoMAE 將影片分割成三維的「Cube」（立方體塊）。一個 Cube 既包含空間資訊（類似圖像的 Patch），也包含了時間維度的連續幾幀。
    
- **極高的遮蔽率 (Extremely High Masking Ratio)**：VideoMAE 的一大特點是採用了非常高的遮蔽率，通常達到 90% 甚至 95%。這意味著模型僅能看到 5%-10% 的原始影片內容。這樣做的原因有二：
    
    1. **應對影片的高度冗餘**：影片的相鄰幀之間通常變化很小，資訊冗餘度高。高遮蔽率強迫模型不能僅僅依賴相鄰可見的 Cube 來簡單「複製貼上」，而是必須學習更深層次的時空語義來進行重建。
        
    2. **提升預訓練效率**：由於只有少量未被遮蔽的 Cube 需要送入編碼器，極大地降低了計算複雜度和記憶體消耗。
        

#### 2. 非對稱的編碼器-解碼器架構

- **強大的編碼器 (Heavy Encoder)**：只有未被遮蔽的少量（例如 10%）影片 Cube 會被送入一個標準且深層的 Transformer 編碼器（通常是 ViT）。編碼器的任務是學習這些可見 Cube 的深度特徵表示。
    
- **輕量級的解碼器 (Lightweight Decoder)**：解碼器的任務是重建被遮蔽的 Cube。它的輸入包括：
    
    1. 編碼器輸出的可見 Cube 的特徵。
        
    
    - 遮蔽位置的「遮蔽標記 (Mask Token)」。 解碼器通常比編碼器更淺、更窄，因為重建像素是一個相對底層的任務，不需要像編碼語義那樣複雜的模型。這種非對稱設計確保了預訓練過程的整體高效性。
        

#### 3. 重建目標 (Reconstruction Target)

VideoMAE 的重建目標是預測被遮蔽的 Cube 的原始像素值。通過最小化重建像素與原始像素之間的均方誤差（MSE Loss），模型被驅動去學習影片的內在結構和運動模式。

### 二、 VideoMAE 與經典 ViT 的核心差別

|特性|經典 Vision Transformer (ViT)|VideoMAE|
|---|---|---|
|**輸入數據**|2D 圖像 (Image)|3D 影片 (Video)|
|**基本單元**|圖像塊 (Image Patches)|時空立方體 (Spatiotemporal Cubes)|
|**模型架構**|單一的 Transformer 編碼器|**非對稱的編碼器-解碼器**|
|**訓練方式**|主要在標註數據上進行監督學習（如分類）|**自監督學習**（遮蔽與重建）|
|**數據處理**|處理圖像中所有的 Patches|**僅處理少量可見的 Cubes** (Encoder)，極大提升效率|
|**核心任務**|圖像分類|學習通用的影片時空表徵|

**總結來説，最大的差別在於：**

1. **架構不同**：ViT 是一個單一的、對稱的編碼器結構，用於對輸入的所有資訊進行編碼並最終用於分類。而 VideoMAE 採用了非對稱的「編碼器-解碼器」結構，其設計初衷是為了高效地進行自監督預訓練。
    
2. **學習範式不同**：ViT 的經典用法是監督學習，需要大量的標註數據。VideoMAE 則是自監督學習的典範，它能從海量的無標註影片中學習知識，這使得它在數據利用上更具擴展性。
    

### 三、 VideoMAE 如何處理影片？

VideoMAE 能夠成功處理影片，關鍵在於其針對影片特性的幾項專門設計：

1. **時空聯合嵌入 (Joint Space-Time Embedding)**：將影片視為一個 3D 的數據體，並從中提取 3D 的 Cube，這使得模型能夠在第一時間就同時捕捉局部空間和時間上的關聯性。
    
2. **Tube Masking 策略**：為了進一步增加任務難度，避免資訊洩漏，VideoMAE 常常採用「Tube Masking」。這種策略下，如果在某個時間點的一個空間位置被遮蔽，那麼在整個影片片段的這個空間位置都會被遮蔽。這迫使模型必須理解物體的運動和變化，而不是簡單地從前後幀複製像素。
    
3. **應對時間冗餘**：通過極高的遮蔽率，模型被迫忽略影片中的大量冗餘資訊，專注於學習更本質的時空模式。這使得學習到的特徵更具魯棒性。
    

### 四、 VideoMAE vs. VideoMAE V2：青出於藍

VideoMAE V2 是在第一代基礎上的重要升級，旨在解決模型擴展性（Scaling）和訓練效率的問題，目標是訓練更大規模的影片基礎模型。其主要改進點如下：

|特性|VideoMAE|VideoMAE V2|
|---|---|---|
|**模型規模**|主要在 ViT-Base/Large 上進行實驗|**可擴展至數十億參數的巨型模型** (ViT-g)|
|**遮蔽策略**|僅對編碼器輸入進行遮蔽|**雙重遮蔽 (Dual Masking)**：同時對編碼器和解碼器的輸入進行遮蔽|
|**訓練流程**|單階段的預訓練+微調|**漸進式訓練 (Progressive Training)**：引入中間階段的微調，更好地適應下游任務|
|**訓練效率**|已經很高效|**進一步提升**，雙重遮蔽顯著降低了總體計算成本和記憶體佔用|

**核心差異詳解：**

1. **雙重遮蔽 (Dual Masking)**：這是 V2 最核心的創新。在 V1 中，解碼器需要處理所有被遮蔽的 Token，當遮蔽率高達 90% 時，這依然是不小的計算量。V2 提出，解碼器也無需重建所有被遮蔽的 Cube，可以再進行一次遮蔽，只重建其中的一部分。這相當於給編碼器和解碼器分別設置了「考題」，進一步壓縮了計算量，使得訓練數十億參數的超大模型成為可能。
    
2. **模型擴展與漸進式訓練**：V2 成功地將 VideoMAE 的架構擴展到了 ViT-g (Giant) 級別的龐大模型。同時，為了更好地釋放這些大模型的潛力，V2 採用了漸進式的訓練策略。例如，先在海量的、多樣化的無標註影片數據上進行預訓練，然後在帶有標籤的混合數據集上進行第二階段的「後預訓練 (post-pre-training)」或中間微調，最後才在特定的下游任務上進行微調。這種方法有助於模型更好地從通用知識過渡到特定任務。
    

**總結而言，如果說 VideoMAE (V1) 的貢獻是提出了一種極其高效的影片自監督學習框架，那麼 VideoMAE V2 的貢獻則是成功地將這一框架擴展到了工業級的、超大規模的基礎模型層面，並通過雙重遮蔽等技術將效率推向了新的高度。**

VideoMAE v2：训练数十亿参数的视频模型！ - 越洋飞机的文章 - 知乎
https://zhuanlan.zhihu.com/p/2906234248

