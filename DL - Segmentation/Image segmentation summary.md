
[[###R-CNN系列模型性能列表]]

| 模型                                     | 網路架構設計                                                                                     | Backbone           | Neck                | Head                                                         |
| :------------------------------------- | :----------------------------------------------------------------------------------------- | :----------------- | :------------------ | :----------------------------------------------------------- |
| [[R-CNN]]                              | Selective Search生成<br>候選區域<br>對每個區域提取CNN特徵<br>SVM分類和回歸。                                    | AlexNet<br>VGG16   |                     | 分類分支：FC+ Softmax<br>回歸分支：FC + 邊框回歸                           |
| Fast R-CNN                             | 整張影像CNN特徵提取<br>RoI Pooling層提取<br>候選區域特徵<br>全連接層和分類                                         | VGG16 or<br>ResNet | RoI Pooling         | 分類分支：FC+ Softmax<br>回歸分支：FC + 邊框回歸                           |
| Faster R-CNN                           | 引入RPN生成候選區域 <br>RoI Pooling層提取特徵<br>全連接層和分類器。                                              | ResNet             | RPN + <br>RoI Align | 分類分支：FC+ Softmax<br>回歸分支：FC + 邊框回歸                           |
| [[Mask R-CNN]]                         | Faster R-CNN基礎上增加<br>分割掩碼分支 <br>FCN生成分割掩碼                                                  | ResNet + <br>FPN   | RPN + <br>RoI Align | 分類分支: FCN + Softmax<br>回歸分支: FCN + 邊框回歸<br>分割分支: FCN         |
| [[DL - RCNN/Centermask2\|Centermask2]] | Backbone model: VoVNet<br>Neck model: FPN<br><br>Head model: <br>FCOS box head<br>SAG-Mask | VoVNet + <br>FPN   |                     | 分類分支: FCOS box head<br>回歸分支: FCOS box head<br>分割分支: SAG-Mask |
<1> [[FCN]](Fully convolutional Networks)是語義分割model(FCN等於經典CNN但將最後FN layer改成FCN layer). 而[[FCOS]]（Fully Convolutional One-Stage Object Detection）是object detection model

<2>FCN 是語義分割的開創性架構，它將 CNN 的全連接層FC（Fully Connected layers）替換成卷積層，實現端到端的 pixel-wise 預測。

<3> ResNet50 參考 [[CNN architecture]], 有4個Residual block殘差塊, 有Skip connections(跳躍連接)

<4>-U-Net 的設計靈感來自 FCN，並在 FCN 的基礎上進行了重要的修改，使其更適合生物醫學圖像分割等需要精確定位的應用。因此，從某種意義上說，U-Net 可以被視為 FCN 的一種特殊形式。

<5> DETR 是第一個應用Transformer到object detection的Model. 流程是1. 用CNN(ResNet)生成feature images, 再經過經典Encoder, Decoder, 然後接上head model輸出bounding box跟類別

|                 |     |
| --------------- | --- |
| [[### QA-list]] |     |

### 總結比較表

|模型|分類損失|回歸損失|掩碼損失|為什麼選擇這些損失函數|
|---|---|---|---|---|
|RCNN|Hinge Loss (SVM)|L2 Loss|無|SVM 適合二分類，L2 簡單適用於回歸|
|Fast RCNN|Cross-Entropy|Smooth L1|無|Cross-Entropy 適合多類，Smooth L1 穩健性好|
|Faster RCNN|Cross-Entropy (RPN + Head)|Smooth L1 (RPN + Head)|無|二階段設計，損失適配區域生成和檢測|
|Mask RCNN|Cross-Entropy (RPN + Head)|Smooth L1 (RPN + Head)|Binary Cross-Entropy|添加掩碼分支，二值交叉熵適配逐像素分割|

#### 損失函數演進

- **RCNN**: 分階段訓練，損失獨立設計。
- **Fast RCNN**: 端到端訓練，引入 Smooth L1 提升穩健性。
- **Faster RCNN**: 增加 RPN，損失分為兩階段。
- **Mask RCNN**: 引入掩碼損失，支持實例分割。

這些損失函數的選擇反映了模型任務的演進和計算穩



[[###UNet系列模型性能列表]]

| 模型        | 特點            | 作用         | 輸入  | 輸出    | Index(大致數值)                                                 |
| :-------- | :------------ | :--------- | :-- | :---- | :---------------------------------------------------------- |
| [[U-Net]] | U形架構，跳躍連接。    | 影像分割。      | 影像。 | 分割掩碼。 | * Params: 中 * FLOPs: 中 * Latency: 中 * Throughput: 中         |
| U-Net++   | 嵌套跳躍連接, 彈性深度。 | 影像分割，高準確性。 | 影像。 | 分割遮罩。 | * Params: 較高。 * FLOPs: 較高。 * Latency: 較高。 * Throughput: 較低。 |
- U-Net 是 FCN 的一個重要的變體。U-Net 通過引入對稱的 U 形架構和大量的跳躍連接，顯著提高了分割的精確度。
- U-Net 的主要創新在於其對稱的 U 形架構和大量的跳躍連接(skip connections)。
- 這些跳躍連接將下採樣路徑中的特徵圖直接連接到上採樣路徑中的相應層，使得模型能夠融合來自不同層級的特徵，從而提高分割的精確度。


[[###Transformer系列模型性能列表]]

| 模型              | 特點                     | 作用      | 輸入     | 輸出    | Index(大致數值)                                          |
| :-------------- | :--------------------- | :------ | :----- | :---- | :--------------------------------------------------- |
| [[SAM - SAM]]   | 提示工程，通用分割。             | 通用影像分割。 | 影像、提示。 | 分割掩碼。 | * Params: 高 * FLOPs: 高 * Latency: 中高 * Throughput: 中 |
| ViT 基礎分割模型      | 長距離依賴，Transformer 解碼器。 | 語意分割。   | 影像。    | 分割掩碼。 | * Params: 高 * FLOPs: 高 * Latency: 中高 * Throughput: 中 |
| [[DETR]] 變形分割模型 | Querries詢問pixel類別。     | 實例分割。   | 影像。    | 分割遮罩。 | *Params: 高. *FLOPs：高。 *Latency：中。 *Throughput：中。     |

[[###其他系列模型性能列表]]

| 模型      | 特點         | 作用    | 輸入  | 輸出    | Index(大致數值)                                            |
| :------ | :--------- | :---- | :-- | :---- | :----------------------------------------------------- |
| DeepLab | 空洞卷積，ASPP。 | 語意分割。 | 影像。 | 分割掩碼。 | * Params: 高 * FLOPs: 高 * Latency: 中高 * Throughput: 中   |
| [[FCN]] | 全卷積網路。     | 語意分割。 | 影像。 | 分割掩碼。 | * Params: 中 * FLOPs: 中 * Latency: 中 * Throughput: 中    |
| SegNet  | 保留邊界細節     | 語意分割  | 影像  | 分割遮罩  | *Params: 中等. *FLOPs: 中等. *Latency: 中等. *Throughput: 中等 |


**模型功能與 Head 設計對比表**

| 模型<br>名稱            | 主要設計任務           | 物件<br>偵測     | 實例<br>分割     | 語義<br>分割     | 主要特點                                                                                                                                          |
| :------------------ | :--------------- | :----------- | :----------- | :----------- | :-------------------------------------------------------------------------------------------------------------------------------------------- |
| **RCNN**            | 物件偵測             | **是**        | 否            | 否            | **兩階段方法**：1. 區域提議 (Selective Search) 2. CNN 特徵提取 3. SVM 分類 + 邊界框回歸。速度慢，訓練複雜。                                                                  |
| **Fast <br>RCNN**   | 物件偵測             | **是**        | 否            | 否            | **改進 R-CNN**：共享卷積計算，引入 RoI Pooling。速度比 R-CNN 快，仍依賴外部區域提議算法。                                                                                   |
| **Faster <br>RCNN** | 物件偵測             | **是**        | 否            | 否            | **端到端的兩階段方法**：引入 RPN (Region Proposal Network) 生成區域提議，與檢測網絡共享卷積特徵，顯著提升速度和準確性。物件偵測的里程碑。                                                        |
| **Mask <br>RCNN**   | **實例分割**<br>物件偵測 | **是**        | **是**        | **是**        | **擴展 Faster R-CNN**：在物件偵測的基礎上，為每個偵測到的物件實例生成一個像素級的分割掩碼 (mask)。實例分割的標竿模型。                                                                       |
| **FCN**             | **語義分割**         | 否            | 否            | **是**        | **端到端的像素級語義分割開創者**：將傳統 CNN 的全連接層替換為卷積層，使其能夠接受任意尺寸輸入並輸出對應尺寸的分割圖。引入跳躍連接 (skip connections) 融合多尺度特徵。                                             |
| **U-Net**           | **語義分割**         | 否            | 否            | **是**        | **專為醫學影像分割設計**：U形對稱結構，編碼器逐步下採樣提取上下文特徵，解碼器逐步上採樣並結合編碼器對應層的特徵（通過跳躍連接）以恢復空間解析度和細節，實現精確分割。對小數據集表現較好。                                               |
| **DETR**            | 物件偵測 <br>(也可擴展)  | **是**        | **可擴展**      | **可擴展**      | **端到端的物件偵測新範式**：將物件偵測視為集合預測問題，使用 Transformer 架構，移除了許多手工設計的組件 (如 Anchor, NMS)。物件查詢 (object queries) 與圖像特徵交互後直接輸出結果。訓練時間長，對小物件偵測有挑戰，但啟發了許多後續工作。 |
| Center<br>Mask2     | 實例分割             | **是**        | **是**        | **是**        | 高質量的實例分割模型，通常基於 VoVNetV2 等高效骨幹網路。結合了 anchor-free 物件偵測和高質量的 Mask 預測。                                                                           |
| SAM                 | 可提示<br>的分割       | **否**        | **是**        | **否**        | **基礎模型**，專為零樣本泛化設計，可分割任何圖像中的任何物件。通過提示與模型交互。包含強大的圖像編碼器、提示編碼器和輕量級的 Mask 解碼器。改變了分割任務的交互方式。                                                       |
| SAM2                | 交互式<br>分割        | **否**        | **是**        | **否**        | **推測性/生態發展**：可能包括更高效的編碼器/解碼器、更好的提示融合機制、對多輪交互的優化、對文本提示更細膩的理解、或在特定領域（如醫學、遙感）的微調版本。重點仍在於交互性和泛化性。                                                 |
| DINOv2              | 自監督視覺特徵學習        | Back<br>bone | Back<br>bone | Back<br>bone | **強大的通用視覺表徵學習模型**。通過自監督學習（如圖像級的自蒸餾和 Patch 級的判別任務）訓練出高性能的 ViT 骨幹網路。其特徵具有優異的泛化能力，在多種下游視覺任務（包括檢測、分割、分類、深度估計等）上僅需少量微調即可取得 SOTA 或接近 SOTA 的性能。      |



影像分割是電腦視覺中一項關鍵任務，旨在將影像分割成多個具有特定意義的區域。以下是幾種常用的影像分割類別，以及與其相關的主流基於人工智慧的影像分割模型：

**影像分割的常見類別：**

- **語意分割（Semantic Segmentation）**
    - 目標：將影像中的每個像素分配給一個類別標籤。
    - 重點：區分「是什麼」，不區分「幾個」。例如，將影像中的所有汽車像素標記為「汽車」，無論有多少輛汽車。
    - 主流模型：
        - **U-Net：** 廣泛應用於醫學影像分割，以其編碼器-解碼器結構著稱。
        - **DeepLab：** Google開發的模型系列，利用空洞卷積（Atrous Convolution）有效捕捉多尺度上下文資訊。
        - **FCN (Fully Convolutional Networks):** 最早的end to end語意分割模型，將傳統的CNN後面的全連接層替換成卷積層，從而得到像素級别的輸出。
- **實例分割（Instance Segmentation）**
    - 目標：不僅標記每個像素的類別，還區分每個物件的獨立實例。
    - 重點：區分「是什麼」和「幾個」。例如，標記影像中的每輛汽車，並將它們作為單獨的實例進行區分。
    - 主流模型：
        - **Mask R-CNN：** 在目標檢測模型Faster R-CNN的基礎上添加了Mask分支，用於預測像素級別的分割掩碼。
        - **YOLACT：** 實時實例分割方法，通過並行生成掩碼原型和係數來提高效率。
- **全景分割（Panoptic Segmentation）**
    - 目標：統一語意分割和實例分割，對影像中的所有像素進行分割，包括背景和前景物件。
    - 重點：完整的場景理解，將影像中的每個像素分類為事物（things，可數物件）或場景（stuff，不可數背景）。
    - 主流模型：
        - **Panoptic FPN：** 基於特徵金字塔網絡（FPN）的架構，用於全景分割。
        - **EfficientPS**：強調效率的全景分割模型。

**技術重點：**

- 這些基於人工智慧的模型通常依賴於深度學習，特別是卷積神經網路（CNN）。
- 它們利用大量標記資料進行訓練，以學習影像中不同物件和區域的特徵。
- 近年來的研究趨勢集中在提高分割的準確性、效率和魯棒性。


### R-CNN系列模型性能列表

以下是對 RCNN、Fast RCNN、Faster RCNN 和 Mask RCNN 的詳細整理，包括它們的 **Backbone**、**Neck Model**、**Head Model**，以及模型的輸入輸出格式和數值的說明。我會盡量清晰地分解每個模型的結構，並提供相關細節。

---

### 1. RCNN (Region-based Convolutional Neural Network)

#### 概述

RCNN 是第一個將深度學習應用於目標檢測的模型，通過 Selective Search 生成候選區域 (Region Proposals)，然後對每個區域進行分類和邊框回歸。

#### 結構分解

1. **Backbone**
    - **架構**: 通常使用預訓練的 CNN，如 AlexNet 或 VGG16。
    - **作用**: 提取圖像特徵，用於後續的區域分類。
    - **細節**: VGG16 包含 13 個卷積層和 3 個全連接層，輸出固定大小的特徵向量 (e.g., 4096 維)。
2. **Neck Model**
    - **架構**: 無明確的 Neck 結構。
    - **作用**: RCNN 不使用獨立的 Neck，而是直接將 Backbone 輸出的特徵向量送入後續處理。
    - **細節**: Selective Search 生成約 2000 個候選區域，每個區域被獨立裁剪並縮放到固定大小 (e.g., 224x224) 後，輸入 Backbone。
3. **Head Model**
    - **架構**: 全連接層 (FC) + 兩個分支：
        - 分類分支：Softmax 用於目標分類。
        - 回歸分支：線性回歸用於邊框修正 (Bounding Box Regression)。
    - **作用**: 對每個候選區域進行目標分類和邊框微調。
    - **細節**: 輸出包括類別概率 (e.g., 21 類，包括背景) 和 4 個邊框坐標偏移量 (x, y, w, h)。

#### 輸入輸出格式

- **輸入**: 整張圖像 (任意大小)，通過 Selective Search 生成約 2000 個候選區域，每個區域縮放到 224x224x3。
- **輸出**: 每個候選區域的類別概率 (e.g., [N, 21]) 和邊框坐標偏移量 (e.g., [N, 4])，N 為候選區域數量。
- **數值範圍**: 類別概率為 [0, 1]，邊框偏移量為實數。

#### 特點

- 每個區域獨立處理，計算效率低。
- 無端到端訓練，需分階段訓練 (CNN 特徵提取、SVM 分類、邊框回歸)。

---

### 2. Fast RCNN

#### 概述

Fast RCNN 改進了 RCNN 的效率，將整張圖像輸入 Backbone，然後通過 RoI Pooling 提取候選區域特徵，實現了部分端到端訓練。

#### 結構分解

1. **Backbone**
    - **架構**: 預訓練 CNN，如 VGG16 或 ResNet-50。
    - **作用**: 提取整張圖像的特徵圖 (Feature Map)。
    - **細節**: 例如 VGG16 輸出特徵圖大小為 H/16 x W/16 x 512 (下採樣 16 倍)。
2. **Neck Model**
    - **架構**: RoI Pooling。
    - **作用**: 從特徵圖中提取固定大小的特徵 (e.g., 7x7x512)，對應 Selective Search 生成的候選區域。
    - **細節**: 將變量大小的 RoI (Region of Interest) 映射到固定大小的特徵表示。
3. **Head Model**
    - **架構**: 全連接層 + 兩個分支：
        - 分類分支：Softmax。
        - 回歸分支：邊框回歸。
    - **作用**: 對每個 RoI 進行分類和邊框調整。
    - **細節**: 全連接層將 7x7x512 的特徵展平為 4096 維向量，輸出類別概率和邊框偏移量。

#### 輸入輸出格式

- **輸入**: 整張圖像 (e.g., 600x800x3)，外加 Selective Search 生成的候選區域坐標 (e.g., [N, 4])。
- **輸出**: 每個 RoI 的類別概率 (e.g., [N, C]) 和邊框偏移量 (e.g., [N, 4])，C 為類別數。
- **數值範圍**: 類別概率為 [0, 1]，邊框偏移量為實數。

#### 特點

- 通過共享特徵圖計算，提升效率。
- RoI Pooling 引入量化誤差，影響精度。

---

### 3. Faster RCNN

#### 概述

Faster RCNN 引入了 RPN (Region Proposal Network)，取代 Selective Search，實現了完全端到端的目標檢測。

#### 結構分解

1. **Backbone**
    - **架構**: 預訓練 CNN，如 ResNet-50 或 ResNet-101。
    - **作用**: 提取特徵圖。
    - **細節**: ResNet-50 輸出特徵圖大小為 H/16 x W/16 x 1024 (假設使用 FPN 時可能不同)。
2. **Neck Model**
    - **架構**: RPN + RoI Pooling。
        - **RPN**: 一個小型卷積網絡，生成候選區域及其目標性得分 (Objectness Score)。
        - **RoI Pooling**: 將 RPN 生成的候選區域映射為固定大小特徵。
    - **作用**: RPN 負責生成候選區域，RoI Pooling 負責特徵提取。
    - **細節**: RPN 使用 Anchor 機制，輸出候選區域坐標 (e.g., [N, 4]) 和得分 (e.g., [N, 2])。
3. **Head Model**
    - **架構**: 全連接層 + 兩個分支：
        - 分類分支：Softmax。
        - 回歸分支：邊框回歸。
    - **作用**: 對 RPN 生成的 RoI 進行分類和邊框微調。
    - **細節**: 輸出類別概率 (e.g., [N, C]) 和邊框偏移量 (e.g., [N, 4])。

#### 輸入輸出格式

- **輸入**: 整張圖像 (e.g., 600x800x3)。
- **輸出**: 檢測結果，包括類別概率 (e.g., [M, C]) 和邊框坐標 (e.g., [M, 4])，M 為最終檢測框數量 (經 NMS 過濾)。
- **數值範圍**: 類別概率為 [0, 1]，邊框坐標為像素值或相對值。

#### 特點

- RPN 使模型更快、更準確。
- 仍然使用 RoI Pooling，可能有量化誤差。

---

### 4. Mask RCNN

#### 概述

Mask RCNN 在 Faster RCNN 的基礎上增加了分割分支，用於實例分割 (Instance Segmentation)，並用 RoI Align 替代 RoI Pooling。

#### 結構分解

1. **Backbone**
    - **架構**: ResNet-50/101 + FPN (Feature Pyramid Network)。
    - **作用**: 提取多尺度特徵圖。
    - **細節**: FPN 輸出多層特徵圖 (e.g., P2: H/4 x W/4 x 256, P3: H/8 x W/8 x 256 等)。
2. **Neck Model**
    - **架構**: RPN + RoI Align。
        - **RPN**: 生成候選區域。
        - **RoI Align**: 改進的 RoI Pooling，避免量化誤差，輸出固定大小特徵 (e.g., 7x7x256 或 14x14x256)。
    - **作用**: 生成精確的特徵表示。
    - **細節**: RoI Align 使用雙線性插值，保留空間信息。
3. **Head Model**
    - **架構**: 三個分支：
        - 分類分支：全連接層 + Softmax。
        - 回歸分支：全連接層 + 邊框回歸。
        - 分割分支：FCN (Fully Convolutional Network)，生成像素級掩碼。
    - **作用**: 同時進行分類、邊框回歸和實例分割。
    - **細節**: 分類輸出 [N, C]，邊框輸出 [N, 4]，掩碼輸出 [N, K, M, M] (K 為類別數，M 為掩碼大小，如 28x28)。

#### 輸入輸出格式

- **輸入**: 整張圖像 (e.g., 1024x1024x3)。
- **輸出**: 檢測結果，包括：
    - 類別概率 (e.g., [M, C])。
    - 邊框坐標 (e.g., [M, 4])。
    - 掩碼 (e.g., [M, 28, 28])，每個檢測框一個二值掩碼。
- **數值範圍**: 類別概率為 [0, 1]，邊框為實數，掩碼為 [0, 1] (經 sigmoid 或 threshold)。

#### 特點

- RoI Align 提升精度。
- 增加掩碼分支，實現實例分割。

---

### 總結比較表

|模型|Backbone|Neck Model|Head Model|輸入格式|輸出格式|
|---|---|---|---|---|---|
|RCNN|VGG16|無 (Selective Search)|FC + 分類/回歸|圖像 + 區域 (224x224x3)|[N, C], [N, 4]|
|Fast RCNN|VGG16/ResNet|RoI Pooling|FC + 分類/回歸|圖像 + 區域坐標|[N, C], [N, 4]|
|Faster RCNN|ResNet + FPN|RPN + RoI Pooling|FC + 分類/回歸|圖像|[M, C], [M, 4]|
|Mask RCNN|ResNet + FPN|RPN + RoI Align|FC + 分類/回歸 + FCN 掩碼|圖像|[M, C], [M, 4], [M, 28, 28]|

#### 說明

- **C**: 類別數 (包括背景)。
- **N**: 候選區域數量。
- **M**: 最終檢測框數量 (經 NMS)。
- **進展**: 從 RCNN 到 Mask RCNN，效率和功能逐步提升，Backbone 從簡單 CNN 進化到 ResNet+FPN，Neck 從無到 RPN+RoI Align，Head 增加了掩碼預測。


### UNet系列模型性能列表

U-Net 系列模型在影像分割領域中扮演著重要的角色，特別是在生物醫學影像分割方面表現出色。以下我將詳細解釋 U-Net 系列的 AI 模型，包含網路架構設計、特點、作用、輸出輸入，以及性能指標。

**U-Net 系列模型**

- **U-Net：**
    - 最初設計用於生物醫學影像分割，其 U 形架構能有效捕捉上下文資訊。
    - 包含編碼路徑（encoding path）和解碼路徑（decoding path），並通過跳躍連接（skip connections）結合特徵。
- **U-Net++：**
    - 改進了 U-Net 的架構，引入了巢狀的跳躍連接（nested skip connections），以提高分割的準確性。
    - 在彈性的深度的encoder，與decoder中，提供了更佳的準確度。
- **其他U-Net變體：**
    - 隨着應用場景的擴展，U-Net 衍生出了許多變體，以適應不同的分割任務。

**網路架構設計**

- **U-Net：**
    - 編碼路徑：由一系列卷積層和最大池化層組成，用於提取影像特徵。
    - 解碼路徑：由一系列上卷積層和卷積層組成，用於恢復影像的空間資訊。
    - 跳躍連接：將編碼路徑中的特徵圖與解碼路徑中的特徵圖連接，以保留細節資訊。
- **U-Net++：**
    - 在 U-Net 的基礎上，引入了巢狀的跳躍連接，使得不同層次的特徵能夠更好地融合。
    - 增加了許多跳耀連接，使得深層與淺層的feature map有更深度的連結。

**特點與作用**

- **特點：**
    - U 形架構能夠有效捕捉上下文資訊。
    - 跳躍連接有助於保留影像細節。
    - U-net++, 以及相關unet的變形，有效的針對unet的效能再優化。
- **作用：**
    - 影像分割，特別適用於生物醫學影像分割。
    - 可以應用在影像復原。

**輸入與輸出**

- **輸入：**
    - 影像。
- **輸出：**
    - 分割掩碼（segmentation mask）。

**重要說明：**

- 這些性能指標的實際數值會因網路架構的具體細節、輸入影像的大小和所使用的硬體而有所不同。
- 因為unet, 屬於卷積網路，所以可以針對網路的深度做調整，以及影像輸入的像素作調整，及可以彈性調整計算量。


### Transformer系列模型性能列表

Transformer 模型最初在自然語言處理（NLP）領域取得了巨大的成功，但近年來，它們在電腦視覺，尤其是影像分割領域也展現出了強大的能力。以下是對使用 Transformer 的影像分割 AI 模型、其網路架構設計、特點、作用、輸入輸出，以及性能指標的詳細解釋：

**使用 Transformer 的影像分割模型**

- **Segment Anything Model (SAM):**
    - 由 Meta AI 開發，SAM 是一個基礎模型，旨在實現通用影像分割。
    - 它能夠處理各種分割任務，包括物件檢測、實例分割和語意分割。
    - SAM 的一個關鍵創新是它能夠根據提示（例如，點擊或框選）來分割影像中的物件。
- **Vision Transformer (ViT) 基礎的分割模型:**
    - 這些模型將 ViT 的 Transformer 架構應用於影像分割任務。
    - 它們將影像分割成 patch，然後使用 Transformer 編碼器來提取特徵。
    - 然後使用解碼器來生成分割掩碼。
- **DETR (DEtection TRansformer) 變形:**
    - DETR 本身是用於物件偵測，但變形的DETR模型也應用在影像分割上面，使用transformer來預測每一個pixel所屬的類別。

**網路架構設計**

- **SAM 的架構：**
    - 包含一個影像編碼器（ViT），一個提示編碼器和一個掩碼解碼器。
    - 影像編碼器將影像轉換為嵌入。
    - 提示編碼器將提示轉換為嵌入。
    - 掩碼解碼器結合影像和提示嵌入來預測分割掩碼。
- **ViT 基礎的分割模型：**
    - 使用 ViT 編碼器來提取影像 patch 的特徵。
    - 使用 Transformer 解碼器（例如，類似於 U-Net 的解碼器）來生成分割掩碼。
    - 利用Transformer的注意力機制，使模型能注意整個圖象的特徵，提高分割的精確度。
- DETR 變形:
    - 沿用DETR的架構，透過querry的方式，詢問每一個pixel的類別。
    - 透過transformer的encoder與decoder，來強化每一個pixel的特徵。

**特點與作用**

- **特點：**
    - Transformer 能夠捕捉影像中的長距離依賴關係。
    - SAM 的提示工程使其具有高度的交互性和通用性。
    - 能更有效的處理影像中的遮擋物。
- **作用：**
    - 語意分割、實例分割、全景分割。
    - 醫學影像分割、自動駕駛、影像編輯等。

**輸入與輸出**

- **輸入：**
    - 影像。
    - 提示（例如，點擊、框選、文字）。（SAM）
- **輸出：**
    - 分割掩碼。

**性能指標 (index)**

- 由於 Transformer 模型通常具有大量的參數，因此它們的 parms 和 FLOPs 通常較高。
- Latency 和 throughput 會因模型的複雜性和硬體而異。

**重要說明：**

- Transformer 模型在影像分割領域仍處於快速發展階段，新的模型和技術不斷湧現。
- 這些數值會因硬體以及模型精確度設定有非常大的不同。


### 其他系列模型性能列表

好的，除了R-CNN、U-Net和Transformer-based模型之外，影像分割領域還有許多其他重要的AI模型。以下我將詳細解釋一些代表性的模型，並提供它們的網路架構設計、特點、作用、輸出輸入，以及性能指標。

**其他影像分割 AI 模型**

- **DeepLab 系列：**
    - DeepLab 系列模型專注於使用空洞卷積（Atrous Convolution）和空洞空間金字塔池化（Atrous Spatial Pyramid Pooling, ASPP）來提高語意分割的準確性。
    - 空洞卷積擴大了感受野，而 ASPP 則捕捉了多尺度的上下文資訊。
- **Fully Convolutional Networks (FCNs)：**
    - FCNs 是最早將深度學習應用於語意分割的開創性工作。
    - 它們將傳統的卷積神經網路中的全連接層替換為卷積層，以實現像素級別的預測。
- **SegNet:**
    - SegNet 是一種編碼器-解碼器架構，其特點是解碼器使用與相應編碼器層相同的最大池化索引來進行上採樣。
    - 這使得 SegNet 在保留高頻細節方面非常有效。

**網路架構設計**

- **DeepLab：**
    - 使用空洞卷積來控制感受野。
    - ASPP 模組使用不同空洞率的空洞卷積來捕捉多尺度上下文。
    - 使用條件隨機場（Conditional Random Fields, CRFs）進行後處理，以提高分割精確度。
- **FCNs：**
    - 將全連接層替換為卷積層。
    - 使用上採樣來恢復特徵圖的空間解析度。
    - 使用跳躍連接來結合不同層次的特徵。
- **SegNet：**
    - 編碼器部分類似於 VGG 網路。
    - 解碼器使用與編碼器相對應的最大池化索引進行上採樣。

**特點與作用**

- **DeepLab：**
    - 擅長捕捉多尺度上下文資訊。
    - 在語意分割任務中具有很高的準確性。
- **FCNs：**
    - 開創了深度學習在語意分割中的應用。
    - 為後續的語意分割模型奠定了基礎。
- **SegNet：**
    - 有效的保留了影像細節，在需要精確輪廓的分割任務中表現良好。

**輸入與輸出**

- **輸入：**
    - 影像。
- **輸出：**
    - 分割掩碼。

**重要說明：**

- 性能指標會因硬體和具體模型而異。
- 這些模型都非常的重要，針對不一樣的使用情境，會有不一樣的效益。



### QA-list

| Q                               | Ans |
| ------------------------------- | --- |
| 简述Deeplab v1,v2,c3网络            |     |
| 条件随机场(CRF)后处理的目的                |     |
| U-Net網路設計思路                     |     |
| 简述encode和decode思想               |     |
| FCN与CNN最大的区别？                   |     |
| 分割出来的结果通常会有不连续的情况，怎么处理？开运算闭运算   |     |
| 简单阐述一下mIOU,写出mIOU的计算公式          |     |
| 空洞卷积的具体实现                       |     |
| 简要阐述一下图像分割中常用的Loss              |     |
| 为什么UNet++可以被剪枝，怎么去决定剪多少？        |     |
| 在Unet网络结构中，四次降采样对于分割网络到底是不是必须的？ |     |
| SAM分割一切网络中的Promot类型以及如何输入进网络    |     |


一位算法工程师从30+场秋招面试中总结出的语义分割超强面经（含答案） - 极市平台的文章 - 知乎
https://zhuanlan.zhihu.com/p/374517360