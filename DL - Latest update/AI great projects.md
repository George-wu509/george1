[1] 
2024.10.30<mark style="background: #BBFABBA6;"> **LongVU** - Multimodal LLM (QA, summary, identify from video)</mark> - Meta
[Linkedin post](https://www.linkedin.com/feed/update/urn:li:activity:7257400108944699392/)

Let's go!! [Meta](https://www.linkedin.com/company/meta/) released a new video LLM on [Hugging Face](https://www.linkedin.com/company/huggingface/), and it sets a new SOTA (state-of-the-art) for open-source video understanding. ğŸ”¥  
  
The model is called LongVU, a new multimodal large language model capable of processing long videos (for things like answering questions about it, summarizing it, identifying important passages, etc).

Reference
[Project link](https://vision-cair.github.io/LongVU/)
[huggingface link](https://huggingface.co/spaces/Vision-CAIR/LongVU)


------------------------------------------------------------------
[2]
2024.10.30  <mark style="background: #BBFABBA6;">NotebookLM and Illuminate - speech generation model</mark> - DeepMind
[Linkedin post ](https://www.linkedin.com/feed/update/urn:li:activity:7257407562084446209/)

We recently helped develop two tools: NotebookLM and Illuminate to narrate articles, generate stories, and even create multi-speaker discussions. Hereâ€™s how the technology works:  
  
Our latest speech generation model can produce 2ï¸âƒ£ minutes of dialogue.

Reference
[DeepMind post](https://deepmind.google/discover/blog/pushing-the-frontiers-of-audio-generation/?utm_source=linkedin&utm_medium=social&utm_campaign=&utm_content=)


------------------------------------------------------------------
[3]
2024.10.30  <mark style="background: #BBFABBA6;">Layer Skip - end-to-end solution for accelerating LLMs</mark> - Meta
[Linkedin post](https://www.linkedin.com/feed/update/urn:li:activity:7257093132419256321/)

We previously shared our research on Layer Skip, an end-to-end solution for accelerating LLMs from researchers at Meta FAIR. It achieves this by executing a subset of an LLMâ€™s layers and utilizing subsequent layers for verification and correction. Weâ€™re now releasing inference code and fine-tuned checkpoints for this work.

Reference
[huggingface model link](https://huggingface.co/collections/facebook/layerskip-666b25c50c8ae90e1965727a)
[Meta post](https://ai.meta.com/blog/fair-news-segment-anything-2-1-meta-spirit-lm-layer-skip-salsa-lingua/?utm_source=linkedin&utm_medium=organic_social&utm_content=video&utm_campaign=fair)


------------------------------------------------------------------
[4]
2024.10.30 
OmniParser - turning screenshots of UIs into structured data - Microsoft
Ferret-UI - turning screenshots of UIs into structured data - Apple
[Linkedin](https://www.linkedin.com/feed/update/urn:li:activity:7256617300131287041/)

Both [Microsoft](https://www.linkedin.com/company/microsoft/) and [Apple](https://www.linkedin.com/company/apple/) released interesting new multimodal models on [Hugging Face](https://www.linkedin.com/company/huggingface/) recently which do a very similar thing: turning screenshots of user interfaces (UIs) into structured data.
Microsoft released OmniParser. OmniParser consists of 2 models, applied in sequence:  
an object detection model (YOLOv8), fine-tuned for Interactable Region Detection (identifying interactable regions from the UI screen).

Apple released Ferret-UI (shown below): a new multimodal large language model (MMLM) tailored for enhanced understanding of mobile UI screens, equipped with referring, grounding, and reasoning capabilities

Reference
[OmniParser github](https://github.com/microsoft/OmniParser)  [OmniParser model](https://huggingface.co/microsoft/OmniParser)
[Ferret-UI demo](https://huggingface.co/spaces/jadechoghari/ferret-demo)   [Ferret-UI model](https://huggingface.co/models?search=ferret-ui)

------------------------------------------------------------------
[5]
2024.10.31
NEO Beta - A Humanoid Robot for the Home
[ç±»ä¼¼æçº¿æœ¨å¶æˆ–è€…æœ‰è½¨ç”µè½¦ï¼ŸNEO Betaçš„æŠ€æœ¯ï¼Œæˆ–è®¸å¸¦æ¥ä¸€äº›äººå½¢æ–°æ€è·¯](https://zhuanlan.zhihu.com/p/721299474)
[Introducing NEO Beta youtube](https://www.youtube.com/watch?v=bUrLuUxv9gE&ab_channel=1X)



------------------------------------------------------------------
[6]
2024.11.01
Compression for AI - Open AI Ilya Sutskever

[An Observation on Generalization youtube](https://www.youtube.com/watch?v=AKMuA_TVz3A&ab_channel=SimonsInstitute)
[lya Sutskever 2023ä¼¯å…‹åˆ©å¤§å­¦æ¼”è®²å›é¡¾](https://www.youtube.com/watch?v=RH-IdE9udMc&ab_channel=%E6%9C%80%E4%BD%B3%E6%8B%8D%E6%A1%A3)
å‹ç¼©å³æ™ºèƒ½ ï¼ˆcompression for AIï¼‰éšç¬” - é™ˆé›„è¾‰çš„æ–‡ç«  - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/671472916
ç›´æ¥å‹ç¼©ä¸€åˆ‡ï¼OpenAIé¦–å¸­ç§‘å­¦å®¶Ilya Sutskeverè¿™ä¹ˆçœ‹æ— ç›‘ç£å­¦ä¹  - æœºå™¨ä¹‹å¿ƒçš„æ–‡ç«  - çŸ¥ä¹
https://zhuanlan.zhihu.com/p/651170707

---
[7]
2024.11.27

@@ [Adobe](https://zhuanlan.zhihu.com/p/4948174414):
[1] Clean Machineï¼šè—åœ¨è§†é¢‘é‡Œçš„ç‘•ç–µï¼Œéƒ½èƒ½æ¸…ç†æ˜¯èƒ½å¤Ÿè‡ªåŠ¨æ£€æµ‹å¹¶æ¸…é™¤ç‘•ç–µé•œå¤´çš„ Clean Machineã€‚ç”±é•œå¤´çœ©å…‰é€ æˆçš„å…‰æ–‘ã€è¿‡æ›ç­‰ï¼Œå½“ç„¶å¯ä»¥æ‰‹åŠ¨æ¶ˆé™¤ï¼Œé‚£å°±è¦ä¸€å¸§ä¸€å¸§çš„æ£€æŸ¥ï¼Œå†æ…¢æ…¢ä¿®æ•´ã€‚Clean Machine è‡ªåŠ¨æ£€æŸ¥æ•´ä¸ªè§†é¢‘ï¼Œæ‰¾å‡ºæ‰€æœ‰çš„ç‘•ç–µå¸§ï¼Œç„¶åè‡ªåŠ¨ä¿®å¤ï¼ŒåŒæ—¶å¡«è¡¥ä¸ŠåŸæœ‰çš„å…ƒç´ ã€‚åœ¨ç»è¿‡ä¿®å¤ä¹‹åï¼Œä¸ä»…è¿‡æ›çš„ç”»é¢ç§»é™¤äº†ï¼Œä¹Ÿæ²¡æœ‰æŸä¼¤æ•´ä¸ªè§†é¢‘çš„å“è´¨ï¼Œéå¸¸ä¸æ»‘ã€‚è¿™é¡¹æŠ€æœ¯åº”ç”¨åœºæ™¯å¾ˆå¤šï¼Œæ¯”å¦‚ä¸‹å›¾è¿™ç§å¤§é¢ç§¯é˜´å½±ï¼Œæ˜¯å› ä¸ºè·¯äººé—¯è¿›äº†é•œå¤´é€ æˆäº†é®æŒ¡ã€‚å¯ä»¥çœ‹åˆ°æ¨¡å‹ä»ç„¶å¯ä»¥ç»™å‡ºä¸€ä¸ªå¹²å‡€ã€æ— é®æŒ¡çš„ç”»é¢ã€‚

[2] HiFiï¼šå®æ—¶ç”Ÿæˆï¼Œè¿˜èƒ½å®æ—¶ç¼–è¾‘. HiFi æ˜¯ä¸€ä¸ªèƒ½å¤Ÿå®Œæˆå®æ—¶ç”Ÿæˆå’Œç¼–è¾‘çš„åŠŸèƒ½ï¼Œä¸ç®—æ˜¯å¾ˆæ–°é²œçš„æƒ³æ³•ï¼Œå¤–å¤´ä¸å°‘åˆ›ä¸šå…¬å¸å·²ç»åšäº†å‡ºæ¥ã€‚ è¿™ä¸ªç‰¹æ€§åœ¨ Adobe çš„æ¼”ç¤ºä¸­ï¼Œè¿˜æ˜¯èƒ½çœ‹åˆ°åŸºæœ¬é€»è¾‘æ˜¯ä¸å˜çš„ï¼šæŠ½å¡ã€‚åªä¸è¿‡ä¸€è‡´æ€§è¡¨ç°å¾—å¥½ï¼Œè€Œä¸”ç”¨æˆ·å¯ä»¥ç²¾å‡†æ§åˆ¶ã€è°ƒèŠ‚çš„é¢—ç²’åº¦æ›´ç»†ã€‚åƒä¸Šé¢è¿™ä¸ªæ ‘å¶ï¼Œæ¯ä¸€ç¬”æ·»ä¸Šå»çš„æ—¶å€™ï¼Œå…¶å®ç»¿æ¤çš„æ•´ä½“éƒ½åœ¨å˜åŒ–ï¼Œçœ‹å¾—å‡ºè¿˜æ˜¯åœ¨æŠ½å¡ã€‚ä¸è¿‡è½®å»“ã€å¶ç‰‡æ•°é‡è¿™äº›ï¼ŒåŸºæœ¬èƒ½ä¿æŒä¸€è‡´ã€‚è¿™åœ¨è¢«æŠ½å¡å›°æ‰°çš„ç”Ÿæˆå·¥å…·ä¸­ï¼Œå±å®éš¾å¾—äº†ã€‚ HiFi è¿˜è®¡åˆ’æ”¯æŒé•œå¤´æ•æ‰ã€‚åœ¨å¤§ä¼šä¸Šï¼Œå˜‰å®¾åœ¨é•œå¤´ä¸‹ç”¨çº¸æ ·æ­é…äº†ä¸€ä¸‹å®¶å…·ï¼Œæ‰«æä¹‹åï¼Œç«‹åˆ»å°±ç”Ÿæˆäº†å¯¹åº”çš„æ•ˆæœå›¾ï¼Œè€Œä¸”é«˜åº¦è¿˜åŸã€‚

[3] Perfect Blendï¼šä¸€é”® PS æŒ‡æ—¥å¯å¾…. å¦ä¸€ä¸ªæµ·æŠ¥ç¥å™¨æ˜¯ Perfect Blendï¼Œå¯èƒ½æ˜¯è¿™æ¬¡æ‰€æœ‰ Sneaks ä¸­æœ€å®ç”¨çš„ä¸€ä¸ªï¼ŒæœŸå¾…å®ƒèƒ½å®è£…è¿› Photoshop é‡Œï¼ŒAdobe æå¿«ç‚¹ã€‚Perfect Blend éå¸¸æ¥è¿‘ã€Œä¸€é”® PSã€ï¼Œè€Œä¸”ä¸æ­¢æ˜¯å¥—ä¸ªç¾é¢œæ»¤é•œé‚£ä¹ˆç®€å•ï¼Œæ˜¯çœŸ-ä¿®å›¾ã€‚æŠ å›¾å•¥çš„ä¸ç”¨è¯´ï¼Œé‡ç‚¹æ˜¯è°ƒæ•´å…‰æ•ˆå’Œè‰²è°ƒã€‚ç»è¿‡æ¨¡å‹è®¡ç®—ï¼ŒåŸæœ¬å…‰çº¿å·®å¼‚æå¤§çš„å…ƒç´ ï¼Œéƒ½ã€Œèåˆã€åœ¨äº†åŒä¸€ä¸ªå…‰çº¿é€»è¾‘ä¸­ã€‚å°¤å…¶æ˜¯å¥³å­©ï¼ˆå¥¹æ˜¯è¿™ä¸ªé¡¹ç›®çš„å¼€å‘è€…ï¼‰ï¼Œè„¸ä¸ŠåŸæœ¬çš„æ …æ ¼å½±å­éƒ½è¢«å®Œç¾ä¿®æ‰ã€‚å…‰çº¿å·®å¼‚æå¤§çš„æƒ…å†µï¼Œä¹Ÿä¾ç„¶å¯ä»¥æŒæ§ã€‚è¿™å¼ äººåƒé‡Œï¼Œè„¸éƒ¨å‡ ä¹éƒ½éšåœ¨é»‘æš—ä¸­ï¼Œä¹Ÿèƒ½æ ¹æ®æ–°çš„å¤•é˜³èƒŒæ™¯ï¼Œè¦†ä¸Šæ–°çš„å…‰æ³½ã€‚å“ªæ€•æ‰€æœ‰çš„äººåƒæ˜¯ç”±ä¸åŒçš„ç›¸æœºæ‹æ‘„ã€åœ¨ä¸åŒçš„æ—¶é—´ã€æœ‰ä¸åŒçš„è§’åº¦ã€ä¸åŒçš„å…‰çº¿ï¼Œéƒ½ä¸æ˜¯é—®é¢˜ï¼Œç›´æ¥ä¸€é”®ç”Ÿæˆã€Šå†°ä¸ç«ä¹‹æ­Œã€‹é£æ ¼æµ·æŠ¥ã€‚

[4] Turntableï¼šèƒ½è½¬çš„çŸ¢é‡å›¾. Turntable æ˜¯æœ€æœ‰å·§æ€çš„ä¸€ä¸ªï¼Œå¯¹ç”»æ‰‹ã€åŠ¨ç”»äººä¹Ÿå¾ˆæœ‰ç”¨ï¼Œå®è£…è¿› Illustrator æŒ‡æ—¥å¯å¾…ã€‚è¿™æ˜¯ä¸€ä¸ªè½¬åŒ–çŸ¢é‡å›¾åƒçš„åŠŸèƒ½ï¼ŒæŠŠå·²ç»æ•´ä½“å­˜åœ¨çš„çŸ¢é‡å›¾å±‚ï¼Œä¸€é”®è½¬åŒ–æˆã€Œç«‹ä½“ã€æ¨¡å‹ã€‚ä¸‹å›¾ä¸­çš„éª‘å£«ï¼Œè¢«è½¬åŒ–ä¹‹åï¼Œè§’åº¦å°±å¯ä»¥è‡ªç”±è°ƒæ•´ã€‚é™¤äº†æ¨ªå‘æ—‹åŠ¨ä¹Ÿå¯ä»¥çºµå‘ï¼Œç»¼åˆèµ·æ¥å°±èƒ½å¤Ÿè°ƒèŠ‚è§†è§’ç»´åº¦ã€‚å®é™…ä¸Š Turntable ä¸­ï¼Œå³ä¾¿å›¾åƒèƒ½å¤Ÿå…¨æ–¹ä½æ—‹è½¬ï¼Œä¾ç„¶è¿˜æ˜¯çŸ¢é‡å›¾ã€‚æ¨¡å‹æ‰€åšçš„æ˜¯è¡¥å…¨äº†åŸå§‹ç”»é¢æ²¡æœ‰æ¶‰åŠåˆ°çš„éƒ¨ä½ã€‚æ¯”å¦‚ä¸‹é¢è¿™åŒ¹é©¬ï¼š åœ¨åŸå§‹ç”»é¢ä¸­ï¼Œè¿™åŒ¹é©¬æ˜¯æ­£ä¾§é¢ï¼Œåªèƒ½çœ‹è§ä¸¤æ¡è…¿ã€‚å¯ä¸€æ—¦è¦æ¢è§’åº¦ï¼Œå¦å¤–ä¸¤æ¡è…¿åŠ¿å¿…è¦éœ²å‡ºæ¥ã€‚è€Œæœ‰äº† Turntableï¼Œä¸å¿…é‡ç”»ï¼Œä¸å¿…è¡¥å¸§ï¼Œåªéœ€è¦é¼ æ ‡æ‹–ä¸€æ‹–ï¼Œå°±æ˜¯æœ€ç¬¦åˆé€»è¾‘çš„ç”»é¢ã€‚åˆæˆ–è€…æ˜¯è¿™ä¸ªè–¯ç‰‡ä¾ ï¼Œå®ƒçš„èƒŒé¢å’Œä¾§é¢éƒ½æ²¡æœ‰ç”»å‡ºæ¥ã€‚å¦‚æœæƒ³æ”¹å˜è§’åº¦ï¼Œéœ€è¦è¡¥è¶³è¿™äº›æ²¡æœ‰çš„éƒ¨åˆ†

**

---

[8]
2024.12.03  <mark style="background: #BBFABBA6;">Motion Prompting: Controlling Video Generation with Motion Trajectories</mark> - DeepMind
[Github](https://motion-prompting.github.io/)

Step 1: Train a Track Conditioned Video Model
Step 2: Prompt the model with Motion Prompts
	1. Object Controlç‰©ä»¶æ§åˆ¶
	2. Emergent Physicsæ–°èˆˆç‰©ç†å­¸
	3. Control with Geometryå¹¾ä½•æ§åˆ¶
	4. Camera Controlç›¸æ©Ÿæ§åˆ¶
	5. Object + Camera Controlç‰©é«”+ç›¸æ©Ÿæ§åˆ¶
	6. Motion Transferå‹•ä½œè½‰ç§»


---