
## AI視覺雙雄：GroundedSAM 與 GroundingDINO 深度解析

近年來，電腦視覺領域湧現出眾多強大的基礎模型，其中由Meta AI 推出的 **SAM (Segment Anything Model)** 和 **DINOv2** 以其卓越的通用能力備受矚目。然而，在此基礎上，更新的 **GroundedSAM** 和 **GroundingDINO** 模型進一步拓展了AI的感知邊界，實現了從單純的視覺理解到與自然語言深度融合的跨越。本文將詳細解釋GroundedSAM與GroundingDINO的核心概念，深入剖析它們與SAM和DINOv2的根本區別，並探討其模型架構與應用場景的異同。

十分钟解读Grounding DINO-根据文字提示检测任意目标 - 李小羊学AI的文章 - 知乎
https://zhuanlan.zhihu.com/p/627646794

Grounded-SAM（最强Zero-Shot视觉应用）：本地部署及各个模块的全网最详细使用教程！ - 共由的文章 - 知乎
https://zhuanlan.zhihu.com/p/690398665

---

### 核心差異：從「分割一切」到「指定分割」，從「自監督學習」到「開放集檢測」

在深入技術細節之前，我們首先需要理解這四個模型在核心思想上的根本不同：

|模型|核心思想|輸入|輸出|與前代關係|
|---|---|---|---|---|
|**SAM**|**可提示的圖像分割 (Promptable Segmentation)**|圖像 + 提示 (點、框、文本)|精細的分割掩碼 (Mask)|基礎分割模型|
|**GroundedSAM**|**基於文本的指定對象分割 (Text-prompted Segmentation)**|圖像 + 自由文本描述|特定對象的邊界框和分割掩碼|**組合模型 (Pipeline)**：GroundingDINO + SAM|
|**DINOv2**|**自監督視覺特徵學習 (Self-supervised Feature Learning)**|大量未標註圖像|高質量的通用圖像特徵向量|基礎視覺特徵提取器|
|**GroundingDINO**|**開放集對象檢測 (Open-Set Object Detection)**|圖像 + 自由文本描述|符合文本描述的對象邊界框 (Bounding Box)|**架構融合**：將類DINO架構與語言模態結合|

匯出到試算表

簡單來說，**GroundedSAM並不是一個全新的獨立模型，而是一個巧妙的「組合體」或「工作流」**，它將GroundingDINO的語言檢測能力與SAM的精準分割能力結合在一起。而**GroundingDINO則是在DINO的Transformer架構思想上，進行了多模態的改造**，使其能夠理解文本，從而在沒有特定訓練類別的情況下檢測任意對象。

---

### 模型架構與工作原理

#### 1. SAM vs. GroundedSAM

**SAM (Segment Anything Model)**

SAM的目標是成為視覺分割領域的基礎模型，它能夠對圖像中的「任何」對象生成高質量的分割掩碼。其成功的關鍵在於獨特的模型架構和龐大的訓練數據集 (SA-1B)。

- **模型架構**：
    
    1. **圖像編碼器 (Image Encoder)**：採用重量級的Vision Transformer (ViT)，將輸入圖像轉換為高維度的特徵嵌入 (Embedding)。這個過程計算量最大，但對於同一張圖只需進行一次。
        
    2. **提示編碼器 (Prompt Encoder)**：輕量級的編碼器，用於將用戶提供的各種提示（如點擊、畫框、簡單文本）轉換為特徵向量。
        
    3. **掩碼解碼器 (Mask Decoder)**：一個高效的Transformer解碼器，它將圖像嵌入和提示嵌入結合起來，在毫秒級別內快速預測出對應的分割掩碼。
        
- **工作流程**：用戶輸入一張圖像，SAM會先計算出圖像的特徵嵌入。接著，無論用戶提供何種提示（例如在一個物體上點一下），提示編碼器會將其編碼，並與圖像嵌入一同送入掩碼解碼器，最終輸出該物體的精確分割結果。
    

**GroundedSAM (Grounded Segment Anything)**

GroundedSAM的核心思想是解決SAM本身無法直接理解複雜自由文本指令的問題。例如，你不能直接告訴SAM「分割出那隻正在追逐飛盤的棕色狗狗」，SAM需要一個明確的框或點來啟動分割。GroundedSAM通過引入GroundingDINO來解決這個問題。

- **模型架構 (工作流)**：
    
    1. **輸入**：用戶提供一張圖像和一段描述性文本（如 "the brown dog catching a frisbee"）。
        
    2. **第一步：使用 GroundingDINO 進行檢測**：GroundingDINO接收圖像和文本，準確地定位到文本所描述的「棕色狗狗」和「飛盤」，並輸出它們的邊界框 (Bounding Box)。
        
    3. **第二步：使用 SAM 進行分割**：將上一步中GroundingDINO生成的邊界框作為「提示」，輸入給SAM模型。
        
    4. **輸出**：SAM根據這些邊界框提示，對框內的物體進行精確到像素級別的分割，生成高質量的掩碼。
        

因此，GroundedSAM的架構可以視為一個**檢測後分割 (Detect-then-Segment)** 的策略，它巧妙地將兩個強大的基礎模型串聯起來，實現了端到端的、由自然語言驅動的精準分割。

程式碼片段

```
graph TD
    A[用戶輸入: 圖像 + "穿紅色裙子的女孩"] --> B{GroundingDINO};
    B -->|生成邊界框| C{SAM};
    C -->|根據邊界框生成分割掩碼| D[輸出: 紅色裙子女孩的精確分割圖像];
```

---

#### 2. DINOv2 vs. GroundingDINO

**DINOv2**

DINOv2是一個自監督學習的典範，其目標不是完成特定的下游任務（如檢測或分割），而是學習一種**通用的、高質量的視覺表示 (Visual Representation)**。它通過讓模型在沒有任何人工標籤的情況下，自行從海量圖像數據中學習内在規律。

- **訓練方式**：
    
    - **自監督學習 (Self-supervised Learning)**：DINOv2的核心是知識蒸餾 (Knowledge Distillation) 和圖像級別與塊級別的目標匹配。簡單來說，它將一張圖像的不同視圖（例如，全局視圖和局部視圖）輸入給一個學生網絡和一個教師網絡（教師網絡的權重是學生網絡的指數移動平均），並強迫學生網絡的輸出與教師網絡的輸出保持一致。
        
    - **不依賴文本**：整個訓練過程只使用圖像，不涉及任何文本數據。
        
- **模型架構**：
    
    - 主要基於 **Vision Transformer (ViT)**。
        
- **輸出與應用**：DINOv2的輸出是圖像的特徵向量。這些特徵非常強大，無需對模型進行微調 (Fine-tuning)，就可以直接應用於各種下游任務，如圖像分類、深度估計、語義分割和圖像檢索，並取得極具競爭力的結果。它相當於為電腦視覺界提供了一個強大的「特徵提取預訓練模型」。
    

**GroundingDINO**

GroundingDINO的目標是**開放集對象檢測**，即根據任意的文本描述來檢測圖像中的物體，即使這些物體的類別從未在訓練數據中明確出現過。它將傳統的閉集檢測器（只能檢測預定義的類別）擴展到了開放世界。

- **模型架構**：
    
    - **雙編碼器-單解碼器 (Dual-Encoder, Single-Decoder)** 架構：
        
        1. **圖像編碼器 (Image Backbone)**：如Swin Transformer，用於提取圖像特徵。
            
        2. **文本編碼器 (Text Backbone)**：如BERT，用於提取輸入文本的特徵。
            
        3. **特徵增強模塊 (Feature Enhancer)**：這是關鍵的跨模態融合部分。它通過交叉注意力機制 (Cross-Attention)，讓圖像特徵和文本特徵進行深度交互和融合，使得模型能夠理解文本和圖像之間的對應關係。
            
        4. **語言引導的查詢選擇 (Language-guided Query Selection)**：從融合後的特徵中，選擇與文本意圖最相關的查詢 (Query)，用於初始化解碼器。
            
        5. **跨模態解碼器 (Cross-Modality Decoder)**：類似於DINO和DETR的解碼器，但它接收的是融合了語言信息的查詢，最終預測出物體的邊界框和對應的短語標籤。
            
- **核心優勢**：
    
    - **無需NMS**：繼承自DETR和DINO系列，GroundingDINO採用端到端的訓練方式，無需非極大值抑制 (NMS) 等後處理步驟。
        
    - **真正的零樣本檢測**：能夠理解複雜的指代性表達，例如，你可以要求它檢測「一個坐在椅子上的人」，而不僅僅是檢測所有的「人」和「椅子」。
        

---

### 應用場景的區別

|模型|主要應用領域|示例|
|---|---|---|
|**SAM**|**交互式圖像編輯與標註**|• 在Photoshop等軟體中，點擊一下即可快速摳圖。  <br>• 為自動駕駛或醫學影像數據集提供半自動的精細標註。|
|**GroundedSAM**|**自動化內容生成與分析、機器人指令**|• **自動數據標註**：輸入「所有車輛」，自動標註出圖像中所有車輛的邊界框和掩碼。  <br>• **可控圖像編輯**：結合Stable Diffusion，實現「將背景中的藍色汽車換成紅色跑車」。  <br>• **機器人視覺**：指令機器人「拿起桌子上的蘋果」。|
|**DINOv2**|**作為通用視覺特徵提取的基礎模型**|• 提升現有圖像分類、分割、檢測模型的性能。  <br>• 應用於圖像相似性搜索，快速找到風格或內容相似的圖片。  <br>• 無需訓練即可進行深度估計。|
|**GroundingDINO**|**開放世界中的智能監控、零售分析、圖像檢索**|• **智能安防**：在監控視頻中檢測「一個穿著背包的可疑人員」。  <br>• **電商零售**：自動識別商品圖片中的屬性，如「帶有花卉圖案的絲質連衣裙」。  <br>• **基於文本的圖像搜索**：在海量圖庫中搜索「有三隻鳥在沙灘上」的圖片。|

匯出到試算表

### 總結

**SAM** 和 **DINOv2** 是視覺領域的基石，前者定義了分割的交互新範式，後者提供了強大的通用視覺特徵。而 **GroundedSAM** 和 **GroundingDINO** 則是建立在這兩塊基石之上的、更面向應用的進階模型，它們的核心貢獻是成功地**將自然語言的靈活性與視覺感知能力深度融合**。

- **GroundedSAM** 透過 **組合** 的方式，賦予了SAM **聽懂人話** 的能力，讓大規模、自動化的精準分割成為可能。
    
- **GroundingDINO** 透過 **架構創新**，將檢測模型從封閉世界帶入 **開放世界**，使其能夠應對現實世界中無窮無盡的對象類別。
    

理解它們之間的區別與聯繫，不僅能幫助我們更好地選擇合適的工具解決實際問題，更能讓我們洞見AI視覺技術從感知到認知，再到與人類指令交互的發展趨勢。




「**分析一張包含人物、動物和物體的公園照片**」來舉例。

#### 1. SAM (Segment Anything Model) - 可提示的分割模型

**核心概念**：SAM是一個「聽話的分割工具」。你必須用**具體的提示（點、框）**來告訴它你要分割**哪裡**，它不理解複雜的語意，但分割得極其精準。

- **輸入**：一張公園照片 + **用戶提示**
    
- **輸出**：精確的像素級**分割掩碼 (Mask)**
    

|實例|輸入 (圖像 + 提示)|輸出|概念說明|
|---|---|---|---|
|**例1: 快速摳圖**|公園照片 + 用戶在照片中的一隻狗身上**點擊一下**。|一個完美包裹住這隻狗輪廓的**分割掩碼**。|**點提示 (Point Prompt)**：最簡單直接的交互方式。用戶只需指出一個點，SAM就能理解並分割出該點所在的整個物體。|
|**例2: 精修標註**|公園照片 + 用戶用滑鼠粗略地畫一個**方框**框住一個正在玩飛盤的小孩。|一個精確描繪小孩身體、衣服和飛盤輪廓的**分割掩碼**。|**框提示 (Box Prompt)**：用戶提供一個大致範圍，SAM 在此範圍內進行精確的、像素級的分割，將粗略標註變為精細標註。|
|**例3: 排除特定區域**|公園照片 + 用戶在長椅上點一個**前景點**，在背景的樹上點一個**背景點**。|一個只包含長椅、**排除了背景樹木**的分割掩碼。|**前景/背景點提示**：透過正、負樣本點，用戶可以更精準地控制分割範圍，明確告知模型哪些部分需要、哪些部分不需要。|

#### 2. DINOv2 - 自監督特徵學習模型

**核心概念**：DINOv2是一個「沉默的學習者」。它自己從海量**無標註**圖像中學習普世的視覺規律。它的輸出不是給人看的圖像，而是給機器看的**特徵向量 (Feature Vector)**，這個向量代表了圖像的深層含義。

- **輸入**：一張公園照片
    
- **輸出**：一個高維度的**特徵向量（一組數字）**
    

|實例|輸入|輸出|概念說明|
|---|---|---|---|
|**例1: 圖像相似性搜索**|一張包含一隻柯基犬的公園照片。|代表這張照片的**特徵向量A**。|**特徵比對**：將向量A與數據庫中成千上萬張其他照片的特徵向量進行數學比對（如計算餘弦相似度），可以快速找出所有包含柯基犬的照片，無論背景或拍攝角度如何。|
|**例2: 無監督圖像聚類**|1000張不同的公園照片（未分類）。|每張照片對應一個**特徵向量**。|**發現潛在類別**：將這1000個向量輸入到聚類算法（如K-Means）中，算法會自動將它們分成不同的群組，例如「有噴泉的場景」、「有很多樹的場景」、「有兒童遊樂設施的場景」，而無需任何人工標籤。|
|**例3: 作為下游任務的骨幹**|一張公園照片，需要判斷其**深度**（哪裡離鏡頭近，哪裡遠）。|一個包含豐富空間資訊的**特徵圖 (Feature Map)**。|**遷移學習**：我們不需要從零開始訓練一個深度估計模型。可以直接使用DINOv2強大的預訓練骨幹來提取特徵，然後只訓練一個非常小的「任務頭」來將這些特徵轉換為深度圖。這大大節省了訓練成本和數據需求。|

#### 3. GroundingDINO - 開放集物件偵測模型

**核心概念**：GroundingDINO是一個「能聽懂描述的偵測器」。你可以用**自由文本**告訴它你要找**什麼**，它能在圖像中用**方框**把它標出來，即使這個東西它從未被專門訓練過。

- **輸入**：一張公園照片 + **一段描述性文字**
    
- **輸出**：符合文字描述的**邊界框 (Bounding Box)**
    

|實例|輸入 (圖像 + 文本)|輸出|概念說明|
|---|---|---|---|
|**例1: 精準查找**|公園照片 + 文本 "a boy in a red t-shirt" (穿紅色T恤的男孩)。|一個**邊界框**，精確框出穿著紅色T恤的那個男孩。|**開放集偵測 (Open-Set)**：模型能理解「紅色T恤」這種屬性描述，而不是只能尋找預設的「男孩」類別。|
|**例2: 關係理解**|公園照片 + 文本 "a dog catching a frisbee" (正在接飛盤的狗)。|兩個**邊界框**，一個框住狗，一個框住飛盤，並能理解它們之間的互動關係。|**關係與動作理解**：模型不僅識別物體，還能理解物體間的空間和動作關係，這是傳統偵測器無法做到的。|
|**例3: 場景清點**|公園照片 + 文本 "benches and trash cans" (長椅和垃圾桶)。|所有**邊界框**，分別框出圖像中每一條長椅和每一個垃圾桶。|**多目標與多類別偵測**：一個文本提示可以同時檢測多個不同類別的目標。|

匯出到試算表

#### 4. GroundedSAM - 接地氣的分割模型

**核心概念**：GroundedSAM是GroundingDINO和SAM的「強力組合」。它實現了終極目標：用**自由文本**告訴它你要分割**什麼**，它就能輸出**精確的分割掩碼**。

- **輸入**：一張公園照片 + **一段描述性文字**
    
- **輸出**：符合文字描述的、像素級的**分割掩碼**
    

|實例|輸入 (圖像 + 文本)|輸出|概念說明|
|---|---|---|---|
|**例1: 語義化圖像編輯**|公園照片 + 文本 "the shadow of the big tree" (大樹的影子)。|一個精確覆蓋地面上樹影區域的**分割掩碼**。|**分割非固體物體**：GroundedSAM能理解並分割像「影子」、「水面倒影」這樣抽象或無固定形態的物體，這是傳統分割難以做到的。|
|**例2: 自動化數據標註**|公園照片 + 文本 "all the pedestrians on the sidewalk" (人行道上所有的行人)。|**多個分割掩碼**，分別精確地分割出每一個在人行道上的行人。|**管道化工作流 (Pipeline)**：內部流程是：1. GroundingDINO先根據文本找到所有行人的邊界框；2. SAM再接收這些邊界框作為提示，完成精確分割。|
|**例3: 機器人交互**|公... (更換場景) 一張機器人視角的桌面照片 + 文本 "the blue pen next to the notebook" (筆記本旁邊的藍色筆)。|一個僅包含那支特定藍色筆的**分割掩碼**。|**精準目標定位**：這個掩碼可以直接用於機器人視覺系統，指導機器手臂準確地抓取目標物體，實現了從自然語言指令到物理操作的轉化。|

匯出到試算表

---

### 第二部分：解釋 "Surgical Video Analysis Platform"

這段話描述了一個在**聖地牙哥超級電腦中心 (SDSC)** 進行的機器學習研究項目，目標是建立一個強大的「**手術影片分析平台**」。這是一個旨在自動化分析手術錄影，從中提取有價值數據的軟體系統。

讓我們逐句分析：

> **Surgical Video Analysis Platform**

**中文解釋**：**手術影片分析平台**。 這是一個綜合性的軟體平台，其功能是接收手術過程的錄影，並利用AI技術進行深入分析，以提供數據洞察、輔助培訓、評估手術技巧或改進手術流程。

> **This work was done at SDSC**

**中文解釋**：**此項工作於聖地牙哥超級電腦中心 (SDSC) 完成。** 這說明了研究機構的背景。SDSC擁有強大的計算資源，非常適合進行這種需要大量計算的AI模型訓練。

> **ML research**
> 
> - **Object detection and multi-objects tracking for surgical tools - export analytics for surgeons**
>     

**中文解釋**：**機器學習研究**

- **手術器械的物件偵測與多物件追蹤 - 為外科醫生匯出分析數據**
    
    - **物件偵測**：識別影片中出現了哪些手術器械（如手術刀、鑷子、持針器）。
        
    - **多物件追蹤**：持續追蹤每一個器械的運動軌跡。
        
    - **匯出分析數據**：這是最終目的。平台能生成報告，告訴醫生：
        
        - **器械使用時長**：某個器械在手術中被使用了多久？
            
        - **運動效率**：醫生的操作是否流暢，有沒有多餘的動作？
            
        - **器械交替頻率**：分析醫生使用不同工具的習慣。 這些數據對於**手術技巧評估**和**新手醫生培訓**非常有價值。
            

> - **Surgical procedure’s phases classification with video vision transformer**
>     

**中文解釋**：**使用影片視覺Transformer進行手術流程的階段分類** - **手術階段分類**：一場手術可以被分解為多個標準階段（如：切皮、分離組織、止血、縫合、關閉傷口）。這個模型能自動識別影片當前處於哪個階段。 - **影片視覺Transformer**：這是一種先進的AI模型架構（如VideoMAE或TimeSformer），它特別擅長理解影片中的時間序列關係，因此非常適合用於分析手術這種具有先後順序的流程。

> - **De-identification classification model for surgical videos (Ex Situ / In Situ)**
>     

**中文解釋**：**用於手術影片的去識別化分類模型 (體外/體內視角)** - **去識別化 (De-identification)**：為了保護病人和醫護人員的隱私（符合HIPAA等法規），需要移除影片中的個人身份資訊（如人臉、姓名標籤等）。 - **Ex Situ / In Situ**：這是拉丁文，是區分視角的關鍵。 - **In Situ (體內/原位)**：指內視鏡在**病患體內**拍攝的畫面。這些畫面通常不含個人隱私，是安全的。 - **Ex Situ (體外/離位)**：指鏡頭被移出體外，拍到了**手術室環境**的畫面。這些畫面可能包含醫生或護士的臉、寫有病人資訊的白板等，需要進行模糊或遮蔽處理。 - **模型的作用**：這個分類模型會自動判斷每一幀是「In Situ」還是「Ex Situ」，從而實現**自動化的隱私保護**，只對需要處理的幀進行去識別化操作。

> - **Implemented automatic (zero-shot) labeling pipeline using self-supervised models such as GroundedSAM/GroundingDINO linked to the labeling platform Encord**
>     

**中文解釋**：**實現了一套自動化（零樣本）標註流程，該流程使用如GroundedSAM/GroundingDINO等自監督模型，並與標註平台Encord連接。** - **自動化標註流程 (Automatic labeling pipeline)**：這是整個項目中最先進的部分，也是對您第一個問題的完美呼應。傳統上，要訓練AI模型，需要人工一幀一幀地在影片上框出所有手術器械，這極其耗時耗力。 - **零樣本 (Zero-shot)**：意味著模型能夠標註它**從未被專門訓練過去識別的物體**。 - **如何實現**：他們正是利用了 **GroundedSAM/GroundingDINO** 的能力。標註人員不再需要手動畫框，只需給出**文本指令**，例如 "label all needle holders" (標註所有持針器)。 1. **GroundingDINO** 理解這段文字，在影片中自動找到所有持針器並生成**邊界框**。 2. **GroundedSAM** 接收這些邊界框作為提示，生成像素級的**精確分割掩碼**。 - **與Encord平台連接**：Encord是一個專業的數據標註平台。這意味著AI生成的標註結果會自動導入到Encord中，人工標註員只需**快速審核和修正**AI的結果，而不是從零開始。這將數據標註的效率提升了數十甚至數百倍。