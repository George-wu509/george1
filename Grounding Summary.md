



|                                              | AI Model<br>(No Transformer)                                                           | Transformer<br>(closeset)           | Grounding<br>(openset/zero-shot)                           |
| -------------------------------------------- | -------------------------------------------------------------------------------------- | ----------------------------------- | ---------------------------------------------------------- |
| *Image*<br>*object detection*                | RCNN series<br>YOLO(FCOS)<br>Lightweight<br>RetainNet<br>EfficientDet<br>MobileNet-SSD | DETR<br>DINO                        | GLIP<br>Grounding DINO                                     |
| *Image*<br>*segmentation*                    | RCNN Series<br>UNet(FCN)                                                               | DETR<br>SAM                         | Grounded SAM                                               |
| *Video*<br>*object detection*<br>*/Tracking* | YOLO<br>+<br>SORT<br>DeepSORT<br>ByteSORT<br>(ReID)                                    | Video-DETR<br>TrackFormer           | GroundingDINO + Tracker<br>                                |
| *Video*<br>*segmentation*                    | FCN / UNet <br>+ <br>RNN / LSTM<br><br>XMem                                            | VideoMAE <br>+ <br>XMem<br><br>SAM2 | Track Anything Model (TAM)<br>Grounded SAM + Tracker(XMem) |

Object detection [[###### Image object detection model]]


Image segmentation [[###### Image segmentation model]]


Video object detection [[###### Video Object Detection model]]


Video segmentation  [[###### Video segmentation model]]
1. 傳統方法: optical flow估計像素在連續幀之間的運動軌跡
           或用graph Cut (image segmentation)到每一frame
2. AI model:  Image segmentation到每一frame, 也可以與RNN or LSTM結合












===============================================================
### <mark style="background: #BBFABBA6;">Image object detection</mark>
###### Image object detection model

理解圖像物件偵測（Image Object Detection）技術的演進脈絡。從經典的卷積神經網絡（CNN）結構，到革新性的Transformer架構，再到與自然語言深度結合的Grounding模型，每一步都代表了AI感知能力的巨大飛躍。

以下我將按照您指定的**「AI model no transformer」、「transformer」、「grounding」**三種類別，為您整理各類別中主流且重要的模型。

---

### 1. AI Model (No Transformer) - 經典CNN架構

這類模型是物件偵測領域的基石，它們的設計和優化主要圍繞卷積神經網絡（CNN）展開，是理解物件偵測原理的必經之路。其核心思想是通過CNN提取圖像特徵，然後設計不同的策略來預測物體的邊界框和類別。

|模型家族|代表模型|核心思想與技術特點|
|---|---|---|
|**R-CNN 家族  <br>(兩階段)**|**Faster R-CNN**|**兩階段檢測 (Two-Stage Detection)**：這是這類方法的標誌。  <br>1. **第一階段 (區域提議)**：模型首先通過一個輕量的**區域提議網路 (Region Proposal Network, RPN)**，快速地在圖像上找出數百個可能包含物體的候選區域（Proposals）。  <br>2. **第二階段 (精確分類與回歸)**：然後，模型再對這些候選區域進行第二次的精細處理，分別送入分類器和回歸器，來確定每個區域的精確類別和邊界框位置。  <br>  <br>**優點**：由於有第二次的精細校準，這類模型的**準確率通常非常高**，尤其在小物體和定位精度上表現出色。  <br>**缺點**：流程分為兩步，**速度相對較慢**。|
|**YOLO 家族  <br>(單階段)**|**YOLO系列  <br>(v3, v5, v7, v8, v9)**|**單階段檢測 (One-Stage Detection)**：YOLO（You Only Look Once）的革命性在於它將偵測視為一個單一的迴歸問題。  <br>1. **端到端預測**：輸入一張完整的圖像，模型直接在一個前向傳播中，同時預測出所有物體的**邊界框位置、類別機率和置信度分數**。  <br>2. **網格劃分 (Grid Cell)**：它將圖像劃分為一個S×S的網格，每個網格單元負責預測中心點落入其中的物體。  <br>  <br>**優點**：速度極快，非常適合**即時（Real-time）應用**，如影片監控、自動駕駛等。  <br>**缺點**：相較於兩階段模型，在早期版本中對小物體的檢測能力和定位精度稍弱（儘管後續版本已大幅改進）。|
|**SSD  <br>(單階段)**|**SSD (Single Shot MultiBox Detector)**|**多尺度特徵圖 (Multi-Scale Feature Maps)**：SSD是另一種經典的單階段檢測器，它的核心創新是解決單階段模型難以檢測不同大小物體的問題。  <br>1. **金字塔結構**：SSD在CNN的不同深度層（特徵圖）上都設置了預測頭。較淺層的特徵圖解析度高，適合檢測小物體；較深層的特徵圖感受野大，適合檢測大物體。  <br>  <br>**優點**：在速度和精度之間取得了很好的平衡，比YOLO早期版本精度更高，比Faster R-CNN速度更快。  <br>**缺點**：對於極小物體的檢測仍然是一個挑戰。|

---

### 2. Transformer - 革新性架構

Transformer最初在自然語言處理（NLP）領域取得巨大成功，後來被引入電腦視覺。它利用自注意力機制（Self-Attention）來捕捉圖像的全局依賴關係，徹底改變了物件偵測的設計範式。

|代表模型|核心思想與技術特點|
|---|---|
|**DETR (DEtection TRansformer)**|**將物件偵測視為集合預測問題 (Set Prediction)**：這是DETR的顛覆性思想。  <br>1. **物件查詢 (Object Queries)**：模型不再需要成千上萬的錨框（Anchors）或區域提議。取而代之的是一小組（例如100個）可學習的、固定的**「物件查詢」向量**。  <br>2. **全局注意力**：編碼器-解碼器（Encoder-Decoder）架構中的注意力機制會分析整張圖像的全局資訊，然後每個物件查詢會「學會」去關注圖像中的某一個特定物體。  <br>3. **端到端無NMS**：輸出直接就是最終的預測結果集合，無需非極大值抑制（NMS）等手工設計的後處理步驟。  <br>  <br>**優點**：架構極其簡潔優雅，移除了大量手工設計的組件，是真正的端到端模型。  <br>**缺點**：原始版本的訓練收斂速度慢，且對小物體的檢測性能不佳。|
|**DINO (DETR with Improved deNoising)**|**DETR的強力升級版**：DINO在DETR的基礎上進行了多項關鍵改進，大幅提升了性能和訓練效率，成為了目前最強的閉集（Closed-Set）物件偵測器之一。  <br>1. **去噪訓練 (Denoising Training)**：在訓練中，除了標準的匹配任務，還額外給模型加入一些帶有噪聲（輕微擾動）的真值框，並要求模型去重建原始的、乾淨的框。這個過程極大地**加速了模型的收斂**並提升了性能。  <br>2. **混合查詢選擇 (Mixed Query Selection)**：結合了靜態內容查詢和動態位置查詢，讓模型更好地定位目標。  <br>  <br>**優點**：**性能極高，收斂速度快**，在COCO等標準資料集上達到了SOTA（State of the Art）水準，且保留了DETR的端到端優雅架構。|

---

### 3. Grounding - 語言驅動的開放世界

這類模型是目前最前沿的方向，它們的目標是打破傳統物件偵測「只能檢測預定義類別」的束縛，讓模型能夠理解自然語言，檢測任意可以用文字描述的物體。

|代表模型|核心思想與技術特點|
|---|---|
|**GLIP (Grounded Language-Image Pre-training)**|**統一偵測與接地 (Unifying Detection and Grounding)**：GLIP是這個領域的開創性工作之一。  <br>1. **任務轉化**：它巧妙地將傳統的物件偵測任務（如檢測「貓」）轉化為一個片語接地任務，即在圖像中定位文本提示「a cat」。  <br>2. **圖文深度融合**：通過一個**跨模態的深度融合模塊**，讓圖像特徵和文本特徵進行充分的交互，使得視覺特徵本身就「理解」了語言的含義。  <br>3. **大規模預訓練**：在海量的「圖像-文本對」數據上進行預訓練，使得模型具備了強大的泛化能力。  <br>  <br>**優點**：具備強大的**零樣本（Zero-Shot）和開放集（Open-Set）檢測能力**，可以檢測訓練時從未見過的類別。  <br>**缺點**：其底層架構仍部分依賴傳統檢測器的思想。|
|**GroundingDINO**|**將最強閉集檢測器（DINO）改造成開放集模型**：GroundingDINO是目前該領域的標竿模型。  <br>1. **架構融合**：它將DINO的強大端到端Transformer架構與GLIP的圖文接地思想完美結合。  <br>2. **語言引導的查詢 (Language-Guided Query)**：它的物件查詢（Object Queries）不再是隨機的，而是**根據輸入的文本提示來初始化**的。這意味著模型從一開始就帶著「我要找什麼」的明確目的去分析圖像。  <br>3. **貫穿始終的融合**：圖文融合不僅發生在特徵提取階段，而是貫穿了整個解碼過程，使得定位更加精準。  <br>  <br>**優點**：集大成者，在**開放集檢測的準確性、對複雜語言的理解能力和架構的優雅性**上都達到了新的高度。是目前開放世界物件偵測的首選模型。|

### 總結

| 類別                 | 核心範式                            | 優點                                | 缺點                          | 代表                      |
| ------------------ | ------------------------------- | --------------------------------- | --------------------------- | ----------------------- |
| **No Transformer** | 透過CNN提取特徵，依賴手工設計的檢測流程（如錨框、RPN）。 | 速度快（單階段）或精度高（兩階段），技術成熟，易於理解。      | 閉集限制，無法檢測預定義類別之外的物體，流程相對繁瑣。 | Faster R-CNN, YOLO, SSD |
| **Transformer**    | 將檢測視為集合預測問題，利用全局注意力機制，端到端輸出。    | 架構簡潔優雅，無需NMS等後處理，性能強大。            | 閉集限制，訓練複雜度較高。               | DETR, DINO              |
| **Grounding**      | 將自然語言作為指令，實現開放世界檢測。             | **開放集/零樣本能力**，極致的靈活性，可檢測任意可描述的物體。 | 模型複雜度最高，對計算資源要求高。           | GLIP, GroundingDINO     |





===============================================================
### <mark style="background: #BBFABBA6;">Image segmentation</mark>
###### Image segmentation model

圖像分割（Image Segmentation）領域。圖像分割比物件偵測更進一步，它追求的是像素級別的精確分類，而不是簡單的邊界框。

在開始整理模型之前，我們必須先回答您的第二個問題，因為這是理解不同模型設計初衷的關鍵：**Semantic Segmentation 和 Instance Segmentation 有差別嗎？**

答案是：**有，而且差別巨大。**

1. **語義分割 (Semantic Segmentation)**
    
    - **目標**：將圖像中的**每個像素**分配到其所屬的**物體類別**。
        
    - **特點**：它只關心「類別」，不區分同類別中的不同個體。
        
    - **例子**：在一張有三個人的照片中，語義分割會將所有屬於「人」這個類別的像素標記為同一種顏色。它告訴你「這片區域是人」，但不會告訴你這是三個人。
        
2. **實例分割 (Instance Segmentation)**
    
    - **目標**：在語義分割的基礎上，進一步區分出**同類別中的每一個獨立個體（實例）**。
        
    - **特點**：它既要知道一個像素屬於「人」，又要知道它屬於「1號人」、「2號人」還是「3號人」。
        
    - **例子**：在同一張有三個人的照片中，實例分割會為每個人都分配一個獨特的顏色。它告訴你「這是1號人，這是2號人，這是3號人」。
        

**簡單來說：語義分割回答「這塊是什麼？」，而實例分割回答「這裡有幾個？它們分別是誰？」**。因此，實例分割通常被認為是比語義分割更複雜、更具挑戰性的任務。

接下來，我將按照**「AI model no transformer」、「transformer」、「grounding」**三種類別，並標明每個模型主要解決的是「語義分割」還是「實例分割」問題，為您整理主流模型。

---

### 圖像分割主流模型整理

### 1. AI Model (No Transformer) - 經典CNN架構

這類模型是圖像分割的基石，它們的設計精巧地利用了卷積神經網路的特性，尤其是編碼器-解碼器結構。

|代表模型|主要任務|核心思想與技術特點|
|---|---|---|
|**U-Net**|**語義分割**|**對稱的編碼器-解碼器結構 (Encoder-Decoder)**：U-Net是醫學影像分割領域的開創性工作，其優雅的U形結構影響深遠。  <br>1. **編碼器 (下採樣路徑)**：一個標準的卷積網路，用於逐步提取圖像的深層語義特徵，同時空間解析度降低。  <br>2. **解碼器 (上採樣路徑)**：逐步將低解析度的深層特徵圖恢復到原始圖像大小，以實現像素級的預測。  <br>3. **跳躍連接 (Skip Connections)**：**這是U-Net的靈魂**。它將編碼器中對應層級的、包含豐富空間細節的淺層特徵，直接「跳躍」並拼接（Concatenate）到解碼器中。這極大地緩解了下採樣過程中的細節資訊丟失，使得分割邊緣更為精確。  <br>  <br>**應用**：醫學影像、衛星圖像等需要高精度分割的場景。|
|**DeepLab 系列  <br>(v1, v2, v3, v3+)**|**語義分割**|**空洞卷積/帶孔卷積 (Atrous/Dilated Convolution)**：DeepLab系列是語義分割領域的另一個里程碑，其核心是解決如何在不降低解析度的情況下擴大感受野的問題。  <br>1. **空洞卷積**：它在標準卷積核的元素之間插入「空洞」（零），從而可以用同樣的計算成本看到更廣泛的圖像區域。這使得模型能夠在保持高解析度特徵圖的同時，捕捉多尺度的上下文資訊。  <br>2. **ASPP (Atrous Spatial Pyramid Pooling)**：進一步使用多個不同「空洞率」的卷積核並行提取特徵，然後將結果融合，從而更有效地捕捉同一像素周圍的多尺度上下文。  <br>  <br>**應用**：街景解析、自動駕駛等複雜場景。|
|**Mask R-CNN**|**實例分割**|**在Faster R-CNN上擴展分割分支**：Mask R-CNN是實例分割領域的標竿之作，其思想非常直觀。  <br>1. **基於兩階段檢測**：它繼承了Faster R-CNN的框架。在RPN找到候選區域後，除了原有的「分類分支」和「邊界框迴歸分支」外，額外並行地增加了一個**「掩碼預測分支 (Mask Branch)」**。  <br>2. **RoIAlign**：針對分割任務對像素對齊的高要求，它提出了RoIAlign來取代Faster R-CNN中的RoIPooling，解決了特徵圖與原始圖像之間的像素錯位問題，顯著提升了掩碼的質量。  <br>  <br>**本質**：可以理解為「先做物件偵測，再對每個檢測出的物體進行語義分割」。|

---

### 2. Transformer - 革新性架構

Transformer的全局注意力機制能夠很好地捕捉長距離依賴關係，為分割任務帶來了新的視角，催生了許多簡潔而強大的模型。

|代表模型|主要任務|核心思想與技術特點|
|---|---|---|
|**ViT (Vision Transformer) + 分割頭**|**語義分割**|**將圖像視為Patch序列**：ViT是將Transformer直接應用於圖像分類的開創性工作。要將其用於分割，通常做法如下：  <br>1. **ViT骨幹網路**：使用預訓練好的ViT作為強大的特徵提取器。  <br>2. **簡單的解碼器/分割頭**：在ViT提取的特徵之上，接一個輕量級的解碼器（如卷積層或MLP），將特徵圖上採樣回原始解析度，並預測每個像素的類別。  <br>  <br>**優點**：架構簡單，能有效利用ViT強大的全局建模能力。  <br>**缺點**：ViT本身對計算和數據量要求高，且缺乏類似U-Net的精心設計的解碼器結構。|
|**Mask2Former**|**通用分割架構  <br>(語義/實例/全景)**|**將分割視為掩碼分類問題 (Mask Classification)**：Mask2Former是一個里程碑，它首次提出了一個**統一的架構**來同時解決語義、實例和全景分割三大任務。  <br>1. **基於查詢 (Query-based)**：與DETR類似，它也使用一組可學習的物件查詢。  <br>2. **預測掩碼嵌入**：模型不直接預測像素的類別，而是為每個查詢預測一個二值的**掩碼（Mask）和一個類別嵌入向量（Class Embedding）**。  <br>3. **分類方式**：對於語義分割，它將所有預測為同一類別的掩碼合併；對於實例分割，每個查詢輸出的掩碼自然對應一個實例。  <br>  <br>**優點**：**架構統一、性能極強**，在三大分割任務上同時達到了SOTA水準，是目前最強的分割模型之一。|

---

### 3. Grounding - 語言驅動的開放世界

這類模型將分割任務從「識別固定類別」提升到了「理解自然語言指令並進行分割」的全新高度。

| 代表模型                             | 主要任務                                | 核心思想與技術特點                                                                                                                                                                                                                                                                                                                                                                              |
| -------------------------------- | ----------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **SAM (Segment Anything Model)** | **可提示分割 (Promptable Segmentation)** | **分割一切的基礎模型**：SAM的目標不是去識別「這是什麼」，而是根據用戶的**提示（Prompt）來分割出對應的任何物體。  <br>1. 提示多樣性：提示可以是點、框、文本甚至是粗略的掩碼**。  <br>2. **解耦的架構**：包含一個重量級的圖像編碼器（只對圖像計算一次）和一個輕量級的掩碼解碼器（根據提示快速生成掩碼），這使得交互式分割非常高效。  <br>  <br>**限制**：SAM本身對複雜的自由文本理解能力有限，其文本提示通常指代簡單的類別名稱。  <br>**主要任務**：其輸出天生是**實例級別**的，因為每個提示都旨在分割出一個特定的對象實例。                                                                                      |
| **Grounded SAM**                 | **開放集實例分割**                         | **GroundingDINO + SAM 的強力組合**：Grounded SAM是一個**工作流（Pipeline）**，而非單一模型，它完美地解決了SAM語言理解能力不足的問題。  <br>1. **第一步 (接地檢測)**：用戶輸入一段複雜的自然語言描述（如「穿著紅色裙子的女孩」），**GroundingDINO**負責理解這段話，並在圖像中定位出目標的**邊界框**。  <br>2. **第二步 (提示分割)**：將上一步得到的邊界框作為「提示」自動輸入給**SAM**。  <br>3. **第三步 (輸出掩碼)**：SAM根據這個精確的邊界框提示，生成目標物體的像素級**實例分割掩碼**。  <br>  <br>**優點**：實現了真正的**用自然語言進行任意物體的精確實例分割**，是目前開放世界分割領域最強大、最靈活的工具。 |




===============================================================
### <mark style="background: #BBFABBA6;">video object detection</mark>
###### Video Object Detection model

影片物件偵測 (Video Object Detection, VOD) 和物件追蹤 (Object Tracking, OT) 關係極其緊密，但它們的目標和所使用的技術既有重疊，也有顯著的區別。

首先，我們來回答第一個問題：**用的model或技術是不是一樣的？**

答案是：**不完全一樣，但高度相關，且正在走向融合。**

傳統上，這兩個任務是分開處理的，構成了一個經典的範式，稱為「**先偵測後追蹤 (Tracking-by-Detection)**」。

- **Video Object Detection (VOD)**：目標是**在影片的每一幀中，盡可能準確地找到所有感興趣的物體**。它回答的是「這一幀裡有什麼？」這個問題。每一幀的處理是相對獨立的。
    
- **Object Tracking (OT)**：目標是**在連續的幀之間，為同一個物體分配一個獨一無二且保持不變的ID**。它回答的是「上一幀的『人A』現在在哪裡？」這個問題。它的核心在於**建立跨幀的身份關聯 (Identity Association)**。
    

在「先偵測後追蹤」的框架下，VOD模型和OT技術是**解耦合**的：

1. 先用一個**物件偵測模型**（如YOLO, Faster R-CNN）處理每一幀，得到當前的偵測框。
    
2. 再用一個**追蹤演算法**（如DeepSORT, FairMOT）來匹配當前幀的偵測框和上一幀已經存在的物體軌跡。
    

然而，隨著Transformer技術的發展，這兩個任務正在被統一到一個**端到端的單一模型**中，模型能夠同時完成偵測和追蹤。

---

接下來，我將按照您指定的**「AI model no transformer」、「transformer」、「grounding」**三種類別，為您整理這兩個領域中主流且重要的模型。

### 主流模型與技術整理

|類別|核心思想|主流/重要模型|技術特點與關係|
|---|---|---|---|
|**1. AI Model (No Transformer)**|**先偵測後追蹤 (Tracking-by-Detection)**|**偵測器**:  <br>• **YOLO系列 (v3-v9)**  <br>• **Faster R-CNN**  <br>• **SSD**  <br>  <br>**追蹤器**:  <br>• **SORT (Simple Online and Realtime Tracking)**  <br>• **DeepSORT**  <br>• **FairMOT**  <br>• **JDE (Jointly learns the Detector and Embedding)**|**技術關係**：這是一個**組合式**的方案。偵測器和追蹤器是兩個獨立的模組，可以任意搭配。  <br>  <br>**工作流程**：  <br>1. **偵測**：YOLO等模型在當前幀找出所有物體的位置。  <br>2. **預測**：SORT使用卡爾曼濾波器(Kalman Filter)預測上一幀的物體在新一幀中的可能位置。  <br>3. **匹配**：通過計算預測位置與當前偵測位置的重疊度（IoU）來進行數據關聯。  <br>  <br>**DeepSORT的改進**：在SORT的基礎上，額外加入了一個**外觀特徵提取網路 (Re-ID model)**。即使物體被短暫遮擋後重現，導致位置預測不準，DeepSORT依然可以通過比較外觀相似度來重新識別同一個物體，大大提升了追蹤的魯棒性。  <br>  <br>**FairMOT/JDE的進步**：嘗試在一個網路中同時完成偵測和外觀特徵提取（Re-ID），提高了效率，但本質上仍是「偵測+匹配」的思想。|
|**2. Transformer**|**端到端偵測與追蹤 (End-to-End Detection & Tracking)**|• **Video-DETR**  <br>• **TrackFormer**  <br>• **TransTrack**  <br>• **MeMOT (Memory-based)**|**技術關係**：這是一個**一體化**的方案。單一的Transformer模型可以直接輸出帶有身份ID的物體軌跡，不再需要獨立的匹配步驟。  <br>  <br>**工作流程**：  <br>1. **物件查詢 (Object Query)**：模型在第一幀初始化一組可學習的「物件查詢向量」，每一個查詢代表一個潛在的物體。  <br>2. **查詢傳播 (Query Propagation)**：在處理後續幀時，前一幀的物件查詢會被**直接傳遞**到當前幀，並與當前幀的圖像特徵進行交互（通過注意力機制）。  <br>3. **狀態更新**：交互後的查詢向量會更新自己的狀態（位置、外觀等），並輸出當前幀的偵測框。  <br>  <br>**核心優勢**：由於查詢向量本身就攜帶了物體的身份資訊，因此**追蹤是內建於偵測過程中的**。這種架構對遮擋和物體交互的建模能力更強，時間連貫性也更好。**TrackFormer** 和 **TransTrack** 是這個方向上非常經典且重要的模型。|
|**3. Grounding**|**開放集/零樣本偵測與追蹤 (Open-Set/Zero-Shot Tracking)**|• **GroundingDINO + Tracker**  <br>• **Grounded-SAM + Tracker**  <br>• **OVTrack (Open-Vocabulary Tracker)**|**技術關係**：這是一個新興的、更具挑戰性的領域，它旨在**追蹤任意可以用自然語言描述的物體**，而不僅僅是預定義的類別。  <br>  <br>**工作流程**（以GroundingDINO + Tracker為例）：  <br>1. **首次定位 (Initial Grounding)**：用戶給定一個文本指令，例如「追蹤那隻戴著紅色項圈的黑狗」。在第一幀，**GroundingDINO**負責根據這段話，準確地找到目標狗的位置。  <br>2. **目標建立**：一旦目標被定位，系統會為其創建一個追蹤實例，並提取其視覺特徵。  <br>3. **後續追蹤**：在後續的影片幀中，可以使用傳統的追蹤技術（如基於特徵匹配的SiamRPN）或更先進的Transformer追蹤器來持續跟蹤這個特定的目標。  <br>  <br>**核心優勢**：**極致的靈活性**。傳統模型只能追蹤「狗」，而Grounding模型可以追蹤「戴著紅色項圈的黑狗」，甚至可以追蹤「畫面左側跑得最快的那個人」。它將追蹤任務從一個**封閉集合 (Closed-Set)** 推向了一個**開放世界 (Open-World)**。  <br>  <br>**Grounded-SAM**則是在此基礎上，將輸出從邊界框變為了更精細的**分割掩碼 (Mask)**，實現**影片物件分割追蹤**。|

### 總結與趨勢

1. **從分離到統一**：技術的演進是從「偵測」和「追蹤」兩個分離的任務，走向由Transformer驅動的**端到端統一模型**。
    
2. **從閉集到開放集**：應用的目標是從只能追蹤預定義的幾個類別（人、車），走向由Grounding技術驅動的、能夠**追蹤任意可描述物體**的開放世界。
    

您可以這樣理解這三類方法的關係：

- **No Transformer**：像是一條**傳統的工廠流水線**，分工明確，每個工站（偵測、匹配）只做一件事。成熟、高效，但靈活性差。
    
- **Transformer**：像是一個**高度整合的智慧製造單元**，原材料（影片幀）進去，成品（帶ID的軌跡）直接出來。流程更順暢，品質更高，但對「原材料」的種類有要求（閉集）。
    
- **Grounding**：則為這條流水線或智慧單元增加了一個**「語音控制」的智慧接口**，你可以隨時通過說話來改變生產目標。這是目前最前沿、最具挑戰性也最有潛力的發展方向。









===============================================================
### <mark style="background: #BBFABBA6;">video segmentation</mark>
###### Video segmentation model

影片分割（Video Segmentation）這個更複雜的領域。影片分割不僅要處理單一圖像的空間資訊，還必須應對時間維度上的動態變化和物體間的關聯。

同樣地，在整理模型前，我們需要先明確影片語義分割 (Video Semantic Segmentation, VSS) 和影片實例分割 (Video Instance Segmentation, VIS) 的區別：

- **影片語義分割 (VSS)**：目標是為影片中**每一幀的每一個像素**分配一個**物體類別**。它在時間上是連貫的，例如，影片中所有屬於「道路」的像素在每一幀都會被標記為「道路」。
    
- **影片實例分割 (VIS)**：目標是在VSS的基礎上，進一步**追蹤每一個獨立的物體實例**。例如，影片中有三輛車，VIS不僅要將它們都分割出來，還要為它們分配獨一無二且在整段影片中保持不變的ID（Car_1, Car_2, Car_3），並追蹤它們的運動軌跡。
    

**結論：VIS任務包含了VSS的要素，並增加了「實例追蹤」，因此是更複雜、更全面的任務。** 大多數先進的影片分割模型都聚焦在VIS上，因為它提供了更豐富的資訊。

接下來，我將按照**「AI model no transformer」、「transformer」、「grounding」**三種類別，為您整理影片分割領域的主流模型。

---

### 影片分割主流模型整理

### 1. AI Model (No Transformer) - 經典CNN與時序結合架構

這類方法的經典思想是「**先分割，再追蹤**」或「**利用時序資訊輔助分割**」。它們通常在強大的圖像分割模型（如Mask R-CNN）基礎上，增加一個用於處理時間維度的模組。

|代表模型|主要任務|核心思想與技術特點|
|---|---|---|
|**MaskTrack R-CNN**|**影片實例分割 (VIS)**|**在Mask R-CNN上增加追蹤分支**：這是將成功的圖像實例分割模型擴展到影片的典型代表。  <br>1. **基於Mask R-CNN**：對每一幀獨立進行實例分割。  <br>2. **增加追蹤頭 (Tracking Head)**：除了原有的分類、邊界框和掩碼分支外，它額外增加了一個「追蹤頭」。這個頭會計算當前幀檢測到的物體與前一幀已存在物體之間的相似度，從而將它們關聯起來，賦予穩定的ID。  <br>3. **特徵引導**：它會利用前一幀的物體特徵來指導當前幀的檢測，幫助模型更好地處理遮擋和運動模糊。  <br>  <br>**本質**：這是一種**先偵測後追蹤 (Tracking-by-Detection)** 的哲學在分割任務上的延伸，流程清晰但並非端到端。|
|**光流輔助法  <br>(Flow-warping based)**|**影片語義分割 (VSS)**|**利用運動資訊傳播掩碼**：這類方法的核心是利用**光流 (Optical Flow)** 來估計相鄰幀之間的像素級運動。  <br>1. **計算光流**：首先計算第 `t-1` 幀到第 `t` 幀的光流場。<br>2. **扭曲/傳播 (Warping)**：根據光流場，將第 `t-1` 幀的分割結果「扭曲」到第 `t` 幀，得到一個初步的預測。<br>3. **精煉 (Refinement)**：再用一個輕量級的CNN對這個初步預測進行修正和精煉，得到最終的分割結果。  <br>  <br>**優點**：可以顯著降低計算量，因為不需要對每一幀都運行重量級的分割模型。  <br>**缺點**：非常依賴光流的計算精度，在快速運動、遮擋或光照劇變的場景下容易失效。|

---

### 2. Transformer - 革新性端到端架構

Transformer的注意力機制天然適合處理序列數據，因此在影片這種時空序列任務上大放異彩。它將影片分割從多階段流程變革為單一的端到端模型。

|代表模型|主要任務|核心思想與技術特點|
|---|---|---|
|**VisTR (Video Instance Segmentation Transformer)**|**影片實例分割 (VIS)**|**將影片實例分割視為端到端的序列預測**：VisTR是該領域的開創性工作，其思想與DETR一脈相承。  <br>1. **實例查詢序列 (Instance Query Sequence)**：模型在影片的第一幀初始化一組可學習的「物件查詢」。與DETR不同的是，**同一個物體的查詢在不同幀之間是共享的**。  <br>2. **時空注意力**：模型在一個統一的編碼器-解碼器架構中，同時對影片的空間維度（單幀圖像內）和時間維度（跨多幀之間）進行注意力計算。  <br>3. **直接預測**：每個物件查詢在每一幀都會直接預測出一個掩碼，從而自然地形成了帶有ID的實例分割軌跡。  <br>  <br>**優點**：首個真正的**端到端**VIS模型，架構簡潔，無需追蹤匹配等複雜後處理。  <br>**缺點**：計算成本高，處理長影片有困難。|
|**VITA (Video Instance-level Task-Agnostic)**|**影片實例分割 (VIS)**|**更高效的時序特徵聚合**：VITA是對VisTR等早期模型的改進，旨在更高效地處理長影片。  <br>1. **解耦時空處理**：它不再同時計算昂貴的時空注意力，而是先用一個圖像模型（如Mask2Former）提取每幀的實例特徵。  <br>2. **時序Transformer**：然後用一個獨立的、輕量級的時序Transformer模組來對這些跨幀的實例特徵進行關聯和追蹤。  <br>3. **記憶機制**：引入了記憶標記（memory tokens）來存儲和更新長期的物體外觀和運動歷史，提高了處理長影片時的追蹤魯棒性。  <br>  <br>**優點**：在保持高性能的同時，**計算效率顯著提升**，能更好地處理長影片。|

---

### 3. Grounding - 語言驅動的開放世界

這是目前最前沿、最靈活的方向，目標是實現「用自然語言指令，分割並追蹤影片中的任意物體」。

|代表模型|主要任務|核心思想與技術特點|
|---|---|---|
|**Track Anything Model (TAM) /  <br>Grounded SAM + Tracker**|**開放集影片實例分割**|**「一次接地，持續追蹤」(Grounding Once, Tracking Anything)**：這類方法是典型的組合式（Pipeline）工作流，但威力巨大。  <br>1. **第一步 (任意目標的首次分割)**：用戶在影片的任意一幀，通過文本指令（如「那隻跳起來的貓」）或點擊，使用**Grounded SAM**生成目標物體的**第一個精確掩碼**。  <br>2. **第二步 (啟動專業追蹤器)**：將這個初始掩碼送入一個強大的、專門的影片分割追蹤模型（如 **XMem**）。  <br>3. **第三步 (穩定追蹤)**：XMem等模型會利用其高效的記憶網絡，在後續的影片中穩定、精確地**傳播和追蹤**這個初始掩碼，形成完整的分割軌跡。  <br>  <br>**優點**：**極致的靈活性和交互性**。結合了Grounding模型的「什麼都能分」和影片追蹤模型的「什麼都能穩穩跟住」的優點。  <br>**能否做語義分割？** 可以。您可以通過提示一個類別（如"car"），Grounded SAM會找到所有車的實例，然後您可以為每個實例都啟動一個追蹤器。如果只需要語義結果，將所有追蹤的車的掩碼合併即可。|

### 總結

|類別|主要任務|核心範式|
|---|---|---|
|**No Transformer**|VIS / VSS|**先分割後追蹤**或**光流輔助**，流程分明，但模組間存在壁壘。|
|**Transformer**|主要是VIS|**端到端序列預測**，利用物件查詢在時空中直接生成帶有ID的掩碼軌跡。|
|**Grounding**|開放集VIS|**「接地」與「追蹤」的組合**，先用語言定位任意目標，再用專業追蹤器進行穩定跟蹤。|

總的來說，與圖像分割類似，影片分割的發展趨勢也是從**多階段、非端到端**的傳統方法，走向由Transformer驅動的**端到端模型**，並最終邁向由Grounding技術賦能的、能夠理解人類指令的**開放世界交互式分割與追蹤**。

我們來深入探討您提到的這幾種先進的影片分割模型。這是一個非常精彩的對比，因為它們正好代表了當前影片分割領域的幾大主流思想：**高效的專用追蹤器**、**端到端的Transformer**、**強大的預訓練**以及**靈活的開放集互動**。

### 第一部分：模型類型歸屬

首先，我們將您提到的幾個模型進行歸類：

|模型/方法|所屬類型|核心解釋|
|---|---|---|
|**XMem**|**AI Model (No Transformer) / 混合型**|XMem的核心是一個基於CNN的架構，但它巧妙地融合了類似注意力機制的**記憶讀取（Memory Read）操作。它不屬於ViT或DETR那樣的純Transformer架構，其主體是卷積網路和一個精心設計的記憶模塊。因此，將它歸類為「No Transformer」或更精確的「混合型」是合適的。|
|VideoMAE + XMem|預訓練方法 + 模型 (混合型)|這是一個工作流（Pipeline）**。**VideoMAE**是一個純粹的**Transformer**模型，但它本身**不負責分割**，而是作為一個強大的**預訓練工具**，用於從無標註影片中學習通用的時空特徵。最終執行分割任務的仍然是**XMem**。所以這個組合的本質是：用Transformer預訓練來增強一個非Transformer的分割模型。|
|**Track Anything Model (TAM)**|**Grounding (接地)**|TAM是典型的**Grounding**類應用。它的核心思想是「**一次接地，持續追蹤**」。它首先利用一個Grounding模型（如Grounded SAM）來響應自然語言或用戶點擊，完成任意物體的**首次分割**，然後將這個初始掩碼交給一個專業的追蹤器去完成後續影片的追蹤。|
|**SAM2 (預期)**|**Transformer**|雖然SAM2的確切架構尚未完全公布，但基於SAM（其圖像編碼器是Vision Transformer）和當前的技術趨勢，學術界和工業界普遍預期SAM2將是一個**完全基於Transformer的、內建了時序處理能力的模型**，旨在原生支持影片的分割與追蹤。|

### 第二部分：Grounded SAM + Tracker 的追蹤器選擇

在「Grounded SAM + Tracker」這種組合式方案中，Tracker的選擇至關重要，它決定了追蹤的穩定性和效率。

目前，最常用且性能最強的選擇是 **XMem** 或其改進版 **XMem++**。

**原因如下**：

1. **SOTA性能**：XMem是影片物件分割（VOS）領域的頂級模型，以其極高的準確性和對長影片的魯棒性著稱。
    
2. **效率與記憶體管理**：XMem的設計精髓在於其獨特的記憶網絡，它將記憶分為「永久記憶」和「瞬時記憶」，避免了隨時間推移而導致的記憶體無限增長和計算成本飆升，非常適合長影片追蹤。
    
3. **兼容性**：它的輸入恰好是一個初始掩碼，這與Grounded SAM的輸出完美匹配，可以無縫地將兩者銜接起來。
    

除了XMem，也有研究使用其他VOS模型如**DeAOT (Decoupling AOT)**等，但XMem無疑是當前最主流的選擇。

---

### 第三部分：全面比較分析 (效能與手術影片應用)

這部分我們將把新舊模型放在一起，從「效能（運行時間、記憶體）」和「手術影片應用」兩個維度進行全面比較。

#### 1. 效能比較 (運行時間 & 記憶體)

這裡我們給出一個定性的比較，從最快/最輕量到最慢/最重型：

1. **最快/最低**：**專用VOS模型 (如 XMem)**
    
    - **運行時間**：非常快。因為它不需要對每一幀都進行重量級的檢測和分割，主要是進行高效的特徵匹配和掩碼傳播。
        
    - **記憶體**：非常低。其精心設計的記憶機制使其記憶體佔用幾乎是恆定的，不會隨影片長度增加而爆炸。
        
2. **較快/較低**：**傳統方法 (如 MaskTrack R-CNN)**
    
    - **運行時間**：比XMem慢，因為每幀仍需運行一個完整的檢測器，但比端到端Transformer快。
        
    - **記憶體**：相對可控，主要開銷來自於偵測器和儲存前一幀的特徵。
        
3. **較慢/較高**：**端到端Transformer (如 VITA, VisTR)**
    
    - **運行時間**：較慢。雖然比最早的VisTR有所改進，但Transformer的全局注意力計算仍然非常耗時。
        
    - **記憶體**：較高。需要緩存多幀的特徵圖和注意力矩陣。
        
4. **最慢/最高**：**逐幀Grounding (如 Grounded SAM逐幀應用)**
    
    - **運行時間**：極慢。每一幀都要運行一次重量級的GroundingDINO和SAM，完全沒有特徵重用，計算成本巨大。
        
    - **記憶體**：極高。每一幀的計算都需要加載完整的模型並處理高解析度圖像。
        

**Track Anything Model (TAM)** 的效能取決於其追蹤器。如果使用XMem，那麼在完成首次接地後，其後續的追蹤效能就等同於XMem。

**VideoMAE + XMem** 的效能與XMem本身一致，因為VideoMAE只在**離線的預訓練階段**使用，不影響**線上推理**時的速度和記憶體。

#### 2. 手術影片應用場景比較

手術影片的挑戰：**器械移動快、反光、煙霧遮擋、組織形變、外觀相似、對即時性和穩定性要求極高**。

|模型/方法|手術影片應用 - 優點|手術影片應用 - 缺點|
|---|---|---|
|**傳統方法 (MaskTrack R-CNN)**|• 技術相對成熟，易於理解和部署。  <br>• 對於一些標準器械的追蹤有不錯的效果。|• **對快速運動和遮擋敏感**，手術器械的快速移動極易導致ID交換或跟丟。  <br>• **閉集限制**，無法識別和追蹤非標準器械或異常組織。|
|**端到端Transformer (VITA)**|• **時間連貫性好**，內建的追蹤機制使其在遮擋情況下比傳統方法更魯棒。  <br>• 能夠端到端學習手術場景的特定運動模式。|• **閉集限制**，和傳統方法一樣，靈活性差。  <br>• **計算成本高**，在手術室中實現即時部署挑戰巨大。|
|**專用VOS (XMem)**|• **極佳的追蹤穩定性**，一旦鎖定目標，即使在快速運動和部分遮擋下也能穩穩跟住，非常適合追蹤手術器械。  <br>• **高效**，有潛力在邊緣計算設備上實現即時運行。|• **需要初始掩碼**，這是一個核心矛盾。在手術中，如何方便快捷地提供第一個目標掩碼是一個難題。  <br>• **無語義理解**，它只負責「跟住」，不知道自己跟的是什麼。|
|**VideoMAE + XMem**|• **🌟 可能是目前最適合特定任務的方案**。除了XMem的所有優點外，通過在海量**無標註手術影片**上用VideoMAE進行預訓練，可以讓XMem的特徵提取器**極其適應手術環境**，對反光、煙霧、血液等干擾有超強的魯棒性。|• 同樣面臨「需要初始掩碼」和「無語義理解」的問題。  <br>• 預訓練階段需要巨大的計算資源。|
|**Track Anything (TAM)**|• **🌟 交互範式的革命**。它完美解決了XMem的初始掩碼問題。外科醫生可以**通過點擊甚至語音指令（未來）**，即時指定需要關注的目標，例如「追蹤這個出血點」或「分割這塊異常組織」。**靈活性無與倫比**。|• **可靠性是關鍵**。首次接地的準確性至關重要，一旦失敗，後續追蹤也無從談起。  <br>• **延遲問題**，首次接地的過程可能耗時較長，對即時手術輔助構成挑戰。|
|**SAM2 (預期)**|• **🌟 理論上的最優解**。預期它將融合TAM的**靈活性**和VITA/XMem的**時間連貫性與效率**。一個可以被即時提示、並且原生支持穩定追蹤的單一模型，將是手術輔助的終極形態。|• 目前仍處於概念和研發階段，實際性能和局限性未知。|

### 總結列表

|類別/模型|核心思想|效能 (時間/記憶體)|手術應用優點|手術應用缺點|
|---|---|---|---|---|
|**傳統方法**|先分割後追蹤|較快 / 較低|技術成熟|閉集，對遮擋敏感，易跟丟|
|**端到端Transformer**|端到端序列預測|較慢 / 較高|時間連貫性好|閉集，計算成本高|
|**專用VOS (XMem)**|高效的記憶傳播|**最快 / 最低**|**追蹤極其穩定、高效**|需要初始掩碼，無語義理解|
|**VideoMAE + XMem**|預訓練增強+記憶傳播|最快 / 最低|**對手術偽影魯棒性極強**|需要初始掩碼，無語義理解|
|**Track Anything (TAM)**|一次接地+持續追蹤|首次慢，後續快|**交互靈活性無與倫比**|首次定位的可靠性和延遲是挑戰|
|**SAM2 (預期)**|可提示的端到端追蹤|未知，預期高效|**潛在的理想形態**：靈活+穩定+高效|尚處於研發階段|